ARM GAS  /tmp/cc0b6Xiw.s 			page 1


   1              		.cpu cortex-m4
   2              		.arch armv7e-m
   3              		.fpu fpv4-sp-d16
   4              		.eabi_attribute 27, 1
   5              		.eabi_attribute 28, 1
   6              		.eabi_attribute 20, 1
   7              		.eabi_attribute 21, 1
   8              		.eabi_attribute 23, 3
   9              		.eabi_attribute 24, 1
  10              		.eabi_attribute 25, 1
  11              		.eabi_attribute 26, 1
  12              		.eabi_attribute 30, 1
  13              		.eabi_attribute 34, 1
  14              		.eabi_attribute 18, 4
  15              		.file	"bio.c"
  16              		.text
  17              	.Ltext0:
  18              		.cfi_sections	.debug_frame
  19              		.file 1 "./block/bio.c"
  20              		.section	.text.__spin_unlock,"ax",%progbits
  21              		.align	1
  22              		.syntax unified
  23              		.thumb
  24              		.thumb_func
  26              	__spin_unlock:
  27              	.LVL0:
  28              	.LFB254:
  29              		.file 2 "./arch/arm_m/include/asm/spinlock.h"
   1:./arch/arm_m/include/asm/spinlock.h **** #ifndef __SPIN_LOCK_H_
   2:./arch/arm_m/include/asm/spinlock.h **** #define __SPIN_LOCK_H_
   3:./arch/arm_m/include/asm/spinlock.h **** 
   4:./arch/arm_m/include/asm/spinlock.h **** #include <linux/sched.h>
   5:./arch/arm_m/include/asm/spinlock.h **** 
   6:./arch/arm_m/include/asm/spinlock.h **** typedef struct { 
   7:./arch/arm_m/include/asm/spinlock.h ****      int flag; 
   8:./arch/arm_m/include/asm/spinlock.h **** } arch_spinlock_t;
   9:./arch/arm_m/include/asm/spinlock.h **** 
  10:./arch/arm_m/include/asm/spinlock.h **** static int __spin_init(arch_spinlock_t *lock) 
  11:./arch/arm_m/include/asm/spinlock.h **** {
  12:./arch/arm_m/include/asm/spinlock.h ****     lock->flag = 0;
  13:./arch/arm_m/include/asm/spinlock.h **** }
  14:./arch/arm_m/include/asm/spinlock.h **** 
  15:./arch/arm_m/include/asm/spinlock.h **** static int __spin_lock(arch_spinlock_t *lock) {
  16:./arch/arm_m/include/asm/spinlock.h **** 
  17:./arch/arm_m/include/asm/spinlock.h ****     stop_all_scheduler();
  18:./arch/arm_m/include/asm/spinlock.h ****     if(lock->flag == 0){
  19:./arch/arm_m/include/asm/spinlock.h ****           lock->flag = 1;  
  20:./arch/arm_m/include/asm/spinlock.h ****           start_all_scheduler();
  21:./arch/arm_m/include/asm/spinlock.h ****         return 1;
  22:./arch/arm_m/include/asm/spinlock.h ****     }
  23:./arch/arm_m/include/asm/spinlock.h ****     else
  24:./arch/arm_m/include/asm/spinlock.h ****     {
  25:./arch/arm_m/include/asm/spinlock.h ****         start_all_scheduler();
  26:./arch/arm_m/include/asm/spinlock.h ****         return 0;
  27:./arch/arm_m/include/asm/spinlock.h ****     }
  28:./arch/arm_m/include/asm/spinlock.h **** }
  29:./arch/arm_m/include/asm/spinlock.h **** 
ARM GAS  /tmp/cc0b6Xiw.s 			page 2


  30:./arch/arm_m/include/asm/spinlock.h **** static void  __spin_unlock( arch_spinlock_t *lock ){
  30              		.loc 2 30 52 view -0
  31              		.cfi_startproc
  32              		@ args = 0, pretend = 0, frame = 0
  33              		@ frame_needed = 0, uses_anonymous_args = 0
  34              		@ link register save eliminated.
  31:./arch/arm_m/include/asm/spinlock.h ****     lock->flag = 0;
  35              		.loc 2 31 5 view .LVU1
  36              		.loc 2 31 16 is_stmt 0 view .LVU2
  37 0000 0023     		movs	r3, #0
  38 0002 0360     		str	r3, [r0]
  32:./arch/arm_m/include/asm/spinlock.h **** }
  39              		.loc 2 32 1 view .LVU3
  40 0004 7047     		bx	lr
  41              		.cfi_endproc
  42              	.LFE254:
  44              		.section	.text.spin_unlock,"ax",%progbits
  45              		.align	1
  46              		.syntax unified
  47              		.thumb
  48              		.thumb_func
  50              	spin_unlock:
  51              	.LVL1:
  52              	.LFB257:
  53              		.file 3 "./include/linux/spinlock.h"
   1:./include/linux/spinlock.h **** #ifndef __SPINLOCK_H__
   2:./include/linux/spinlock.h **** #define __SPINLOCK_H__
   3:./include/linux/spinlock.h **** 
   4:./include/linux/spinlock.h **** #include <linux/types.h>
   5:./include/linux/spinlock.h **** #include <linux/spinlock_types.h>
   6:./include/linux/spinlock.h **** #include <linux/sched.h>
   7:./include/linux/spinlock.h **** 
   8:./include/linux/spinlock.h **** 
   9:./include/linux/spinlock.h **** 
  10:./include/linux/spinlock.h **** static void spin_lock_init(spinlock_t* lock){
  11:./include/linux/spinlock.h ****     __spin_init(&lock->rlock.raw_lock);
  12:./include/linux/spinlock.h **** }
  13:./include/linux/spinlock.h **** 
  14:./include/linux/spinlock.h **** static void spin_lock(spinlock_t* lock)
  15:./include/linux/spinlock.h **** {  
  16:./include/linux/spinlock.h ****     while (1)
  17:./include/linux/spinlock.h ****     {
  18:./include/linux/spinlock.h ****         if(__spin_lock(&lock->rlock.raw_lock) == 1){
  19:./include/linux/spinlock.h ****             lock->owner = get_current_task();
  20:./include/linux/spinlock.h ****             break;
  21:./include/linux/spinlock.h ****         }
  22:./include/linux/spinlock.h ****         else  if(lock->owner == get_current_task()){  //å¦‚æžœå·²ç»è¢«é”ä½ä½†æ˜¯é”æ˜¯è‡ªå·±çš„ï
  23:./include/linux/spinlock.h ****             return;   
  24:./include/linux/spinlock.h ****         }
  25:./include/linux/spinlock.h ****         else{
  26:./include/linux/spinlock.h ****             __delay(5); //ä¸»åŠ¨è®©å‡ºæ—¶é—´ç‰‡
  27:./include/linux/spinlock.h ****         }
  28:./include/linux/spinlock.h ****     }
  29:./include/linux/spinlock.h **** }
  30:./include/linux/spinlock.h **** 
  31:./include/linux/spinlock.h **** static void spin_unlock(spinlock_t* lock)
  32:./include/linux/spinlock.h **** {
ARM GAS  /tmp/cc0b6Xiw.s 			page 3


  54              		.loc 3 32 1 is_stmt 1 view -0
  55              		.cfi_startproc
  56              		@ args = 0, pretend = 0, frame = 0
  57              		@ frame_needed = 0, uses_anonymous_args = 0
  58              		.loc 3 32 1 is_stmt 0 view .LVU5
  59 0000 08B5     		push	{r3, lr}
  60              	.LCFI0:
  61              		.cfi_def_cfa_offset 8
  62              		.cfi_offset 3, -8
  63              		.cfi_offset 14, -4
  33:./include/linux/spinlock.h ****     __spin_unlock(&lock->rlock.raw_lock);
  64              		.loc 3 33 5 is_stmt 1 view .LVU6
  65 0002 FFF7FEFF 		bl	__spin_unlock
  66              	.LVL2:
  34:./include/linux/spinlock.h **** }
  67              		.loc 3 34 1 is_stmt 0 view .LVU7
  68 0006 08BD     		pop	{r3, pc}
  69              		.cfi_endproc
  70              	.LFE257:
  72              		.section	.text.__spin_lock,"ax",%progbits
  73              		.align	1
  74              		.syntax unified
  75              		.thumb
  76              		.thumb_func
  78              	__spin_lock:
  79              	.LVL3:
  80              	.LFB253:
  15:./arch/arm_m/include/asm/spinlock.h **** 
  81              		.loc 2 15 47 is_stmt 1 view -0
  82              		.cfi_startproc
  83              		@ args = 0, pretend = 0, frame = 0
  84              		@ frame_needed = 0, uses_anonymous_args = 0
  15:./arch/arm_m/include/asm/spinlock.h **** 
  85              		.loc 2 15 47 is_stmt 0 view .LVU9
  86 0000 38B5     		push	{r3, r4, r5, lr}
  87              	.LCFI1:
  88              		.cfi_def_cfa_offset 16
  89              		.cfi_offset 3, -16
  90              		.cfi_offset 4, -12
  91              		.cfi_offset 5, -8
  92              		.cfi_offset 14, -4
  93 0002 0446     		mov	r4, r0
  17:./arch/arm_m/include/asm/spinlock.h ****     if(lock->flag == 0){
  94              		.loc 2 17 5 is_stmt 1 view .LVU10
  95 0004 FFF7FEFF 		bl	stop_all_scheduler
  96              	.LVL4:
  18:./arch/arm_m/include/asm/spinlock.h ****           lock->flag = 1;  
  97              		.loc 2 18 5 view .LVU11
  18:./arch/arm_m/include/asm/spinlock.h ****           lock->flag = 1;  
  98              		.loc 2 18 12 is_stmt 0 view .LVU12
  99 0008 2368     		ldr	r3, [r4]
  18:./arch/arm_m/include/asm/spinlock.h ****           lock->flag = 1;  
 100              		.loc 2 18 7 view .LVU13
 101 000a 2BB9     		cbnz	r3, .L5
  19:./arch/arm_m/include/asm/spinlock.h ****           start_all_scheduler();
 102              		.loc 2 19 11 is_stmt 1 view .LVU14
  19:./arch/arm_m/include/asm/spinlock.h ****           start_all_scheduler();
ARM GAS  /tmp/cc0b6Xiw.s 			page 4


 103              		.loc 2 19 22 is_stmt 0 view .LVU15
 104 000c 0125     		movs	r5, #1
 105 000e 2560     		str	r5, [r4]
  20:./arch/arm_m/include/asm/spinlock.h ****         return 1;
 106              		.loc 2 20 11 is_stmt 1 view .LVU16
 107 0010 FFF7FEFF 		bl	start_all_scheduler
 108              	.LVL5:
  21:./arch/arm_m/include/asm/spinlock.h ****     }
 109              		.loc 2 21 9 view .LVU17
  21:./arch/arm_m/include/asm/spinlock.h ****     }
 110              		.loc 2 21 16 is_stmt 0 view .LVU18
 111 0014 2846     		mov	r0, r5
 112              	.L4:
  28:./arch/arm_m/include/asm/spinlock.h **** 
 113              		.loc 2 28 1 view .LVU19
 114 0016 38BD     		pop	{r3, r4, r5, pc}
 115              	.LVL6:
 116              	.L5:
  25:./arch/arm_m/include/asm/spinlock.h ****         return 0;
 117              		.loc 2 25 9 is_stmt 1 view .LVU20
 118 0018 FFF7FEFF 		bl	start_all_scheduler
 119              	.LVL7:
  26:./arch/arm_m/include/asm/spinlock.h ****     }
 120              		.loc 2 26 9 view .LVU21
  26:./arch/arm_m/include/asm/spinlock.h ****     }
 121              		.loc 2 26 16 is_stmt 0 view .LVU22
 122 001c 0020     		movs	r0, #0
 123 001e FAE7     		b	.L4
 124              		.cfi_endproc
 125              	.LFE253:
 127              		.section	.text.spin_lock,"ax",%progbits
 128              		.align	1
 129              		.syntax unified
 130              		.thumb
 131              		.thumb_func
 133              	spin_lock:
 134              	.LVL8:
 135              	.LFB256:
  15:./include/linux/spinlock.h ****     while (1)
 136              		.loc 3 15 1 is_stmt 1 view -0
 137              		.cfi_startproc
 138              		@ args = 0, pretend = 0, frame = 0
 139              		@ frame_needed = 0, uses_anonymous_args = 0
  15:./include/linux/spinlock.h ****     while (1)
 140              		.loc 3 15 1 is_stmt 0 view .LVU24
 141 0000 38B5     		push	{r3, r4, r5, lr}
 142              	.LCFI2:
 143              		.cfi_def_cfa_offset 16
 144              		.cfi_offset 3, -16
 145              		.cfi_offset 4, -12
 146              		.cfi_offset 5, -8
 147              		.cfi_offset 14, -4
 148 0002 0446     		mov	r4, r0
 149 0004 06E0     		b	.L11
 150              	.LVL9:
 151              	.L13:
  19:./include/linux/spinlock.h ****             break;
ARM GAS  /tmp/cc0b6Xiw.s 			page 5


 152              		.loc 3 19 13 is_stmt 1 view .LVU25
  19:./include/linux/spinlock.h ****             break;
 153              		.loc 3 19 27 is_stmt 0 view .LVU26
 154 0006 FFF7FEFF 		bl	get_current_task
 155              	.LVL10:
  19:./include/linux/spinlock.h ****             break;
 156              		.loc 3 19 25 discriminator 1 view .LVU27
 157 000a 2060     		str	r0, [r4]
  20:./include/linux/spinlock.h ****         }
 158              		.loc 3 20 13 is_stmt 1 view .LVU28
 159              	.L8:
  29:./include/linux/spinlock.h **** 
 160              		.loc 3 29 1 is_stmt 0 view .LVU29
 161 000c 38BD     		pop	{r3, r4, r5, pc}
 162              	.LVL11:
 163              	.L14:
  26:./include/linux/spinlock.h ****         }
 164              		.loc 3 26 13 is_stmt 1 view .LVU30
 165 000e 0520     		movs	r0, #5
 166 0010 FFF7FEFF 		bl	__delay
 167              	.LVL12:
  16:./include/linux/spinlock.h ****     {
 168              		.loc 3 16 11 view .LVU31
 169              	.L11:
  16:./include/linux/spinlock.h ****     {
 170              		.loc 3 16 5 view .LVU32
  18:./include/linux/spinlock.h ****             lock->owner = get_current_task();
 171              		.loc 3 18 9 view .LVU33
  18:./include/linux/spinlock.h ****             lock->owner = get_current_task();
 172              		.loc 3 18 12 is_stmt 0 view .LVU34
 173 0014 2046     		mov	r0, r4
 174 0016 FFF7FEFF 		bl	__spin_lock
 175              	.LVL13:
  18:./include/linux/spinlock.h ****             lock->owner = get_current_task();
 176              		.loc 3 18 11 discriminator 1 view .LVU35
 177 001a 0128     		cmp	r0, #1
 178 001c F3D0     		beq	.L13
  22:./include/linux/spinlock.h ****             return;   
 179              		.loc 3 22 15 is_stmt 1 view .LVU36
  22:./include/linux/spinlock.h ****             return;   
 180              		.loc 3 22 22 is_stmt 0 view .LVU37
 181 001e 2568     		ldr	r5, [r4]
  22:./include/linux/spinlock.h ****             return;   
 182              		.loc 3 22 33 view .LVU38
 183 0020 FFF7FEFF 		bl	get_current_task
 184              	.LVL14:
  22:./include/linux/spinlock.h ****             return;   
 185              		.loc 3 22 17 discriminator 1 view .LVU39
 186 0024 8542     		cmp	r5, r0
 187 0026 F2D1     		bne	.L14
 188 0028 F0E7     		b	.L8
 189              		.cfi_endproc
 190              	.LFE256:
 192              		.section	.text.submit_bio_wait,"ax",%progbits
 193              		.align	1
 194              		.global	submit_bio_wait
 195              		.syntax unified
ARM GAS  /tmp/cc0b6Xiw.s 			page 6


 196              		.thumb
 197              		.thumb_func
 199              	submit_bio_wait:
 200              	.LVL15:
 201              	.LFB1040:
   1:./block/bio.c **** #include <linux/kernel.h>
   2:./block/bio.c **** #include <linux/string.h>
   3:./block/bio.c **** #include <linux/blkdev.h>
   4:./block/bio.c **** #include <linux/bio.h>  
   5:./block/bio.c **** #include <linux/slab.h>
   6:./block/bio.c **** #include <linux/errno.h>
   7:./block/bio.c **** #include <linux/spinlock.h>
   8:./block/bio.c **** #include <linux/kthread.h>
   9:./block/bio.c **** 
  10:./block/bio.c **** #define BIO_INLINE_VECS 4
  11:./block/bio.c **** 
  12:./block/bio.c **** struct bio_set fs_bio_set;
  13:./block/bio.c **** 
  14:./block/bio.c **** struct bio *bio_alloc_bioset(struct block_device *bdev, unsigned short nr_iovecs,
  15:./block/bio.c ****     blk_opf_t opf, gfp_t gfp_mask,
  16:./block/bio.c ****     struct bio_set *bs)
  17:./block/bio.c **** {
  18:./block/bio.c ****     struct bio *bio;
  19:./block/bio.c ****     size_t bio_size;
  20:./block/bio.c ****     bool use_inline_vecs = false;
  21:./block/bio.c **** 
  22:./block/bio.c ****     if (nr_iovecs <= BIO_INLINE_VECS) {
  23:./block/bio.c ****         use_inline_vecs = true;
  24:./block/bio.c ****         bio_size = sizeof(struct bio) + sizeof(struct bio_vec) * nr_iovecs;
  25:./block/bio.c ****     } 
  26:./block/bio.c ****     else
  27:./block/bio.c ****         bio_size = sizeof(struct bio);
  28:./block/bio.c ****     
  29:./block/bio.c **** 
  30:./block/bio.c ****    
  31:./block/bio.c ****     bio = kmalloc(bio_size, gfp_mask);
  32:./block/bio.c ****     if (!bio) {
  33:./block/bio.c ****         return NULL;
  34:./block/bio.c ****     }
  35:./block/bio.c ****     memset(bio, 0, bio_size);
  36:./block/bio.c ****     
  37:./block/bio.c ****     
  38:./block/bio.c ****     if (use_inline_vecs) {
  39:./block/bio.c ****         bio->bi_io_vec = bio->bi_inline_vecs;
  40:./block/bio.c ****         bio->bi_max_vecs = nr_iovecs;
  41:./block/bio.c ****     }
  42:./block/bio.c ****     else {
  43:./block/bio.c ****         bio->bi_io_vec = kmalloc_array(nr_iovecs, sizeof(struct bio_vec), gfp_mask);
  44:./block/bio.c ****         if (!bio->bi_io_vec) {
  45:./block/bio.c ****             kfree(bio);
  46:./block/bio.c ****             return NULL;
  47:./block/bio.c ****         }
  48:./block/bio.c ****         bio->bi_max_vecs = nr_iovecs;
  49:./block/bio.c ****     }
  50:./block/bio.c ****     bio->bi_next = NULL;
  51:./block/bio.c ****     bio->bi_bdev = NULL;
  52:./block/bio.c ****     bio->bi_opf = 0;
ARM GAS  /tmp/cc0b6Xiw.s 			page 7


  53:./block/bio.c ****     bio->bi_flags = 0;
  54:./block/bio.c ****     bio->bi_status = BLK_STS_OK;
  55:./block/bio.c ****     atomic_set(&bio->__bi_remaining, 1);
  56:./block/bio.c ****     atomic_set(&bio->__bi_cnt, 1);
  57:./block/bio.c ****     bio->bi_iter.bi_sector = 0;
  58:./block/bio.c ****     bio->bi_iter.bi_size = 0;
  59:./block/bio.c ****     bio->bi_iter.bi_idx = 0;
  60:./block/bio.c ****     bio->bi_iter.bi_bvec_done = 0;
  61:./block/bio.c ****     bio->bi_vcnt = 0;
  62:./block/bio.c ****     bio->bi_end_io = NULL;
  63:./block/bio.c ****     bio->bi_private = NULL;
  64:./block/bio.c ****     bio->bi_bdev = bdev;
  65:./block/bio.c ****     bio->bi_opf = opf;
  66:./block/bio.c ****     return bio;
  67:./block/bio.c **** }
  68:./block/bio.c ****  
  69:./block/bio.c **** void bio_put(struct bio *bio)
  70:./block/bio.c **** {
  71:./block/bio.c ****     if(!bio)return;
  72:./block/bio.c ****     if (atomic_dec_and_test(&bio->__bi_cnt)) {
  73:./block/bio.c ****         if (bio->bi_io_vec != bio->bi_inline_vecs) {
  74:./block/bio.c ****             kfree(bio->bi_io_vec);
  75:./block/bio.c ****         }
  76:./block/bio.c ****         kfree(bio);
  77:./block/bio.c ****     }
  78:./block/bio.c ****     bio = NULL;
  79:./block/bio.c **** }
  80:./block/bio.c **** 
  81:./block/bio.c **** 
  82:./block/bio.c **** 
  83:./block/bio.c **** int bio_add_page(struct bio *bio, struct page *page,
  84:./block/bio.c ****     unsigned int len, unsigned int offset)
  85:./block/bio.c **** {
  86:./block/bio.c ****     return __bio_add_page(bio,page,len,offset);
  87:./block/bio.c **** }
  88:./block/bio.c **** 
  89:./block/bio.c **** 
  90:./block/bio.c **** int __bio_add_page(struct bio *bio, struct page *page,
  91:./block/bio.c ****     unsigned int len, unsigned int off)
  92:./block/bio.c **** {
  93:./block/bio.c ****     struct bio_vec *bv;
  94:./block/bio.c ****     if (unlikely(!bio || !page || off >= PAGE_SIZE)) return  -ENOMEM;;
  95:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
  96:./block/bio.c ****     if (bio->bi_iter.bi_size + len > (256 << PAGE_SHIFT))return -ENOMEM;;
  97:./block/bio.c **** 
  98:./block/bio.c ****     bv = &bio->bi_io_vec[bio->bi_vcnt];
  99:./block/bio.c ****     bv->bv_page = page;
 100:./block/bio.c ****     bv->bv_offset = off;
 101:./block/bio.c ****     bv->bv_len = len;
 102:./block/bio.c **** 
 103:./block/bio.c ****     bio->bi_iter.bi_size += len;
 104:./block/bio.c ****     bio->bi_vcnt++;
 105:./block/bio.c ****     return 0;
 106:./block/bio.c **** }
 107:./block/bio.c **** 
 108:./block/bio.c **** 
 109:./block/bio.c **** void submit_bio_wait(struct bio *bio)
ARM GAS  /tmp/cc0b6Xiw.s 			page 8


 110:./block/bio.c **** {
 202              		.loc 1 110 1 is_stmt 1 view -0
 203              		.cfi_startproc
 204              		@ args = 0, pretend = 0, frame = 0
 205              		@ frame_needed = 0, uses_anonymous_args = 0
 206              		.loc 1 110 1 is_stmt 0 view .LVU41
 207 0000 70B5     		push	{r4, r5, r6, lr}
 208              	.LCFI3:
 209              		.cfi_def_cfa_offset 16
 210              		.cfi_offset 4, -16
 211              		.cfi_offset 5, -12
 212              		.cfi_offset 6, -8
 213              		.cfi_offset 14, -4
 214 0002 0446     		mov	r4, r0
 111:./block/bio.c ****     struct request *rq =  blk_get_request(bio->bi_bdev->bd_queue,bio->bi_opf,GFP_KERNEL);  
 215              		.loc 1 111 5 is_stmt 1 view .LVU42
 216              		.loc 1 111 46 is_stmt 0 view .LVU43
 217 0004 4368     		ldr	r3, [r0, #4]
 218              		.loc 1 111 27 view .LVU44
 219 0006 DD68     		ldr	r5, [r3, #12]
 220 0008 8668     		ldr	r6, [r0, #8]
 221              	.LVL16:
 222              	.LBB80:
 223              	.LBI80:
 224              		.file 4 "./include/linux/blkdev.h"
   1:./include/linux/blkdev.h **** /* SPDX-License-Identifier: GPL-2.0 */
   2:./include/linux/blkdev.h **** /*
   3:./include/linux/blkdev.h ****  * Portions Copyright (C) 1992 Drew Eckhardt
   4:./include/linux/blkdev.h ****  */
   5:./include/linux/blkdev.h **** #ifndef _LINUX_BLKDEV_H
   6:./include/linux/blkdev.h **** #define _LINUX_BLKDEV_H
   7:./include/linux/blkdev.h **** 
   8:./include/linux/blkdev.h **** #include <linux/types.h>
   9:./include/linux/blkdev.h **** #include <linux/blk-mq.h>
  10:./include/linux/blkdev.h **** #include <linux/blk_types.h>
  11:./include/linux/blkdev.h **** #include <linux/refcount_types.h>
  12:./include/linux/blkdev.h **** #include <linux/lockdep_types.h>
  13:./include/linux/blkdev.h **** #include <linux/blk_types.h>
  14:./include/linux/blkdev.h **** #include <linux/workqueue_types.h>
  15:./include/linux/blkdev.h **** #include <linux/blk.h>
  16:./include/linux/blkdev.h **** #include <linux/pr.h>
  17:./include/linux/blkdev.h **** #include <linux/hdreg.h>
  18:./include/linux/blkdev.h **** #include <linux/kdev_t.h>
  19:./include/linux/blkdev.h **** 
  20:./include/linux/blkdev.h **** struct gendisk;
  21:./include/linux/blkdev.h **** 
  22:./include/linux/blkdev.h **** 
  23:./include/linux/blkdev.h **** /*
  24:./include/linux/blkdev.h ****  * Maximum number of blkcg policies allowed to be registered concurrently.
  25:./include/linux/blkdev.h ****  * Defined here to simplify include dependency.
  26:./include/linux/blkdev.h ****  */
  27:./include/linux/blkdev.h **** #define BLKCG_MAX_POLS		6
  28:./include/linux/blkdev.h **** 
  29:./include/linux/blkdev.h **** #define DISK_MAX_PARTS			256
  30:./include/linux/blkdev.h **** #define DISK_NAME_LEN			32
  31:./include/linux/blkdev.h **** 
  32:./include/linux/blkdev.h **** #define PARTITION_META_INFO_VOLNAMELTH	64
ARM GAS  /tmp/cc0b6Xiw.s 			page 9


  33:./include/linux/blkdev.h **** /*
  34:./include/linux/blkdev.h ****  * Enough for the string representation of any kind of UUID plus NULL.
  35:./include/linux/blkdev.h ****  * EFI UUID is 36 characters. MSDOS UUID is 11 characters.
  36:./include/linux/blkdev.h ****  */
  37:./include/linux/blkdev.h **** #define PARTITION_META_INFO_UUIDLTH	(UUID_STRING_LEN + 1)
  38:./include/linux/blkdev.h **** 
  39:./include/linux/blkdev.h **** struct partition_meta_info {
  40:./include/linux/blkdev.h **** 	char uuid[PARTITION_META_INFO_UUIDLTH];
  41:./include/linux/blkdev.h **** 	u8 volname[PARTITION_META_INFO_VOLNAMELTH];
  42:./include/linux/blkdev.h **** };
  43:./include/linux/blkdev.h **** 
  44:./include/linux/blkdev.h **** /**
  45:./include/linux/blkdev.h ****  * DOC: genhd capability flags
  46:./include/linux/blkdev.h ****  *
  47:./include/linux/blkdev.h ****  * ``GENHD_FL_REMOVABLE``: indicates that the block device gives access to
  48:./include/linux/blkdev.h ****  * removable media.  When set, the device remains present even when media is not
  49:./include/linux/blkdev.h ****  * inserted.  Shall not be set for devices which are removed entirely when the
  50:./include/linux/blkdev.h ****  * media is removed.
  51:./include/linux/blkdev.h ****  *
  52:./include/linux/blkdev.h ****  * ``GENHD_FL_HIDDEN``: the block device is hidden; it doesn't produce events,
  53:./include/linux/blkdev.h ****  * doesn't appear in sysfs, and can't be opened from userspace or using
  54:./include/linux/blkdev.h ****  * blkdev_get*. Used for the underlying components of multipath devices.
  55:./include/linux/blkdev.h ****  *
  56:./include/linux/blkdev.h ****  * ``GENHD_FL_NO_PART``: partition support is disabled.  The kernel will not
  57:./include/linux/blkdev.h ****  * scan for partitions from add_disk, and users can't add partitions manually.
  58:./include/linux/blkdev.h ****  *
  59:./include/linux/blkdev.h ****  */
  60:./include/linux/blkdev.h **** enum {
  61:./include/linux/blkdev.h **** 	GENHD_FL_REMOVABLE			= 1 << 0,
  62:./include/linux/blkdev.h **** 	GENHD_FL_HIDDEN				= 1 << 1,
  63:./include/linux/blkdev.h **** 	GENHD_FL_NO_PART			= 1 << 2,
  64:./include/linux/blkdev.h **** };
  65:./include/linux/blkdev.h **** 
  66:./include/linux/blkdev.h **** enum {
  67:./include/linux/blkdev.h **** 	DISK_EVENT_MEDIA_CHANGE			= 1 << 0, /* media changed */
  68:./include/linux/blkdev.h **** 	DISK_EVENT_EJECT_REQUEST		= 1 << 1, /* eject requested */
  69:./include/linux/blkdev.h **** };
  70:./include/linux/blkdev.h **** 
  71:./include/linux/blkdev.h **** enum {
  72:./include/linux/blkdev.h **** 	/* Poll even if events_poll_msecs is unset */
  73:./include/linux/blkdev.h **** 	DISK_EVENT_FLAG_POLL			= 1 << 0,
  74:./include/linux/blkdev.h **** 	/* Forward events to udev */
  75:./include/linux/blkdev.h **** 	DISK_EVENT_FLAG_UEVENT			= 1 << 1,
  76:./include/linux/blkdev.h **** 	/* Block event polling when open for exclusive write */
  77:./include/linux/blkdev.h **** 	DISK_EVENT_FLAG_BLOCK_ON_EXCL_WRITE	= 1 << 2,
  78:./include/linux/blkdev.h **** };
  79:./include/linux/blkdev.h **** 
  80:./include/linux/blkdev.h **** 
  81:./include/linux/blkdev.h **** 
  82:./include/linux/blkdev.h **** enum blk_integrity_checksum {
  83:./include/linux/blkdev.h **** 	BLK_INTEGRITY_CSUM_NONE		= 0,
  84:./include/linux/blkdev.h **** 	BLK_INTEGRITY_CSUM_IP		= 1,
  85:./include/linux/blkdev.h **** 	BLK_INTEGRITY_CSUM_CRC		= 2,
  86:./include/linux/blkdev.h **** 	BLK_INTEGRITY_CSUM_CRC64	= 3,
  87:./include/linux/blkdev.h **** } __packed ;
  88:./include/linux/blkdev.h **** 
  89:./include/linux/blkdev.h **** struct blk_integrity {
ARM GAS  /tmp/cc0b6Xiw.s 			page 10


  90:./include/linux/blkdev.h **** 	unsigned char				flags;
  91:./include/linux/blkdev.h **** 	enum blk_integrity_checksum		csum_type;
  92:./include/linux/blkdev.h **** 	unsigned char				tuple_size;
  93:./include/linux/blkdev.h **** 	unsigned char				pi_offset;
  94:./include/linux/blkdev.h **** 	unsigned char				interval_exp;
  95:./include/linux/blkdev.h **** 	unsigned char				tag_size;
  96:./include/linux/blkdev.h **** };
  97:./include/linux/blkdev.h **** 
  98:./include/linux/blkdev.h **** 
  99:./include/linux/blkdev.h **** typedef unsigned int __bitwise blk_mode_t;
 100:./include/linux/blkdev.h **** 
 101:./include/linux/blkdev.h **** /* open for reading */
 102:./include/linux/blkdev.h **** #define BLK_OPEN_READ		((__force blk_mode_t)(1 << 0))
 103:./include/linux/blkdev.h **** /* open for writing */
 104:./include/linux/blkdev.h **** #define BLK_OPEN_WRITE		((__force blk_mode_t)(1 << 1))
 105:./include/linux/blkdev.h **** /* open exclusively (vs other exclusive openers */
 106:./include/linux/blkdev.h **** #define BLK_OPEN_EXCL		((__force blk_mode_t)(1 << 2))
 107:./include/linux/blkdev.h **** /* opened with O_NDELAY */
 108:./include/linux/blkdev.h **** #define BLK_OPEN_NDELAY		((__force blk_mode_t)(1 << 3))
 109:./include/linux/blkdev.h **** /* open for "writes" only for ioctls (specialy hack for floppy.c) */
 110:./include/linux/blkdev.h **** #define BLK_OPEN_WRITE_IOCTL	((__force blk_mode_t)(1 << 4))
 111:./include/linux/blkdev.h **** /* open is exclusive wrt all other BLK_OPEN_WRITE opens to the device */
 112:./include/linux/blkdev.h **** #define BLK_OPEN_RESTRICT_WRITES	((__force blk_mode_t)(1 << 5))
 113:./include/linux/blkdev.h **** /* return partition scanning errors */
 114:./include/linux/blkdev.h **** #define BLK_OPEN_STRICT_SCAN	((__force blk_mode_t)(1 << 6))
 115:./include/linux/blkdev.h **** 
 116:./include/linux/blkdev.h **** 
 117:./include/linux/blkdev.h **** enum block_device_flags_t
 118:./include/linux/blkdev.h **** {
 119:./include/linux/blkdev.h **** 	BLOCK_DEVICE_FLAG_NOT_INITIALIZED,       //æœªåˆå§‹åŒ–å­˜å‚¨è®¾å¤‡
 120:./include/linux/blkdev.h ****     BLOCK_DEVICE_FLAG_MBR,                   //å•mbråˆ†åŒº
 121:./include/linux/blkdev.h ****     BLOCK_DEVICE_FLAG_GPT,                   //å•gptåˆ†åŒº
 122:./include/linux/blkdev.h ****     BLOCK_DEVICE_FLAG_PROTECTIVE_MBR ,       //ä¿æŠ¤MBRåˆ†åŒº(gptåˆ†åŒº,ä¸ºäº†å…¼å®¹æ—§çš„æ“MBRåˆ
 123:./include/linux/blkdev.h ****     BLOCK_DEVICE_FLAG_BROKEN_MBR,            //æŸåçš„MBRåˆ†åŒº
 124:./include/linux/blkdev.h ****     BLOCK_DEVICE_FLAG_BROKEN_GPT,            //æŸåçš„GPTåˆ†åŒº
 125:./include/linux/blkdev.h ****     BLOCK_DEVICE_FLAG_BROKEN_PROTECTIVE_MBR, //æŸåçš„ä¿æŠ¤MBRåˆ†åŒº
 126:./include/linux/blkdev.h **** };
 127:./include/linux/blkdev.h **** 
 128:./include/linux/blkdev.h **** static const char *block_device_flag_to_string(enum block_device_flags_t flag) {
 129:./include/linux/blkdev.h ****     switch (flag) {
 130:./include/linux/blkdev.h ****         case BLOCK_DEVICE_FLAG_MBR:
 131:./include/linux/blkdev.h ****             return "BLOCK_DEVICE_FLAG_MBR";
 132:./include/linux/blkdev.h ****         case BLOCK_DEVICE_FLAG_GPT:
 133:./include/linux/blkdev.h ****             return "BLOCK_DEVICE_FLAG_GPT";
 134:./include/linux/blkdev.h ****         case BLOCK_DEVICE_FLAG_PROTECTIVE_MBR:
 135:./include/linux/blkdev.h ****             return "BLOCK_DEVICE_FLAG_PROTECTIVE_MBR";
 136:./include/linux/blkdev.h ****         case BLOCK_DEVICE_FLAG_NOT_INITIALIZED:
 137:./include/linux/blkdev.h ****             return "BLOCK_DEVICE_FLAG_NOT_INITIALIZED";
 138:./include/linux/blkdev.h ****         case BLOCK_DEVICE_FLAG_BROKEN_MBR:
 139:./include/linux/blkdev.h ****             return "BLOCK_DEVICE_FLAG_BROKEN_MBR";
 140:./include/linux/blkdev.h ****         case BLOCK_DEVICE_FLAG_BROKEN_GPT:
 141:./include/linux/blkdev.h ****             return "BLOCK_DEVICE_FLAG_BROKEN_GPT";
 142:./include/linux/blkdev.h ****         case BLOCK_DEVICE_FLAG_BROKEN_PROTECTIVE_MBR:
 143:./include/linux/blkdev.h ****             return "BLOCK_DEVICE_FLAG_BROKEN_PROTECTIVE_MBR";
 144:./include/linux/blkdev.h ****         default:
 145:./include/linux/blkdev.h ****             return "Unknown flag";
 146:./include/linux/blkdev.h ****     }
ARM GAS  /tmp/cc0b6Xiw.s 			page 11


 147:./include/linux/blkdev.h **** }
 148:./include/linux/blkdev.h **** 
 149:./include/linux/blkdev.h **** 
 150:./include/linux/blkdev.h **** 
 151:./include/linux/blkdev.h **** struct gpt_header {
 152:./include/linux/blkdev.h ****     uint64_t signature;             // å¿…é¡»æ˜¯ "EFI PART"
 153:./include/linux/blkdev.h ****     uint32_t revision;              // ä¿®è®¢ç‰ˆæœ¬ï¼Œé€šå¸¸ä¸º0x00010000
 154:./include/linux/blkdev.h ****     uint32_t header_size;           // å¤´éƒ¨å¤§å°ï¼ˆé€šå¸¸ä¸º92å­—èŠ‚ï¼‰
 155:./include/linux/blkdev.h ****     uint32_t header_crc32;          // å¤´éƒ¨çš„CRC32æ ¡éªŒå’Œ
 156:./include/linux/blkdev.h ****     uint32_t reserved;              // ä¿ç•™å­—æ®µ
 157:./include/linux/blkdev.h ****     uint64_t my_lba;                // å½“å‰GPTè¡¨æ‰€åœ¨çš„LBA
 158:./include/linux/blkdev.h ****     uint64_t backup_lba;            // å¤‡ä»½GPTè¡¨æ‰€åœ¨çš„LBA
 159:./include/linux/blkdev.h ****     uint64_t first_usable_lba;      // ç¬¬ä¸€ä¸ªå¯ç”¨çš„LBAï¼ˆGPTåˆ†åŒºçš„å¼€å§‹ä½ç½®ï¼‰
 160:./include/linux/blkdev.h ****     uint64_t last_usable_lba;       // æœ€åŽä¸€ä¸ªå¯ç”¨çš„LBAï¼ˆGPTåˆ†åŒºçš„ç»“æŸä½ç½®ï¼‰
 161:./include/linux/blkdev.h ****     uint8_t disk_guid[16];          // ç£ç›˜çš„GUID
 162:./include/linux/blkdev.h ****     uint64_t partition_entry_start_lba; // åˆ†åŒºè¡¨çš„èµ·å§‹LBA
 163:./include/linux/blkdev.h ****     uint32_t partition_entry_count; // åˆ†åŒºè¡¨é¡¹çš„æ•°é‡    
 164:./include/linux/blkdev.h ****     uint32_t partition_entry_size;  // æ¯ä¸ªåˆ†åŒºè¡¨é¡¹çš„å¤§å°
 165:./include/linux/blkdev.h ****     uint32_t partition_entry_crc32; // åˆ†åŒºè¡¨é¡¹çš„CRC32æ ¡éªŒå’Œ
 166:./include/linux/blkdev.h **** } __attribute__((packed));
 167:./include/linux/blkdev.h **** 
 168:./include/linux/blkdev.h **** 
 169:./include/linux/blkdev.h **** struct gpt_partition_entry {
 170:./include/linux/blkdev.h ****     uint8_t partition_type_guid[16];   // åˆ†åŒºç±»åž‹çš„GUID
 171:./include/linux/blkdev.h ****     uint8_t unique_partition_guid[16]; // åˆ†åŒºçš„å”¯ä¸€GUID
 172:./include/linux/blkdev.h ****     uint64_t starting_lba;             // åˆ†åŒºçš„èµ·å§‹LBA
 173:./include/linux/blkdev.h ****     uint64_t ending_lba;               // åˆ†åŒºçš„ç»“æŸLBA
 174:./include/linux/blkdev.h ****     uint64_t attributes;               // åˆ†åŒºå±žæ€§
 175:./include/linux/blkdev.h ****     uint16_t partition_name[36];       // åˆ†åŒºåç§°ï¼ˆUTF-16ç¼–ç ï¼‰
 176:./include/linux/blkdev.h **** }__attribute__((packed));
 177:./include/linux/blkdev.h **** 
 178:./include/linux/blkdev.h **** 
 179:./include/linux/blkdev.h **** struct gpt_partition {
 180:./include/linux/blkdev.h ****     struct gpt_header header;
 181:./include/linux/blkdev.h ****     struct gpt_partition_entry entries[8];
 182:./include/linux/blkdev.h **** };
 183:./include/linux/blkdev.h **** 
 184:./include/linux/blkdev.h **** 
 185:./include/linux/blkdev.h **** struct mbr_partition {
 186:./include/linux/blkdev.h ****     uint8_t boot_ind;   // å¯åŠ¨æ ‡å¿—ï¼ˆ0x80è¡¨ç¤ºæ´»åŠ¨åˆ†åŒºï¼‰
 187:./include/linux/blkdev.h ****     
 188:./include/linux/blkdev.h ****     uint8_t start_head; 
 189:./include/linux/blkdev.h ****     uint8_t start_sector; 
 190:./include/linux/blkdev.h ****     uint8_t start_cyl;  
 191:./include/linux/blkdev.h **** 
 192:./include/linux/blkdev.h ****     uint8_t sys_ind;    // ç³»ç»Ÿid(åˆ†åŒºç±»åž‹)
 193:./include/linux/blkdev.h **** 
 194:./include/linux/blkdev.h ****     uint8_t end_head;   
 195:./include/linux/blkdev.h ****     uint8_t end_sector; 
 196:./include/linux/blkdev.h ****     uint8_t end_cyl;    
 197:./include/linux/blkdev.h ****     
 198:./include/linux/blkdev.h ****     uint32_t start_lba; // åˆ†åŒºçš„èµ·å§‹åœ°å€
 199:./include/linux/blkdev.h ****     
 200:./include/linux/blkdev.h ****     uint32_t nr_sectors; // åˆ†åŒºå¤§å°
 201:./include/linux/blkdev.h **** }__attribute__((packed));
 202:./include/linux/blkdev.h **** 
 203:./include/linux/blkdev.h **** 
ARM GAS  /tmp/cc0b6Xiw.s 			page 12


 204:./include/linux/blkdev.h **** struct partition {
 205:./include/linux/blkdev.h **** 	struct gpt_partition gpt_partition;
 206:./include/linux/blkdev.h **** 	struct mbr_partition mbr_partition[4];   
 207:./include/linux/blkdev.h **** };
 208:./include/linux/blkdev.h **** 
 209:./include/linux/blkdev.h **** 
 210:./include/linux/blkdev.h **** 
 211:./include/linux/blkdev.h **** 
 212:./include/linux/blkdev.h **** 
 213:./include/linux/blkdev.h **** struct gendisk {
 214:./include/linux/blkdev.h **** 	int major;
 215:./include/linux/blkdev.h **** 	int first_minor;
 216:./include/linux/blkdev.h **** 	int minors;
 217:./include/linux/blkdev.h **** 	char disk_name[DISK_NAME_LEN];	/* name of major driver */
 218:./include/linux/blkdev.h **** 	unsigned short events;		/* supported events */
 219:./include/linux/blkdev.h **** 	unsigned short event_flags;	/* flags related to event processing */
 220:./include/linux/blkdev.h **** 	struct xarray part_tbl;
 221:./include/linux/blkdev.h **** 	struct block_device *part0;
 222:./include/linux/blkdev.h **** 	const struct block_device_operations *fops; 
 223:./include/linux/blkdev.h ****  	struct request_queue *queue;
 224:./include/linux/blkdev.h **** 	void *private_data;
 225:./include/linux/blkdev.h **** 
 226:./include/linux/blkdev.h **** //	struct bio_set bio_split;    /*ä¸ºèŠ‚çº¦å†…å­˜ä¸ä½¿ç”¨*/
 227:./include/linux/blkdev.h **** 
 228:./include/linux/blkdev.h **** 	int flags;
 229:./include/linux/blkdev.h **** 	unsigned long state;
 230:./include/linux/blkdev.h **** 
 231:./include/linux/blkdev.h **** #define GD_NEED_PART_SCAN		0
 232:./include/linux/blkdev.h **** #define GD_READ_ONLY			1
 233:./include/linux/blkdev.h **** #define GD_DEAD				2
 234:./include/linux/blkdev.h **** #define GD_NATIVE_CAPACITY		3
 235:./include/linux/blkdev.h **** #define GD_ADDED			4
 236:./include/linux/blkdev.h **** #define GD_SUPPRESS_PART_SCAN		5
 237:./include/linux/blkdev.h **** #define GD_OWNS_QUEUE			6
 238:./include/linux/blkdev.h **** 
 239:./include/linux/blkdev.h **** 	struct mutex open_mutex;	/* open/close mutex */
 240:./include/linux/blkdev.h **** 	unsigned open_partitions;	/* number of open partitions */   
 241:./include/linux/blkdev.h **** //	struct backing_dev_info	*bdi;
 242:./include/linux/blkdev.h **** //	struct kobject queue_kobj;	/* the queue/ directory */
 243:./include/linux/blkdev.h **** // 	struct kobject *slave_dir;
 244:./include/linux/blkdev.h **** #ifdef CONFIG_BLOCK_HOLDER_DEPRECATED
 245:./include/linux/blkdev.h **** 	  struct list_head slave_bdevs;
 246:./include/linux/blkdev.h **** #endif
 247:./include/linux/blkdev.h **** 	int node_id;
 248:./include/linux/blkdev.h **** //	struct badblocks *bb;
 249:./include/linux/blkdev.h **** 	struct lockdep_map lockdep_map;
 250:./include/linux/blkdev.h **** 	u64 diskseq;
 251:./include/linux/blkdev.h **** 	blk_mode_t open_mode;
 252:./include/linux/blkdev.h **** 	struct blk_independent_access_ranges *ia_ranges;
 253:./include/linux/blkdev.h **** };
 254:./include/linux/blkdev.h **** 
 255:./include/linux/blkdev.h **** 
 256:./include/linux/blkdev.h **** 
 257:./include/linux/blkdev.h **** 
 258:./include/linux/blkdev.h **** typedef unsigned int __bitwise blk_features_t;
 259:./include/linux/blkdev.h **** /* internal flags in queue_limits.flags */
 260:./include/linux/blkdev.h **** typedef unsigned int __bitwise blk_flags_t;
ARM GAS  /tmp/cc0b6Xiw.s 			page 13


 261:./include/linux/blkdev.h **** 
 262:./include/linux/blkdev.h **** struct queue_limits {
 263:./include/linux/blkdev.h **** 	blk_features_t		features;
 264:./include/linux/blkdev.h **** 	blk_flags_t		flags;
 265:./include/linux/blkdev.h **** 	unsigned long		seg_boundary_mask;
 266:./include/linux/blkdev.h **** 	unsigned long		virt_boundary_mask;
 267:./include/linux/blkdev.h **** 
 268:./include/linux/blkdev.h **** 	unsigned int		max_hw_sectors;
 269:./include/linux/blkdev.h **** 	unsigned int		max_dev_sectors;
 270:./include/linux/blkdev.h **** 	unsigned int		chunk_sectors;
 271:./include/linux/blkdev.h **** 	unsigned int		max_sectors;
 272:./include/linux/blkdev.h **** 	unsigned int		max_user_sectors;
 273:./include/linux/blkdev.h **** 	unsigned int		max_segment_size;
 274:./include/linux/blkdev.h **** 	unsigned int		physical_block_size;
 275:./include/linux/blkdev.h **** 	unsigned int		logical_block_size;
 276:./include/linux/blkdev.h **** 	unsigned int		alignment_offset;
 277:./include/linux/blkdev.h **** 	unsigned int		io_min;
 278:./include/linux/blkdev.h **** 	unsigned int		io_opt;
 279:./include/linux/blkdev.h **** 	unsigned int		max_discard_sectors;
 280:./include/linux/blkdev.h **** 	unsigned int		max_hw_discard_sectors;
 281:./include/linux/blkdev.h **** 	unsigned int		max_user_discard_sectors;
 282:./include/linux/blkdev.h **** 	unsigned int		max_secure_erase_sectors;
 283:./include/linux/blkdev.h **** 	unsigned int		max_write_zeroes_sectors;
 284:./include/linux/blkdev.h **** 	unsigned int		max_hw_zone_append_sectors;
 285:./include/linux/blkdev.h **** 	unsigned int		max_zone_append_sectors;
 286:./include/linux/blkdev.h **** 	unsigned int		discard_granularity;
 287:./include/linux/blkdev.h **** 	unsigned int		discard_alignment;
 288:./include/linux/blkdev.h **** 	unsigned int		zone_write_granularity;
 289:./include/linux/blkdev.h **** 
 290:./include/linux/blkdev.h **** 	/* atomic write limits */
 291:./include/linux/blkdev.h **** 	unsigned int		atomic_write_hw_max;
 292:./include/linux/blkdev.h **** 	unsigned int		atomic_write_max_sectors;
 293:./include/linux/blkdev.h **** 	unsigned int		atomic_write_hw_boundary;
 294:./include/linux/blkdev.h **** 	unsigned int		atomic_write_boundary_sectors;
 295:./include/linux/blkdev.h **** 	unsigned int		atomic_write_hw_unit_min;
 296:./include/linux/blkdev.h **** 	unsigned int		atomic_write_unit_min;
 297:./include/linux/blkdev.h **** 	unsigned int		atomic_write_hw_unit_max;
 298:./include/linux/blkdev.h **** 	unsigned int		atomic_write_unit_max;
 299:./include/linux/blkdev.h **** 
 300:./include/linux/blkdev.h **** 	unsigned short		max_segments;
 301:./include/linux/blkdev.h **** 	unsigned short		max_integrity_segments;
 302:./include/linux/blkdev.h **** 	unsigned short		max_discard_segments;
 303:./include/linux/blkdev.h **** 
 304:./include/linux/blkdev.h **** 	unsigned int		max_open_zones;
 305:./include/linux/blkdev.h **** 	unsigned int		max_active_zones;
 306:./include/linux/blkdev.h **** 
 307:./include/linux/blkdev.h **** 	/*
 308:./include/linux/blkdev.h **** 	 * Drivers that set dma_alignment to less than 511 must be prepared to
 309:./include/linux/blkdev.h **** 	 * handle individual bvec's that are not a multiple of a SECTOR_SIZE
 310:./include/linux/blkdev.h **** 	 * due to possible offsets.
 311:./include/linux/blkdev.h **** 	 */
 312:./include/linux/blkdev.h **** 	unsigned int		dma_alignment;
 313:./include/linux/blkdev.h **** 	unsigned int		dma_pad_mask;
 314:./include/linux/blkdev.h **** 
 315:./include/linux/blkdev.h **** 	struct blk_integrity	integrity;
 316:./include/linux/blkdev.h **** };
 317:./include/linux/blkdev.h **** 
ARM GAS  /tmp/cc0b6Xiw.s 			page 14


 318:./include/linux/blkdev.h **** 
 319:./include/linux/blkdev.h **** 
 320:./include/linux/blkdev.h **** typedef int (*report_zones_cb)(struct blk_zone *zone, unsigned int idx,
 321:./include/linux/blkdev.h ****     void *data);
 322:./include/linux/blkdev.h **** 
 323:./include/linux/blkdev.h **** 
 324:./include/linux/blkdev.h **** 
 325:./include/linux/blkdev.h **** // struct request_queue {
 326:./include/linux/blkdev.h **** 
 327:./include/linux/blkdev.h **** // 	void			*queuedata;           //å­˜å‚¨ç§æœ‰æ•°æ®
 328:./include/linux/blkdev.h **** // 	struct elevator_queue	*elevator;   //æŒ‡å‘ioè°ƒåº¦å™¨çš„å®žçŽ°
 329:./include/linux/blkdev.h **** // 	const struct blk_mq_ops	*mq_ops;     //æŒ‡å‘å—è®¾å¤‡å¤šé˜Ÿåˆ—
 330:./include/linux/blkdev.h **** // 	struct blk_mq_ctx __percpu	*queue_ctx; //æŒ‡å‘æ¯ä¸ª CPU çš„ä¸Šä¸‹æ–‡ç»“æž„
 331:./include/linux/blkdev.h **** // 	unsigned long		queue_flags;         //é˜Ÿåˆ—çš„æ ‡å¿—ä½
 332:./include/linux/blkdev.h **** // 	unsigned int		rq_timeout;        //è¯·æ±‚è¶…æ—¶æ—¶é—´
 333:./include/linux/blkdev.h **** // 	unsigned int		queue_depth;       //é˜Ÿåˆ—æ·±åº¦
 334:./include/linux/blkdev.h **** // 	refcount_t		refs;              //å¼•ç”¨è®¡æ•°
 335:./include/linux/blkdev.h **** // 	unsigned int		nr_hw_queues;  //ç¡¬ä»¶é˜Ÿåˆ—æ•°é‡
 336:./include/linux/blkdev.h **** // 	struct xarray		hctx_table;     //ç¡¬ä»¶ä¸Šä¸‹æ–‡ï¼ˆhardware contextï¼‰çš„ç´¢å¼•è¡¨ã€‚
 337:./include/linux/blkdev.h **** // 	struct percpu_ref	q_usage_counter; //é˜Ÿåˆ—ä½¿ç”¨è®¡æ•°å™¨ã€‚
 338:./include/linux/blkdev.h **** // 	struct lock_class_key	io_lock_cls_key; //I/O é”çš„è°ƒè¯•ä¿¡æ¯ã€‚
 339:./include/linux/blkdev.h **** // 	struct lockdep_map	io_lockdep_map;     
 340:./include/linux/blkdev.h **** // 	struct lock_class_key	q_lock_cls_key;   //é˜Ÿåˆ—é”çš„è°ƒè¯•ä¿¡æ¯ã€‚
 341:./include/linux/blkdev.h **** // 	struct lockdep_map	q_lockdep_map;
 342:./include/linux/blkdev.h **** // 	struct request		*last_merge;          //ä¸Šä¸€æ¬¡åˆå¹¶çš„è¯·æ±‚ã€‚
 343:./include/linux/blkdev.h **** // 	spinlock_t		queue_lock;				  //é˜Ÿåˆ—çš„è‡ªæ—‹é”ã€‚
 344:./include/linux/blkdev.h **** // 	int			quiesce_depth;			     //é™æ­¢ï¼ˆquiesceï¼‰æ·±åº¦
 345:./include/linux/blkdev.h **** // 	struct gendisk		*disk;				//æŒ‡å‘å…³è”çš„ç£ç›˜è®¾å¤‡ã€‚
 346:./include/linux/blkdev.h **** // 	struct kobject *mq_kobj; //å¤šé˜Ÿåˆ—çš„å†…æ ¸å¯¹è±¡ã€‚ç”¨äºŽ sysfs æ–‡ä»¶ç³»ç»Ÿï¼Œå°†é˜Ÿåˆ—çš„å
 347:./include/linux/blkdev.h **** // 	struct queue_limits	limits;//é˜Ÿåˆ—çš„é™åˆ¶å‚æ•°ã€‚å®šä¹‰äº†é˜Ÿåˆ—çš„ç‰©ç†å’Œé€»è¾‘é™åˆ¶ï¼ˆ
 348:./include/linux/blkdev.h **** // 	atomic_t		pm_only;     //ç”µæºç®¡ç†ç›¸å…³çš„æ ‡å¿—
 349:./include/linux/blkdev.h **** // 	struct blk_queue_stats	*stats; //é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
 350:./include/linux/blkdev.h **** // 	struct rq_qos		*rq_qos;         //è¯·æ±‚æœåŠ¡è´¨é‡ï¼ˆQoSï¼‰ç»“æž„åŠå…¶äº’æ–¥é”ã€‚
 351:./include/linux/blkdev.h **** // 	struct mutex		rq_qos_mutex;
 352:./include/linux/blkdev.h **** // 	int			id;							//é˜Ÿåˆ—çš„å”¯ä¸€æ ‡è¯†ç¬¦
 353:./include/linux/blkdev.h **** // 	unsigned long		nr_requests;	    //é˜Ÿåˆ—æ”¯æŒçš„æœ€å¤§è¯·æ±‚æ•°ã€‚
 354:./include/linux/blkdev.h **** // 	struct timer_list	timeout;			//è¶…æ—¶è®¡æ—¶å™¨å’Œå·¥ä½œé˜Ÿåˆ—ã€‚
 355:./include/linux/blkdev.h **** // 	struct work_struct	timeout_work;
 356:./include/linux/blkdev.h **** // 	atomic_t		nr_active_requests_shared_tags; //å…±äº«æ ‡ç­¾çš„æ´»è·ƒè¯·æ±‚æ•°
 357:./include/linux/blkdev.h **** // 	struct blk_mq_tags	*sched_shared_tags;           //è°ƒåº¦å™¨çš„å…±äº«æ ‡ç­¾ã€‚
 358:./include/linux/blkdev.h **** // 	struct list_head	icq_list;          //I/O ä¸Šä¸‹æ–‡é˜Ÿåˆ—é“¾è¡¨
 359:./include/linux/blkdev.h **** // 	int			node;					//NUMA èŠ‚ç‚¹ IDã€‚
 360:./include/linux/blkdev.h **** // 	spinlock_t		requeue_lock;    //é‡æŽ’é˜Ÿåˆ—çš„é”ã€é“¾è¡¨å’Œå·¥ä½œé˜Ÿåˆ—ã€‚
 361:./include/linux/blkdev.h **** // 	struct list_head	requeue_list;
 362:./include/linux/blkdev.h **** //     struct delayed_work	requeue_work;
 363:./include/linux/blkdev.h **** // 	struct blk_flush_queue	*fq;        //åˆ·æ–°é˜Ÿåˆ—åŠå…¶é“¾è¡¨
 364:./include/linux/blkdev.h **** // 	struct list_head	flush_list;
 365:./include/linux/blkdev.h **** // 	struct mutex		sysfs_lock;          //sysfs å’Œé™åˆ¶å‚æ•°çš„äº’æ–¥é”ã€‚
 366:./include/linux/blkdev.h **** // 	struct mutex		sysfs_dir_lock;
 367:./include/linux/blkdev.h **** // 	struct mutex		limits_lock;
 368:./include/linux/blkdev.h **** // 	struct list_head	unused_hctx_list;     //æœªä½¿ç”¨çš„ç¡¬ä»¶ä¸Šä¸‹æ–‡é“¾è¡¨åŠå…¶é”ã€‚
 369:./include/linux/blkdev.h **** // 	spinlock_t		unused_hctx_lock;
 370:./include/linux/blkdev.h **** // 	int			mq_freeze_depth;             //å¤šé˜Ÿåˆ—å†»ç»“æ·±åº¦ã€‚
 371:./include/linux/blkdev.h **** // 	struct rcu_head		rcu_head;           //RCUï¼ˆRead-Copy-Updateï¼‰é‡Šæ”¾é’©å­ã€‚
 372:./include/linux/blkdev.h **** // 	wait_queue_head_t	mq_freeze_wq;         //å¤šé˜Ÿåˆ—å†»ç»“çš„ç­‰å¾…é˜Ÿåˆ—å’Œé”ã€‚
 373:./include/linux/blkdev.h **** // 	struct mutex		mq_freeze_lock; 
 374:./include/linux/blkdev.h **** // 	struct blk_mq_tag_set	*tag_set;       //æ ‡ç­¾é›†åŠå…¶é“¾è¡¨ã€‚
ARM GAS  /tmp/cc0b6Xiw.s 			page 15


 375:./include/linux/blkdev.h **** // 	struct list_head	tag_set_list;
 376:./include/linux/blkdev.h **** // 	struct dentry		*debugfs_dir;       //debugfs æ–‡ä»¶ç³»ç»Ÿçš„ç›®å½•é¡¹ã€‚
 377:./include/linux/blkdev.h **** // 	struct dentry		*sched_debugfs_dir;
 378:./include/linux/blkdev.h **** // 	struct dentry		*rqos_debugfs_dir;
 379:./include/linux/blkdev.h **** // 	struct mutex		debugfs_mutex;     //debugfs æ“ä½œçš„äº’æ–¥é”ã€‚
 380:./include/linux/blkdev.h **** // 	bool			mq_sysfs_init_done;    //å¤šé˜Ÿåˆ— sysfs åˆå§‹åŒ–å®Œæˆæ ‡å¿—ã€‚
 381:./include/linux/blkdev.h **** // };
 382:./include/linux/blkdev.h **** 
 383:./include/linux/blkdev.h **** 
 384:./include/linux/blkdev.h **** typedef void(*request_fn_t)(struct request *req);
 385:./include/linux/blkdev.h **** 
 386:./include/linux/blkdev.h **** struct request_queue {
 387:./include/linux/blkdev.h **** 
 388:./include/linux/blkdev.h **** 	void			*queuedata;             //å­˜å‚¨ç§æœ‰æ•°æ®
 389:./include/linux/blkdev.h **** 	struct request		*last_merge;        //ä¸Šä¸€æ¬¡åˆå¹¶çš„è¯·æ±‚ã€‚
 390:./include/linux/blkdev.h **** 	spinlock_t		queue_lock;				//é˜Ÿåˆ—çš„è‡ªæ—‹é”ã€‚
 391:./include/linux/blkdev.h **** 	int			quiesce_depth;			    //é™æ­¢ï¼ˆquiesceï¼‰æ·±åº¦
 392:./include/linux/blkdev.h **** 	struct gendisk		*disk;				//æŒ‡å‘å…³è”çš„ç£ç›˜è®¾å¤‡ã€‚
 393:./include/linux/blkdev.h **** 	struct queue_limits	limits;             //é˜Ÿåˆ—çš„é™åˆ¶å‚æ•°ã€‚å®šä¹‰äº†é˜Ÿåˆ—çš„ç‰©ç†å’Œé€»è¾
 394:./include/linux/blkdev.h **** 	int			id;							//é˜Ÿåˆ—çš„å”¯ä¸€æ ‡è¯†ç¬¦
 395:./include/linux/blkdev.h **** 	unsigned long		nr_requests;	    //é˜Ÿåˆ—æ”¯æŒçš„æœ€å¤§è¯·æ±‚æ•°ã€‚
 396:./include/linux/blkdev.h **** 	struct list_head	icq_list;           //I/O ä¸Šä¸‹æ–‡é˜Ÿåˆ—é“¾è¡¨
 397:./include/linux/blkdev.h **** 	int			node;					    //NUMA èŠ‚ç‚¹ IDã€‚
 398:./include/linux/blkdev.h **** 	request_fn_t q_fn;                      //å•é˜Ÿåˆ—è®¾å¤‡æ“ä½œ
 399:./include/linux/blkdev.h **** 	spinlock_t*  q_lock;
 400:./include/linux/blkdev.h **** };
 401:./include/linux/blkdev.h **** 
 402:./include/linux/blkdev.h **** 
 403:./include/linux/blkdev.h **** 
 404:./include/linux/blkdev.h **** 
 405:./include/linux/blkdev.h **** static inline dev_t disk_devt(struct gendisk *disk)
 406:./include/linux/blkdev.h **** {
 407:./include/linux/blkdev.h **** 	return MKDEV(disk->major, disk->first_minor);
 408:./include/linux/blkdev.h **** }
 409:./include/linux/blkdev.h **** 
 410:./include/linux/blkdev.h **** 
 411:./include/linux/blkdev.h **** /* blk_validate_limits() validates bsize, so drivers don't usually need to */
 412:./include/linux/blkdev.h **** static inline int blk_validate_block_size(unsigned long bsize)
 413:./include/linux/blkdev.h **** {
 414:./include/linux/blkdev.h **** 	if (bsize < 512 || bsize > PAGE_SIZE || !is_power_of_2(bsize))
 415:./include/linux/blkdev.h **** 		return -EINVAL;
 416:./include/linux/blkdev.h **** 
 417:./include/linux/blkdev.h **** 	return 0;
 418:./include/linux/blkdev.h **** }
 419:./include/linux/blkdev.h **** 
 420:./include/linux/blkdev.h **** // static inline bool blk_op_is_passthrough(blk_opf_t op)
 421:./include/linux/blkdev.h **** // {
 422:./include/linux/blkdev.h **** // 	op &= REQ_OP_MASK;
 423:./include/linux/blkdev.h **** // 	return op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;
 424:./include/linux/blkdev.h **** // }
 425:./include/linux/blkdev.h **** 
 426:./include/linux/blkdev.h **** 
 427:./include/linux/blkdev.h **** 
 428:./include/linux/blkdev.h **** extern struct request_queue *request_queue_init(int id, struct gendisk *gd,gfp_t flags);
 429:./include/linux/blkdev.h **** 
 430:./include/linux/blkdev.h **** extern void request_queue_add(struct request_queue *q, struct request *req);
 431:./include/linux/blkdev.h **** 
ARM GAS  /tmp/cc0b6Xiw.s 			page 16


 432:./include/linux/blkdev.h **** extern void request_queue_remove(struct request_queue *q, struct request *req);
 433:./include/linux/blkdev.h **** 
 434:./include/linux/blkdev.h **** extern void process_requests_in_queue(struct request_queue *q);
 435:./include/linux/blkdev.h **** 
 436:./include/linux/blkdev.h **** extern struct request *blk_fetch_request(struct request_queue *q);
 437:./include/linux/blkdev.h **** 
 438:./include/linux/blkdev.h **** extern void __blk_insert_request(struct request *rq, struct bio *bio);
 439:./include/linux/blkdev.h **** 
 440:./include/linux/blkdev.h **** extern void __blk_cleanup_queue(struct request_queue *q);
 441:./include/linux/blkdev.h **** 
 442:./include/linux/blkdev.h **** static __always_inline struct request_queue *blk_alloc_queue(gfp_t flags){
 443:./include/linux/blkdev.h **** 	return request_queue_init(0, NULL, flags);
 444:./include/linux/blkdev.h **** }
 445:./include/linux/blkdev.h **** static __always_inline struct request_queue *blk_init_queue(request_fn_t *fn,spinlock_t *lock){
 446:./include/linux/blkdev.h **** 	struct request_queue * q= request_queue_init(0, NULL, GFP_KERNEL);
 447:./include/linux/blkdev.h **** 	q->q_fn = fn;
 448:./include/linux/blkdev.h **** 	q->q_lock = lock;
 449:./include/linux/blkdev.h **** 	return q;
 450:./include/linux/blkdev.h **** }
 451:./include/linux/blkdev.h **** static __always_inline void blk_mq_insert_request(struct request_queue *q, struct request *req){
 452:./include/linux/blkdev.h **** 	request_queue_add(q, req);
 453:./include/linux/blkdev.h **** }
 454:./include/linux/blkdev.h **** static __always_inline void blk_queue_make_request(struct request_queue *q, struct request *req){
 455:./include/linux/blkdev.h **** 	request_queue_add(q, req);
 456:./include/linux/blkdev.h **** }
 457:./include/linux/blkdev.h **** static __always_inline void blk_mq_submit_bio(struct request *rq, struct bio *bio){  
 458:./include/linux/blkdev.h **** 	__blk_insert_request(rq, bio);
 459:./include/linux/blkdev.h **** }
 460:./include/linux/blkdev.h **** static __always_inline void blk_insert_request(struct request_queue *q, struct request *req){
 461:./include/linux/blkdev.h **** 	request_queue_add(q, req);
 462:./include/linux/blkdev.h **** }
 463:./include/linux/blkdev.h **** static __always_inline void blk_submit_bio(struct request *rq, struct bio *bio){  
 464:./include/linux/blkdev.h **** 	__blk_insert_request(rq, bio);
 465:./include/linux/blkdev.h **** }
 466:./include/linux/blkdev.h **** 
 467:./include/linux/blkdev.h **** static __always_inline void blk_mq_end_request(struct request *rq,blk_status_t error){
 468:./include/linux/blkdev.h **** 	request_queue_remove(rq->q,rq);
 469:./include/linux/blkdev.h **** }
 470:./include/linux/blkdev.h **** 
 471:./include/linux/blkdev.h **** static __always_inline void end_request(struct request *rq){
 472:./include/linux/blkdev.h **** 	request_queue_remove(rq->q,rq);
 473:./include/linux/blkdev.h **** }
 474:./include/linux/blkdev.h **** static void blk_cleanup_queue(struct request_queue *q){
 475:./include/linux/blkdev.h **** 	__blk_cleanup_queue(q);
 476:./include/linux/blkdev.h **** }
 477:./include/linux/blkdev.h **** 
 478:./include/linux/blkdev.h **** extern void __put_disk(struct gendisk *disk);
 479:./include/linux/blkdev.h **** 
 480:./include/linux/blkdev.h **** static void put_disk(struct gendisk *disk){
 481:./include/linux/blkdev.h **** 	__put_disk(disk);
 482:./include/linux/blkdev.h **** }
 483:./include/linux/blkdev.h **** 
 484:./include/linux/blkdev.h **** #define wait_for_completion(x) 
 485:./include/linux/blkdev.h **** 
 486:./include/linux/blkdev.h **** struct io_comp_batch {
 487:./include/linux/blkdev.h **** 	struct list_head req_list;
 488:./include/linux/blkdev.h **** 	bool need_ts;
ARM GAS  /tmp/cc0b6Xiw.s 			page 17


 489:./include/linux/blkdev.h **** 	void (*complete)(struct io_comp_batch *);
 490:./include/linux/blkdev.h **** };
 491:./include/linux/blkdev.h **** 
 492:./include/linux/blkdev.h **** 
 493:./include/linux/blkdev.h **** enum blk_unique_id {
 494:./include/linux/blkdev.h **** 	/* these match the Designator Types specified in SPC */
 495:./include/linux/blkdev.h **** 	BLK_UID_T10	= 1,
 496:./include/linux/blkdev.h **** 	BLK_UID_EUI64	= 2,
 497:./include/linux/blkdev.h **** 	BLK_UID_NAA	= 3,
 498:./include/linux/blkdev.h **** };
 499:./include/linux/blkdev.h **** 
 500:./include/linux/blkdev.h **** 
 501:./include/linux/blkdev.h **** struct block_device_operations {
 502:./include/linux/blkdev.h **** 	void (*submit_bio)(struct bio *bio);
 503:./include/linux/blkdev.h **** 	int (*poll_bio)(struct bio *bio, struct io_comp_batch *iob,
 504:./include/linux/blkdev.h **** 			unsigned int flags);
 505:./include/linux/blkdev.h **** 	int (*open)(struct gendisk *disk, blk_mode_t mode);
 506:./include/linux/blkdev.h **** 	void (*release)(struct gendisk *disk);
 507:./include/linux/blkdev.h **** 	int (*ioctl)(struct block_device *bdev, blk_mode_t mode,
 508:./include/linux/blkdev.h **** 			unsigned cmd, unsigned long arg);
 509:./include/linux/blkdev.h **** 	int (*compat_ioctl)(struct block_device *bdev, blk_mode_t mode,
 510:./include/linux/blkdev.h **** 			unsigned cmd, unsigned long arg);
 511:./include/linux/blkdev.h **** 	unsigned int (*check_events) (struct gendisk *disk,
 512:./include/linux/blkdev.h **** 				      unsigned int clearing);
 513:./include/linux/blkdev.h **** 	void (*unlock_native_capacity) (struct gendisk *);
 514:./include/linux/blkdev.h **** 	int (*getgeo)(struct block_device *, struct hd_geometry *);
 515:./include/linux/blkdev.h **** 	int (*set_read_only)(struct block_device *bdev, bool ro);
 516:./include/linux/blkdev.h **** 	void (*free_disk)(struct gendisk *disk);
 517:./include/linux/blkdev.h **** 	/* this callback is with swap_lock and sometimes page table lock held */
 518:./include/linux/blkdev.h **** 	void (*swap_slot_free_notify) (struct block_device *, unsigned long);
 519:./include/linux/blkdev.h **** 	int (*report_zones)(struct gendisk *, sector_t sector,
 520:./include/linux/blkdev.h **** 			unsigned int nr_zones, report_zones_cb cb, void *data);
 521:./include/linux/blkdev.h **** 	char *(*devnode)(struct gendisk *disk, umode_t *mode);
 522:./include/linux/blkdev.h **** 	/* returns the length of the identifier or a negative errno: */
 523:./include/linux/blkdev.h **** 	int (*get_unique_id)(struct gendisk *disk, u8 id[16],
 524:./include/linux/blkdev.h **** 			enum blk_unique_id id_type);
 525:./include/linux/blkdev.h **** 	void *owner;
 526:./include/linux/blkdev.h **** 	const struct pr_ops *pr_ops;
 527:./include/linux/blkdev.h **** 
 528:./include/linux/blkdev.h **** 	/*
 529:./include/linux/blkdev.h **** 	 * Special callback for probing GPT entry at a given sector.
 530:./include/linux/blkdev.h **** 	 * Needed by Android devices, used by GPT scanner and MMC blk
 531:./include/linux/blkdev.h **** 	 * driver.
 532:./include/linux/blkdev.h **** 	 */
 533:./include/linux/blkdev.h **** 	int (*alternative_gpt_sector)(struct gendisk *disk, sector_t *sector);
 534:./include/linux/blkdev.h **** };
 535:./include/linux/blkdev.h **** 
 536:./include/linux/blkdev.h **** 
 537:./include/linux/blkdev.h **** extern struct gendisk *__gendisk_alloc(int major,int minors);
 538:./include/linux/blkdev.h **** 
 539:./include/linux/blkdev.h **** static inline struct gendisk *alloc_disk(int minors){
 540:./include/linux/blkdev.h **** 	return __gendisk_alloc(0, minors);
 541:./include/linux/blkdev.h **** }
 542:./include/linux/blkdev.h ****  
 543:./include/linux/blkdev.h **** extern void __put_disk(struct gendisk *disk);
 544:./include/linux/blkdev.h **** static void del_gendisk(struct gendisk *disk){
 545:./include/linux/blkdev.h **** 	__put_disk(disk);
ARM GAS  /tmp/cc0b6Xiw.s 			page 18


 546:./include/linux/blkdev.h **** }
 547:./include/linux/blkdev.h **** 
 548:./include/linux/blkdev.h **** 
 549:./include/linux/blkdev.h **** 
 550:./include/linux/blkdev.h **** 
 551:./include/linux/blkdev.h **** 
 552:./include/linux/blkdev.h **** 
 553:./include/linux/blkdev.h **** 
 554:./include/linux/blkdev.h **** static __always_inline struct request * blk_get_request(struct request_queue *q , unsigned int op, 
 225              		.loc 4 554 41 is_stmt 1 view .LVU45
 226              	.LBB81:
 555:./include/linux/blkdev.h **** {
 556:./include/linux/blkdev.h ****     if(!q) return NULL;
 227              		.loc 4 556 5 view .LVU46
 228              		.loc 4 556 7 is_stmt 0 view .LVU47
 229 000a 3DB1     		cbz	r5, .L16
 557:./include/linux/blkdev.h ****     struct request *rq =  request_alloc(q,gfp_mask, 1);
 230              		.loc 4 557 5 is_stmt 1 view .LVU48
 231              		.loc 4 557 27 is_stmt 0 view .LVU49
 232 000c 0122     		movs	r2, #1
 233 000e 4FF44C61 		mov	r1, #3264
 234 0012 2846     		mov	r0, r5
 235              	.LVL17:
 236              		.loc 4 557 27 view .LVU50
 237 0014 FFF7FEFF 		bl	request_alloc
 238              	.LVL18:
 239 0018 0546     		mov	r5, r0
 240              	.LVL19:
 558:./include/linux/blkdev.h ****     rq->cmd_flags = op;
 241              		.loc 4 558 5 is_stmt 1 view .LVU51
 242              		.loc 4 558 19 is_stmt 0 view .LVU52
 243 001a 4660     		str	r6, [r0, #4]
 559:./include/linux/blkdev.h ****     return rq;
 244              		.loc 4 559 5 is_stmt 1 view .LVU53
 245              	.LVL20:
 246              	.L16:
 247              		.loc 4 559 5 is_stmt 0 view .LVU54
 248              	.LBE81:
 249              	.LBE80:
 112:./block/bio.c ****     rq->__sector = bio->bi_iter.bi_sector + bio->bi_bdev->bd_start_sect;
 250              		.loc 1 112 5 is_stmt 1 view .LVU55
 251              		.loc 1 112 32 is_stmt 0 view .LVU56
 252 001c A369     		ldr	r3, [r4, #24]
 253              		.loc 1 112 48 view .LVU57
 254 001e 6268     		ldr	r2, [r4, #4]
 255              		.loc 1 112 57 view .LVU58
 256 0020 1268     		ldr	r2, [r2]
 257              		.loc 1 112 43 view .LVU59
 258 0022 1344     		add	r3, r3, r2
 259              		.loc 1 112 18 view .LVU60
 260 0024 EB61     		str	r3, [r5, #28]
 113:./block/bio.c ****     blk_submit_bio(rq,bio);   
 261              		.loc 1 113 5 is_stmt 1 view .LVU61
 262              	.LVL21:
 263              	.LBB82:
 264              	.LBI82:
 463:./include/linux/blkdev.h **** 	__blk_insert_request(rq, bio);
ARM GAS  /tmp/cc0b6Xiw.s 			page 19


 265              		.loc 4 463 29 view .LVU62
 266              	.LBB83:
 464:./include/linux/blkdev.h **** }
 267              		.loc 4 464 2 view .LVU63
 268 0026 2146     		mov	r1, r4
 269 0028 2846     		mov	r0, r5
 270 002a FFF7FEFF 		bl	__blk_insert_request
 271              	.LVL22:
 464:./include/linux/blkdev.h **** }
 272              		.loc 4 464 2 is_stmt 0 view .LVU64
 273              	.LBE83:
 274              	.LBE82:
 114:./block/bio.c ****     spin_lock(&bio->bi_bdev->bd_queue->queue_lock);
 275              		.loc 1 114 5 is_stmt 1 view .LVU65
 276              		.loc 1 114 19 is_stmt 0 view .LVU66
 277 002e 6368     		ldr	r3, [r4, #4]
 278              		.loc 1 114 28 view .LVU67
 279 0030 D868     		ldr	r0, [r3, #12]
 280              		.loc 1 114 5 view .LVU68
 281 0032 0830     		adds	r0, r0, #8
 282 0034 FFF7FEFF 		bl	spin_lock
 283              	.LVL23:
 115:./block/bio.c ****     blk_queue_make_request(bio->bi_bdev->bd_queue, rq); 
 284              		.loc 1 115 5 is_stmt 1 view .LVU69
 285              		.loc 1 115 31 is_stmt 0 view .LVU70
 286 0038 6368     		ldr	r3, [r4, #4]
 287              	.LVL24:
 288              	.LBB84:
 289              	.LBI84:
 454:./include/linux/blkdev.h **** 	request_queue_add(q, req);
 290              		.loc 4 454 29 is_stmt 1 view .LVU71
 291              	.LBB85:
 455:./include/linux/blkdev.h **** }
 292              		.loc 4 455 2 view .LVU72
 293 003a 2946     		mov	r1, r5
 294 003c D868     		ldr	r0, [r3, #12]
 295 003e FFF7FEFF 		bl	request_queue_add
 296              	.LVL25:
 455:./include/linux/blkdev.h **** }
 297              		.loc 4 455 2 is_stmt 0 view .LVU73
 298              	.LBE85:
 299              	.LBE84:
 116:./block/bio.c ****     spin_unlock(&bio->bi_bdev->bd_queue->queue_lock);
 300              		.loc 1 116 5 is_stmt 1 view .LVU74
 301              		.loc 1 116 21 is_stmt 0 view .LVU75
 302 0042 6368     		ldr	r3, [r4, #4]
 303              		.loc 1 116 30 view .LVU76
 304 0044 D868     		ldr	r0, [r3, #12]
 305              		.loc 1 116 5 view .LVU77
 306 0046 0830     		adds	r0, r0, #8
 307 0048 FFF7FEFF 		bl	spin_unlock
 308              	.LVL26:
 117:./block/bio.c ****     bio->bi_bdev->bd_queue->q_fn(bio->bi_bdev->bd_queue);
 309              		.loc 1 117 5 is_stmt 1 view .LVU78
 310              		.loc 1 117 8 is_stmt 0 view .LVU79
 311 004c 6368     		ldr	r3, [r4, #4]
 312              		.loc 1 117 17 view .LVU80
ARM GAS  /tmp/cc0b6Xiw.s 			page 20


 313 004e D868     		ldr	r0, [r3, #12]
 314              		.loc 1 117 27 view .LVU81
 315 0050 D0F8D830 		ldr	r3, [r0, #216]
 316              		.loc 1 117 5 view .LVU82
 317 0054 9847     		blx	r3
 318              	.LVL27:
 118:./block/bio.c ****     if(bio->bi_end_io!= NULL)
 319              		.loc 1 118 5 is_stmt 1 view .LVU83
 320              		.loc 1 118 11 is_stmt 0 view .LVU84
 321 0056 E36A     		ldr	r3, [r4, #44]
 322              		.loc 1 118 7 view .LVU85
 323 0058 0BB1     		cbz	r3, .L15
 119:./block/bio.c ****         bio->bi_end_io(bio);
 324              		.loc 1 119 9 is_stmt 1 view .LVU86
 325 005a 2046     		mov	r0, r4
 326 005c 9847     		blx	r3
 327              	.LVL28:
 328              	.L15:
 120:./block/bio.c **** }
 329              		.loc 1 120 1 is_stmt 0 view .LVU87
 330 005e 70BD     		pop	{r4, r5, r6, pc}
 331              		.loc 1 120 1 view .LVU88
 332              		.cfi_endproc
 333              	.LFE1040:
 335              		.section	.text.kthread_run,"ax",%progbits
 336              		.align	1
 337              		.syntax unified
 338              		.thumb
 339              		.thumb_func
 341              	kthread_run:
 342              	.LVL29:
 343              	.LFB1035:
 344              		.file 5 "./include/linux/kthread.h"
   1:./include/linux/kthread.h **** #ifndef _KTHREADS_H_
   2:./include/linux/kthread.h **** #define _KTHREADS_H_
   3:./include/linux/kthread.h **** 
   4:./include/linux/kthread.h **** #include <linux/sched.h>
   5:./include/linux/kthread.h **** #include <linux/sprintf.h>
   6:./include/linux/kthread.h **** #include <linux/stdarg.h>
   7:./include/linux/kthread.h **** #include <generated/autoconf.h>
   8:./include/linux/kthread.h **** 
   9:./include/linux/kthread.h **** #define KTHREAD_DEFAULT_PRIORITY CONFIG_KTHREAD_DEFAULT_PRIORITY
  10:./include/linux/kthread.h **** #define KTHREAD_DEFAULT_STACK_SIZE 1024*CONFIG_KTHREAD_DEFAULT_STACK_SIZE
  11:./include/linux/kthread.h **** #define default_core_number CONFIG_KTHREAD_DEFAULT_USE_CPU_CORE
  12:./include/linux/kthread.h **** 
  13:./include/linux/kthread.h **** static struct task_struct *kthread_run(
  14:./include/linux/kthread.h ****     int (*threadfn)(void *data), void *data, const char namefmt[],...)
  15:./include/linux/kthread.h **** {
 345              		.loc 5 15 1 is_stmt 1 view -0
 346              		.cfi_startproc
 347              		@ args = 4, pretend = 8, frame = 136
 348              		@ frame_needed = 0, uses_anonymous_args = 1
 349              		.loc 5 15 1 is_stmt 0 view .LVU90
 350 0000 0CB4     		push	{r2, r3}
 351              	.LCFI4:
 352              		.cfi_def_cfa_offset 8
 353              		.cfi_offset 2, -8
ARM GAS  /tmp/cc0b6Xiw.s 			page 21


 354              		.cfi_offset 3, -4
 355 0002 2DE9F041 		push	{r4, r5, r6, r7, r8, lr}
 356              	.LCFI5:
 357              		.cfi_def_cfa_offset 32
 358              		.cfi_offset 4, -32
 359              		.cfi_offset 5, -28
 360              		.cfi_offset 6, -24
 361              		.cfi_offset 7, -20
 362              		.cfi_offset 8, -16
 363              		.cfi_offset 14, -12
 364 0006 A4B0     		sub	sp, sp, #144
 365              	.LCFI6:
 366              		.cfi_def_cfa_offset 176
 367 0008 0546     		mov	r5, r0
 368 000a 0E46     		mov	r6, r1
 369 000c 2AAC     		add	r4, sp, #168
 370 000e 54F8047B 		ldr	r7, [r4], #4
  16:./include/linux/kthread.h ****     block_scheduler(NULL);
 371              		.loc 5 16 5 is_stmt 1 view .LVU91
 372 0012 0020     		movs	r0, #0
 373              	.LVL30:
 374              		.loc 5 16 5 is_stmt 0 view .LVU92
 375 0014 FFF7FEFF 		bl	block_scheduler
 376              	.LVL31:
  17:./include/linux/kthread.h ****     char name[128];
 377              		.loc 5 17 5 is_stmt 1 view .LVU93
  18:./include/linux/kthread.h ****     va_list args;
 378              		.loc 5 18 5 view .LVU94
  19:./include/linux/kthread.h ****     va_start(args, namefmt);
 379              		.loc 5 19 5 view .LVU95
 380 0018 0394     		str	r4, [sp, #12]
  20:./include/linux/kthread.h ****     vsnprintf(name, sizeof(name), namefmt, args);
 381              		.loc 5 20 5 view .LVU96
 382 001a 0DF11008 		add	r8, sp, #16
 383 001e 2346     		mov	r3, r4
 384 0020 3A46     		mov	r2, r7
 385 0022 8021     		movs	r1, #128
 386 0024 4046     		mov	r0, r8
 387 0026 FFF7FEFF 		bl	vsnprintf
 388              	.LVL32:
  21:./include/linux/kthread.h ****     va_end(args);
 389              		.loc 5 21 5 view .LVU97
  22:./include/linux/kthread.h ****     struct task_struct *t = task_run(
 390              		.loc 5 22 5 view .LVU98
 391              		.loc 5 22 29 is_stmt 0 view .LVU99
 392 002a 0027     		movs	r7, #0
 393 002c 0197     		str	r7, [sp, #4]
 394 002e CDF80080 		str	r8, [sp]
 395 0032 0823     		movs	r3, #8
 396 0034 3246     		mov	r2, r6
 397 0036 4FF40051 		mov	r1, #8192
 398 003a 2846     		mov	r0, r5
 399 003c FFF7FEFF 		bl	task_run
 400              	.LVL33:
 401 0040 0446     		mov	r4, r0
 402              	.LVL34:
  23:./include/linux/kthread.h ****     threadfn,KTHREAD_DEFAULT_STACK_SIZE,data,KTHREAD_DEFAULT_PRIORITY,name,default_core_number);
ARM GAS  /tmp/cc0b6Xiw.s 			page 22


  24:./include/linux/kthread.h ****     run_scheduler(NULL);
 403              		.loc 5 24 5 is_stmt 1 view .LVU100
 404 0042 3846     		mov	r0, r7
 405              	.LVL35:
 406              		.loc 5 24 5 is_stmt 0 view .LVU101
 407 0044 FFF7FEFF 		bl	run_scheduler
 408              	.LVL36:
  25:./include/linux/kthread.h ****     return t;
 409              		.loc 5 25 5 is_stmt 1 view .LVU102
  26:./include/linux/kthread.h **** }
 410              		.loc 5 26 1 is_stmt 0 view .LVU103
 411 0048 2046     		mov	r0, r4
 412 004a 24B0     		add	sp, sp, #144
 413              	.LCFI7:
 414              		.cfi_def_cfa_offset 32
 415              		@ sp needed
 416 004c BDE8F041 		pop	{r4, r5, r6, r7, r8, lr}
 417              	.LCFI8:
 418              		.cfi_restore 14
 419              		.cfi_restore 8
 420              		.cfi_restore 7
 421              		.cfi_restore 6
 422              		.cfi_restore 5
 423              		.cfi_restore 4
 424              		.cfi_def_cfa_offset 8
 425              	.LVL37:
 426              		.loc 5 26 1 view .LVU104
 427 0050 02B0     		add	sp, sp, #8
 428              	.LCFI9:
 429              		.cfi_restore 3
 430              		.cfi_restore 2
 431              		.cfi_def_cfa_offset 0
 432 0052 7047     		bx	lr
 433              		.cfi_endproc
 434              	.LFE1035:
 436              		.section	.text.bio_alloc_bioset,"ax",%progbits
 437              		.align	1
 438              		.global	bio_alloc_bioset
 439              		.syntax unified
 440              		.thumb
 441              		.thumb_func
 443              	bio_alloc_bioset:
 444              	.LVL38:
 445              	.LFB1036:
  17:./block/bio.c ****     struct bio *bio;
 446              		.loc 1 17 1 is_stmt 1 view -0
 447              		.cfi_startproc
 448              		@ args = 4, pretend = 0, frame = 0
 449              		@ frame_needed = 0, uses_anonymous_args = 0
  17:./block/bio.c ****     struct bio *bio;
 450              		.loc 1 17 1 is_stmt 0 view .LVU106
 451 0000 2DE9F047 		push	{r4, r5, r6, r7, r8, r9, r10, lr}
 452              	.LCFI10:
 453              		.cfi_def_cfa_offset 32
 454              		.cfi_offset 4, -32
 455              		.cfi_offset 5, -28
 456              		.cfi_offset 6, -24
ARM GAS  /tmp/cc0b6Xiw.s 			page 23


 457              		.cfi_offset 7, -20
 458              		.cfi_offset 8, -16
 459              		.cfi_offset 9, -12
 460              		.cfi_offset 10, -8
 461              		.cfi_offset 14, -4
 462 0004 8046     		mov	r8, r0
 463 0006 0D46     		mov	r5, r1
 464 0008 1746     		mov	r7, r2
 465 000a 9946     		mov	r9, r3
  18:./block/bio.c ****     size_t bio_size;
 466              		.loc 1 18 5 is_stmt 1 view .LVU107
  19:./block/bio.c ****     bool use_inline_vecs = false;
 467              		.loc 1 19 5 view .LVU108
  20:./block/bio.c **** 
 468              		.loc 1 20 5 view .LVU109
 469              	.LVL39:
  22:./block/bio.c ****         use_inline_vecs = true;
 470              		.loc 1 22 5 view .LVU110
  22:./block/bio.c ****         use_inline_vecs = true;
 471              		.loc 1 22 8 is_stmt 0 view .LVU111
 472 000c 0429     		cmp	r1, #4
 473 000e 2CD8     		bhi	.L27
  23:./block/bio.c ****         bio_size = sizeof(struct bio) + sizeof(struct bio_vec) * nr_iovecs;
 474              		.loc 1 23 9 is_stmt 1 view .LVU112
 475              	.LVL40:
  24:./block/bio.c ****     } 
 476              		.loc 1 24 9 view .LVU113
  24:./block/bio.c ****     } 
 477              		.loc 1 24 64 is_stmt 0 view .LVU114
 478 0010 01EB4106 		add	r6, r1, r1, lsl #1
 479 0014 B600     		lsls	r6, r6, #2
  24:./block/bio.c ****     } 
 480              		.loc 1 24 18 view .LVU115
 481 0016 4436     		adds	r6, r6, #68
 482              	.LVL41:
  23:./block/bio.c ****         bio_size = sizeof(struct bio) + sizeof(struct bio_vec) * nr_iovecs;
 483              		.loc 1 23 25 view .LVU116
 484 0018 4FF0010A 		mov	r10, #1
 485              	.LVL42:
 486              	.L22:
  31:./block/bio.c ****     if (!bio) {
 487              		.loc 1 31 5 is_stmt 1 view .LVU117
 488              	.LBB86:
 489              	.LBI86:
 490              		.file 6 "./include/linux/slab.h"
   1:./include/linux/slab.h **** /* SPDX-License-Identifier: GPL-2.0 */
   2:./include/linux/slab.h **** /*
   3:./include/linux/slab.h ****  * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
   4:./include/linux/slab.h ****  *
   5:./include/linux/slab.h ****  * (C) SGI 2006, Christoph Lameter
   6:./include/linux/slab.h ****  * 	Cleaned up and restructured to ease the addition of alternative
   7:./include/linux/slab.h ****  * 	implementations of SLAB allocators.
   8:./include/linux/slab.h ****  * (C) Linux Foundation 2008-2013
   9:./include/linux/slab.h ****  *      Unified interface for all slab allocators
  10:./include/linux/slab.h ****  */
  11:./include/linux/slab.h **** 
  12:./include/linux/slab.h **** #ifndef _LINUX_SLAB_H
ARM GAS  /tmp/cc0b6Xiw.s 			page 24


  13:./include/linux/slab.h **** #define	_LINUX_SLAB_H
  14:./include/linux/slab.h **** 
  15:./include/linux/slab.h **** #include <linux/cache.h>
  16:./include/linux/slab.h **** #include <linux/overflow.h>
  17:./include/linux/slab.h **** #include <linux/types.h>
  18:./include/linux/slab.h **** #include <linux/raid/pq.h>
  19:./include/linux/slab.h **** #include <linux/gfp_types.h>
  20:./include/linux/slab.h **** #include <linux/numa.h>
  21:./include/linux/slab.h **** #include <linux/reciprocal_div.h>
  22:./include/linux/slab.h **** #include <linux/spinlock.h>
  23:./include/linux/slab.h **** 
  24:./include/linux/slab.h **** enum _slab_flag_bits {
  25:./include/linux/slab.h **** 	_SLAB_CONSISTENCY_CHECKS,
  26:./include/linux/slab.h **** 	_SLAB_RED_ZONE,
  27:./include/linux/slab.h **** 	_SLAB_POISON,
  28:./include/linux/slab.h **** 	_SLAB_KMALLOC,
  29:./include/linux/slab.h **** 	_SLAB_HWCACHE_ALIGN,
  30:./include/linux/slab.h **** 	_SLAB_CACHE_DMA,
  31:./include/linux/slab.h **** 	_SLAB_CACHE_DMA32,
  32:./include/linux/slab.h **** 	_SLAB_STORE_USER,
  33:./include/linux/slab.h **** 	_SLAB_PANIC,
  34:./include/linux/slab.h **** 	_SLAB_TYPESAFE_BY_RCU,
  35:./include/linux/slab.h **** 	_SLAB_TRACE,
  36:./include/linux/slab.h **** #ifdef CONFIG_DEBUG_OBJECTS
  37:./include/linux/slab.h **** 	_SLAB_DEBUG_OBJECTS,
  38:./include/linux/slab.h **** #endif
  39:./include/linux/slab.h **** 	_SLAB_NOLEAKTRACE,
  40:./include/linux/slab.h **** 	_SLAB_NO_MERGE,
  41:./include/linux/slab.h **** #ifdef CONFIG_FAILSLAB
  42:./include/linux/slab.h **** 	_SLAB_FAILSLAB,
  43:./include/linux/slab.h **** #endif
  44:./include/linux/slab.h **** #ifdef CONFIG_MEMCG
  45:./include/linux/slab.h **** 	_SLAB_ACCOUNT,
  46:./include/linux/slab.h **** #endif
  47:./include/linux/slab.h **** #ifdef CONFIG_KASAN_GENERIC
  48:./include/linux/slab.h **** 	_SLAB_KASAN,
  49:./include/linux/slab.h **** #endif
  50:./include/linux/slab.h **** 	_SLAB_NO_USER_FLAGS,
  51:./include/linux/slab.h **** #ifdef CONFIG_KFENCE
  52:./include/linux/slab.h **** 	_SLAB_SKIP_KFENCE,
  53:./include/linux/slab.h **** #endif
  54:./include/linux/slab.h **** #ifndef CONFIG_SLUB_TINY
  55:./include/linux/slab.h **** 	_SLAB_RECLAIM_ACCOUNT,
  56:./include/linux/slab.h **** #endif
  57:./include/linux/slab.h **** 	_SLAB_OBJECT_POISON,
  58:./include/linux/slab.h **** 	_SLAB_CMPXCHG_DOUBLE,
  59:./include/linux/slab.h **** #ifdef CONFIG_SLAB_OBJ_EXT
  60:./include/linux/slab.h **** 	_SLAB_NO_OBJ_EXT,
  61:./include/linux/slab.h **** #endif
  62:./include/linux/slab.h **** 	_SLAB_FLAGS_LAST_BIT
  63:./include/linux/slab.h **** };
  64:./include/linux/slab.h **** 
  65:./include/linux/slab.h **** 
  66:./include/linux/slab.h **** 
  67:./include/linux/slab.h **** #define __SLAB_FLAG_BIT(nr)	((slab_flags_t __force)(1U << (nr)))
  68:./include/linux/slab.h **** #define __SLAB_FLAG_UNUSED	((slab_flags_t __force)(0U))
  69:./include/linux/slab.h **** 
ARM GAS  /tmp/cc0b6Xiw.s 			page 25


  70:./include/linux/slab.h **** /*
  71:./include/linux/slab.h ****  * Flags to pass to kmem_cache_create().
  72:./include/linux/slab.h ****  * The ones marked DEBUG need CONFIG_SLUB_DEBUG enabled, otherwise are no-op
  73:./include/linux/slab.h ****  */
  74:./include/linux/slab.h **** /* DEBUG: Perform (expensive) checks on alloc/free */
  75:./include/linux/slab.h **** #define SLAB_CONSISTENCY_CHECKS	__SLAB_FLAG_BIT(_SLAB_CONSISTENCY_CHECKS)
  76:./include/linux/slab.h **** /* DEBUG: Red zone objs in a cache */
  77:./include/linux/slab.h **** #define SLAB_RED_ZONE		__SLAB_FLAG_BIT(_SLAB_RED_ZONE)
  78:./include/linux/slab.h **** /* DEBUG: Poison objects */
  79:./include/linux/slab.h **** #define SLAB_POISON		__SLAB_FLAG_BIT(_SLAB_POISON)
  80:./include/linux/slab.h **** /* Indicate a kmalloc slab */
  81:./include/linux/slab.h **** #define SLAB_KMALLOC		__SLAB_FLAG_BIT(_SLAB_KMALLOC)
  82:./include/linux/slab.h **** /**
  83:./include/linux/slab.h ****  * define SLAB_HWCACHE_ALIGN - Align objects on cache line boundaries.
  84:./include/linux/slab.h ****  *
  85:./include/linux/slab.h ****  * Sufficiently large objects are aligned on cache line boundary. For object
  86:./include/linux/slab.h ****  * size smaller than a half of cache line size, the alignment is on the half of
  87:./include/linux/slab.h ****  * cache line size. In general, if object size is smaller than 1/2^n of cache
  88:./include/linux/slab.h ****  * line size, the alignment is adjusted to 1/2^n.
  89:./include/linux/slab.h ****  *
  90:./include/linux/slab.h ****  * If explicit alignment is also requested by the respective
  91:./include/linux/slab.h ****  * &struct kmem_cache_args field, the greater of both is alignments is applied.
  92:./include/linux/slab.h ****  */
  93:./include/linux/slab.h **** #define SLAB_HWCACHE_ALIGN	__SLAB_FLAG_BIT(_SLAB_HWCACHE_ALIGN)
  94:./include/linux/slab.h **** /* Use GFP_DMA memory */
  95:./include/linux/slab.h **** #define SLAB_CACHE_DMA		__SLAB_FLAG_BIT(_SLAB_CACHE_DMA)
  96:./include/linux/slab.h **** /* Use GFP_DMA32 memory */
  97:./include/linux/slab.h **** #define SLAB_CACHE_DMA32	__SLAB_FLAG_BIT(_SLAB_CACHE_DMA32)
  98:./include/linux/slab.h **** /* DEBUG: Store the last owner for bug hunting */
  99:./include/linux/slab.h **** #define SLAB_STORE_USER		__SLAB_FLAG_BIT(_SLAB_STORE_USER)
 100:./include/linux/slab.h **** /* Panic if kmem_cache_create() fails */
 101:./include/linux/slab.h **** #define SLAB_PANIC		__SLAB_FLAG_BIT(_SLAB_PANIC)
 102:./include/linux/slab.h **** /**
 103:./include/linux/slab.h ****  * define SLAB_TYPESAFE_BY_RCU - **WARNING** READ THIS!
 104:./include/linux/slab.h ****  *
 105:./include/linux/slab.h ****  * This delays freeing the SLAB page by a grace period, it does _NOT_
 106:./include/linux/slab.h ****  * delay object freeing. This means that if you do kmem_cache_free()
 107:./include/linux/slab.h ****  * that memory location is free to be reused at any time. Thus it may
 108:./include/linux/slab.h ****  * be possible to see another object there in the same RCU grace period.
 109:./include/linux/slab.h ****  *
 110:./include/linux/slab.h ****  * This feature only ensures the memory location backing the object
 111:./include/linux/slab.h ****  * stays valid, the trick to using this is relying on an independent
 112:./include/linux/slab.h ****  * object validation pass. Something like:
 113:./include/linux/slab.h ****  *
 114:./include/linux/slab.h ****  * ::
 115:./include/linux/slab.h ****  *
 116:./include/linux/slab.h ****  *  begin:
 117:./include/linux/slab.h ****  *   rcu_read_lock();
 118:./include/linux/slab.h ****  *   obj = lockless_lookup(key);
 119:./include/linux/slab.h ****  *   if (obj) {
 120:./include/linux/slab.h ****  *     if (!try_get_ref(obj)) // might fail for free objects
 121:./include/linux/slab.h ****  *       rcu_read_unlock();
 122:./include/linux/slab.h ****  *       goto begin;
 123:./include/linux/slab.h ****  *
 124:./include/linux/slab.h ****  *     if (obj->key != key) { // not the object we expected
 125:./include/linux/slab.h ****  *       put_ref(obj);
 126:./include/linux/slab.h ****  *       rcu_read_unlock();
ARM GAS  /tmp/cc0b6Xiw.s 			page 26


 127:./include/linux/slab.h ****  *       goto begin;
 128:./include/linux/slab.h ****  *     }
 129:./include/linux/slab.h ****  *   }
 130:./include/linux/slab.h ****  *  rcu_read_unlock();
 131:./include/linux/slab.h ****  *
 132:./include/linux/slab.h ****  * This is useful if we need to approach a kernel structure obliquely,
 133:./include/linux/slab.h ****  * from its address obtained without the usual locking. We can lock
 134:./include/linux/slab.h ****  * the structure to stabilize it and check it's still at the given address,
 135:./include/linux/slab.h ****  * only if we can be sure that the memory has not been meanwhile reused
 136:./include/linux/slab.h ****  * for some other kind of object (which our subsystem's lock might corrupt).
 137:./include/linux/slab.h ****  *
 138:./include/linux/slab.h ****  * rcu_read_lock before reading the address, then rcu_read_unlock after
 139:./include/linux/slab.h ****  * taking the spinlock within the structure expected at that address.
 140:./include/linux/slab.h ****  *
 141:./include/linux/slab.h ****  * Note that it is not possible to acquire a lock within a structure
 142:./include/linux/slab.h ****  * allocated with SLAB_TYPESAFE_BY_RCU without first acquiring a reference
 143:./include/linux/slab.h ****  * as described above.  The reason is that SLAB_TYPESAFE_BY_RCU pages
 144:./include/linux/slab.h ****  * are not zeroed before being given to the slab, which means that any
 145:./include/linux/slab.h ****  * locks must be initialized after each and every kmem_struct_alloc().
 146:./include/linux/slab.h ****  * Alternatively, make the ctor passed to kmem_cache_create() initialize
 147:./include/linux/slab.h ****  * the locks at page-allocation time, as is done in __i915_request_ctor(),
 148:./include/linux/slab.h ****  * sighand_ctor(), and anon_vma_ctor().  Such a ctor permits readers
 149:./include/linux/slab.h ****  * to safely acquire those ctor-initialized locks under rcu_read_lock()
 150:./include/linux/slab.h ****  * protection.
 151:./include/linux/slab.h ****  *
 152:./include/linux/slab.h ****  * Note that SLAB_TYPESAFE_BY_RCU was originally named SLAB_DESTROY_BY_RCU.
 153:./include/linux/slab.h ****  */
 154:./include/linux/slab.h **** #define SLAB_TYPESAFE_BY_RCU	__SLAB_FLAG_BIT(_SLAB_TYPESAFE_BY_RCU)
 155:./include/linux/slab.h **** /* Trace allocations and frees */
 156:./include/linux/slab.h **** #define SLAB_TRACE		__SLAB_FLAG_BIT(_SLAB_TRACE)
 157:./include/linux/slab.h **** 
 158:./include/linux/slab.h **** /* Flag to prevent checks on free */
 159:./include/linux/slab.h **** #ifdef CONFIG_DEBUG_OBJECTS
 160:./include/linux/slab.h **** # define SLAB_DEBUG_OBJECTS	__SLAB_FLAG_BIT(_SLAB_DEBUG_OBJECTS)
 161:./include/linux/slab.h **** #else
 162:./include/linux/slab.h **** # define SLAB_DEBUG_OBJECTS	__SLAB_FLAG_UNUSED
 163:./include/linux/slab.h **** #endif
 164:./include/linux/slab.h **** 
 165:./include/linux/slab.h **** /* Avoid kmemleak tracing */
 166:./include/linux/slab.h **** #define SLAB_NOLEAKTRACE	__SLAB_FLAG_BIT(_SLAB_NOLEAKTRACE)
 167:./include/linux/slab.h **** 
 168:./include/linux/slab.h **** /*
 169:./include/linux/slab.h ****  * Prevent merging with compatible kmem caches. This flag should be used
 170:./include/linux/slab.h ****  * cautiously. Valid use cases:
 171:./include/linux/slab.h ****  *
 172:./include/linux/slab.h ****  * - caches created for self-tests (e.g. kunit)
 173:./include/linux/slab.h ****  * - general caches created and used by a subsystem, only when a
 174:./include/linux/slab.h ****  *   (subsystem-specific) debug option is enabled
 175:./include/linux/slab.h ****  * - performance critical caches, should be very rare and consulted with slab
 176:./include/linux/slab.h ****  *   maintainers, and not used together with CONFIG_SLUB_TINY
 177:./include/linux/slab.h ****  */
 178:./include/linux/slab.h **** #define SLAB_NO_MERGE		__SLAB_FLAG_BIT(_SLAB_NO_MERGE)
 179:./include/linux/slab.h **** 
 180:./include/linux/slab.h **** /* Fault injection mark */
 181:./include/linux/slab.h **** #ifdef CONFIG_FAILSLAB
 182:./include/linux/slab.h **** # define SLAB_FAILSLAB		__SLAB_FLAG_BIT(_SLAB_FAILSLAB)
 183:./include/linux/slab.h **** #else
ARM GAS  /tmp/cc0b6Xiw.s 			page 27


 184:./include/linux/slab.h **** # define SLAB_FAILSLAB		__SLAB_FLAG_UNUSED
 185:./include/linux/slab.h **** #endif
 186:./include/linux/slab.h **** /**
 187:./include/linux/slab.h ****  * define SLAB_ACCOUNT - Account allocations to memcg.
 188:./include/linux/slab.h ****  *
 189:./include/linux/slab.h ****  * All object allocations from this cache will be memcg accounted, regardless of
 190:./include/linux/slab.h ****  * __GFP_ACCOUNT being or not being passed to individual allocations.
 191:./include/linux/slab.h ****  */
 192:./include/linux/slab.h **** #ifdef CONFIG_MEMCG
 193:./include/linux/slab.h **** # define SLAB_ACCOUNT		__SLAB_FLAG_BIT(_SLAB_ACCOUNT)
 194:./include/linux/slab.h **** #else
 195:./include/linux/slab.h **** # define SLAB_ACCOUNT		__SLAB_FLAG_UNUSED
 196:./include/linux/slab.h **** #endif
 197:./include/linux/slab.h **** 
 198:./include/linux/slab.h **** #ifdef CONFIG_KASAN_GENERIC
 199:./include/linux/slab.h **** #define SLAB_KASAN		__SLAB_FLAG_BIT(_SLAB_KASAN)
 200:./include/linux/slab.h **** #else
 201:./include/linux/slab.h **** #define SLAB_KASAN		__SLAB_FLAG_UNUSED
 202:./include/linux/slab.h **** #endif
 203:./include/linux/slab.h **** 
 204:./include/linux/slab.h **** /*
 205:./include/linux/slab.h ****  * Ignore user specified debugging flags.
 206:./include/linux/slab.h ****  * Intended for caches created for self-tests so they have only flags
 207:./include/linux/slab.h ****  * specified in the code and other flags are ignored.
 208:./include/linux/slab.h ****  */
 209:./include/linux/slab.h **** #define SLAB_NO_USER_FLAGS	__SLAB_FLAG_BIT(_SLAB_NO_USER_FLAGS)
 210:./include/linux/slab.h **** 
 211:./include/linux/slab.h **** #ifdef CONFIG_KFENCE
 212:./include/linux/slab.h **** #define SLAB_SKIP_KFENCE	__SLAB_FLAG_BIT(_SLAB_SKIP_KFENCE)
 213:./include/linux/slab.h **** #else
 214:./include/linux/slab.h **** #define SLAB_SKIP_KFENCE	__SLAB_FLAG_UNUSED
 215:./include/linux/slab.h **** #endif
 216:./include/linux/slab.h **** 
 217:./include/linux/slab.h **** /* The following flags affect the page allocator grouping pages by mobility */
 218:./include/linux/slab.h **** /**
 219:./include/linux/slab.h ****  * define SLAB_RECLAIM_ACCOUNT - Objects are reclaimable.
 220:./include/linux/slab.h ****  *
 221:./include/linux/slab.h ****  * Use this flag for caches that have an associated shrinker. As a result, slab
 222:./include/linux/slab.h ****  * pages are allocated with __GFP_RECLAIMABLE, which affects grouping pages by
 223:./include/linux/slab.h ****  * mobility, and are accounted in SReclaimable counter in /proc/meminfo
 224:./include/linux/slab.h ****  */
 225:./include/linux/slab.h **** #ifndef CONFIG_SLUB_TINY
 226:./include/linux/slab.h **** #define SLAB_RECLAIM_ACCOUNT	__SLAB_FLAG_BIT(_SLAB_RECLAIM_ACCOUNT)
 227:./include/linux/slab.h **** #else
 228:./include/linux/slab.h **** #define SLAB_RECLAIM_ACCOUNT	__SLAB_FLAG_UNUSED
 229:./include/linux/slab.h **** #endif
 230:./include/linux/slab.h **** #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */
 231:./include/linux/slab.h **** 
 232:./include/linux/slab.h **** /* Slab created using create_boot_cache */
 233:./include/linux/slab.h **** #ifdef CONFIG_SLAB_OBJ_EXT
 234:./include/linux/slab.h **** #define SLAB_NO_OBJ_EXT		__SLAB_FLAG_BIT(_SLAB_NO_OBJ_EXT)
 235:./include/linux/slab.h **** #else
 236:./include/linux/slab.h **** #define SLAB_NO_OBJ_EXT		__SLAB_FLAG_UNUSED
 237:./include/linux/slab.h **** #endif
 238:./include/linux/slab.h **** 
 239:./include/linux/slab.h **** /*
 240:./include/linux/slab.h ****  * freeptr_t represents a SLUB freelist pointer, which might be encoded
ARM GAS  /tmp/cc0b6Xiw.s 			page 28


 241:./include/linux/slab.h ****  * and not dereferenceable if CONFIG_SLAB_FREELIST_HARDENED is enabled.
 242:./include/linux/slab.h ****  */
 243:./include/linux/slab.h **** typedef struct { unsigned long v; } freeptr_t;
 244:./include/linux/slab.h **** 
 245:./include/linux/slab.h **** /*
 246:./include/linux/slab.h ****  * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
 247:./include/linux/slab.h ****  *
 248:./include/linux/slab.h ****  * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.
 249:./include/linux/slab.h ****  *
 250:./include/linux/slab.h ****  * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.
 251:./include/linux/slab.h ****  * Both make kfree a no-op.
 252:./include/linux/slab.h ****  */
 253:./include/linux/slab.h **** #define ZERO_SIZE_PTR ((void *)16)
 254:./include/linux/slab.h **** 
 255:./include/linux/slab.h **** #define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
 256:./include/linux/slab.h **** 				(unsigned long)ZERO_SIZE_PTR)
 257:./include/linux/slab.h **** 
 258:./include/linux/slab.h **** 
 259:./include/linux/slab.h **** 
 260:./include/linux/slab.h **** 
 261:./include/linux/slab.h **** 
 262:./include/linux/slab.h **** #ifdef CONFIG_SLUB_CPU_PARTIAL
 263:./include/linux/slab.h **** #define slub_percpu_partial(c)			((c)->partial)
 264:./include/linux/slab.h **** 
 265:./include/linux/slab.h **** #define slub_set_percpu_partial(c, p)		\
 266:./include/linux/slab.h **** ({						\
 267:./include/linux/slab.h **** 	slub_percpu_partial(c) = (p)->next;	\
 268:./include/linux/slab.h **** })
 269:./include/linux/slab.h **** 
 270:./include/linux/slab.h **** #define slub_percpu_partial_read_once(c)	READ_ONCE(slub_percpu_partial(c))
 271:./include/linux/slab.h **** #else
 272:./include/linux/slab.h **** #define slub_percpu_partial(c)			NULL
 273:./include/linux/slab.h **** 
 274:./include/linux/slab.h **** #define slub_set_percpu_partial(c, p)
 275:./include/linux/slab.h **** 
 276:./include/linux/slab.h **** #define slub_percpu_partial_read_once(c)	NULL
 277:./include/linux/slab.h **** 
 278:./include/linux/slab.h **** 
 279:./include/linux/slab.h **** #endif // CONFIG_SLUB_CPU_PARTIAL
 280:./include/linux/slab.h **** 
 281:./include/linux/slab.h **** /*
 282:./include/linux/slab.h **** 	* Word size structure that can be atomically updated or read and that
 283:./include/linux/slab.h **** 	* contains both the order and the number of objects that a slab of the
 284:./include/linux/slab.h **** 	* given order would contain.
 285:./include/linux/slab.h **** 	*/				
 286:./include/linux/slab.h **** struct kmem_cache_order_objects {
 287:./include/linux/slab.h **** 	unsigned int x;
 288:./include/linux/slab.h **** };
 289:./include/linux/slab.h **** 
 290:./include/linux/slab.h **** struct kmem_cache_node {
 291:./include/linux/slab.h **** 	spinlock_t list_lock;
 292:./include/linux/slab.h **** 	unsigned long nr_partial;
 293:./include/linux/slab.h **** 	struct list_head partial;
 294:./include/linux/slab.h **** #ifdef CONFIG_SLUB_DEBUG
 295:./include/linux/slab.h **** 	atomic_long_t nr_slabs;
 296:./include/linux/slab.h **** 	atomic_long_t total_objects;
 297:./include/linux/slab.h **** 	struct list_head full;
ARM GAS  /tmp/cc0b6Xiw.s 			page 29


 298:./include/linux/slab.h **** #endif
 299:./include/linux/slab.h **** };
 300:./include/linux/slab.h **** 
 301:./include/linux/slab.h **** struct kmem_cache {
 302:./include/linux/slab.h **** 	#ifndef CONFIG_SLUB_TINY
 303:./include/linux/slab.h **** 	//	struct kmem_cache_cpu __percpu *cpu_slab;
 304:./include/linux/slab.h **** 	#endif
 305:./include/linux/slab.h **** 		/* Used for retrieving partial slabs, etc. */
 306:./include/linux/slab.h **** 		slab_flags_t flags;
 307:./include/linux/slab.h **** 		unsigned long min_partial;
 308:./include/linux/slab.h **** 		unsigned int size;		/* Object size including metadata */
 309:./include/linux/slab.h **** 		unsigned int object_size;	/* Object size without metadata */
 310:./include/linux/slab.h **** 		struct reciprocal_value reciprocal_size;
 311:./include/linux/slab.h **** 		unsigned int offset;		/* Free pointer offset */
 312:./include/linux/slab.h **** 	#ifdef CONFIG_SLUB_CPU_PARTIAL
 313:./include/linux/slab.h **** 		/* Number of per cpu partial objects to keep around */
 314:./include/linux/slab.h **** 		unsigned int cpu_partial;
 315:./include/linux/slab.h **** 		/* Number of per cpu partial slabs to keep around */
 316:./include/linux/slab.h **** 		unsigned int cpu_partial_slabs;
 317:./include/linux/slab.h **** 	#endif
 318:./include/linux/slab.h **** 		struct kmem_cache_order_objects oo;
 319:./include/linux/slab.h **** 	
 320:./include/linux/slab.h **** 		/* Allocation and freeing of slabs */
 321:./include/linux/slab.h **** 		struct kmem_cache_order_objects min;
 322:./include/linux/slab.h **** 		gfp_t allocflags;		/* gfp flags to use on each alloc */
 323:./include/linux/slab.h **** 		int refcount;			/* Refcount for slab cache destroy */
 324:./include/linux/slab.h **** 		void (*ctor)(void *object);	/* Object constructor */
 325:./include/linux/slab.h **** 		unsigned int inuse;		/* Offset to metadata */
 326:./include/linux/slab.h **** 		unsigned int align;		/* Alignment */
 327:./include/linux/slab.h **** 		unsigned int red_left_pad;	/* Left redzone padding size */
 328:./include/linux/slab.h **** 		const char *name;		/* Name (only for display!) */
 329:./include/linux/slab.h **** 		struct list_head list;		/* List of slab caches */
 330:./include/linux/slab.h **** 	#ifdef CONFIG_SYSFS
 331:./include/linux/slab.h **** 		struct kobject kobj;		/* For sysfs */
 332:./include/linux/slab.h **** 	#endif
 333:./include/linux/slab.h **** 	#ifdef CONFIG_SLAB_FREELIST_HARDENED
 334:./include/linux/slab.h **** 		unsigned long random;
 335:./include/linux/slab.h **** 	#endif
 336:./include/linux/slab.h **** 	
 337:./include/linux/slab.h **** 	#ifdef CONFIG_NUMA
 338:./include/linux/slab.h **** 		/*
 339:./include/linux/slab.h **** 			* Defragmentation by allocating from a remote node.
 340:./include/linux/slab.h **** 			*/
 341:./include/linux/slab.h **** 		unsigned int remote_node_defrag_ratio;
 342:./include/linux/slab.h **** 	#endif
 343:./include/linux/slab.h **** 	
 344:./include/linux/slab.h **** 	#ifdef CONFIG_SLAB_FREELIST_RANDOM
 345:./include/linux/slab.h **** 		unsigned int *random_seq;
 346:./include/linux/slab.h **** 	#endif
 347:./include/linux/slab.h **** 	
 348:./include/linux/slab.h **** 	#ifdef CONFIG_KASAN_GENERIC
 349:./include/linux/slab.h **** 		struct kasan_cache kasan_info;
 350:./include/linux/slab.h **** 	#endif
 351:./include/linux/slab.h **** 	
 352:./include/linux/slab.h **** 	#ifdef CONFIG_HARDENED_USERCOPY
 353:./include/linux/slab.h **** 		unsigned int useroffset;	/* Usercopy region offset */
 354:./include/linux/slab.h **** 		unsigned int usersize;		/* Usercopy region size */
ARM GAS  /tmp/cc0b6Xiw.s 			page 30


 355:./include/linux/slab.h **** 	#endif
 356:./include/linux/slab.h **** 	
 357:./include/linux/slab.h **** 		struct kmem_cache_node *node[MAX_NUMNODES];
 358:./include/linux/slab.h **** 	};
 359:./include/linux/slab.h **** 					
 360:./include/linux/slab.h **** 
 361:./include/linux/slab.h **** 
 362:./include/linux/slab.h **** 
 363:./include/linux/slab.h **** 
 364:./include/linux/slab.h **** #define KMALLOC_WAIT 1
 365:./include/linux/slab.h **** 
 366:./include/linux/slab.h **** 
 367:./include/linux/slab.h **** extern void* __smalloc__(u32 size, gfp_t flags);
 368:./include/linux/slab.h **** extern void  __sfree__(void* addr);
 369:./include/linux/slab.h **** 
 370:./include/linux/slab.h **** 
 371:./include/linux/slab.h **** static void inline *vmalloc(unsigned long size){
 372:./include/linux/slab.h **** 	return __smalloc__(size,GFP_TRANSHUGE_LIGHT);
 373:./include/linux/slab.h **** }
 374:./include/linux/slab.h **** 
 375:./include/linux/slab.h **** static void inline vfree(void *addr){
 376:./include/linux/slab.h **** 	__sfree__(addr);
 377:./include/linux/slab.h **** }
 378:./include/linux/slab.h **** 
 379:./include/linux/slab.h **** static void inline *kmalloc(size_t size, gfp_t flags){
 491              		.loc 6 379 21 view .LVU118
 492              	.LBB87:
 380:./include/linux/slab.h **** 	return __smalloc__((u32)size,flags);
 493              		.loc 6 380 2 view .LVU119
 494              		.loc 6 380 9 is_stmt 0 view .LVU120
 495 001c 4946     		mov	r1, r9
 496              	.LVL43:
 497              		.loc 6 380 9 view .LVU121
 498 001e 3046     		mov	r0, r6
 499              	.LVL44:
 500              		.loc 6 380 9 view .LVU122
 501 0020 FFF7FEFF 		bl	__smalloc__
 502              	.LVL45:
 503              		.loc 6 380 9 view .LVU123
 504              	.LBE87:
 505              	.LBE86:
  32:./block/bio.c ****         return NULL;
 506              		.loc 1 32 5 is_stmt 1 view .LVU124
  32:./block/bio.c ****         return NULL;
 507              		.loc 1 32 8 is_stmt 0 view .LVU125
 508 0024 0446     		mov	r4, r0
 509 0026 E8B1     		cbz	r0, .L21
  35:./block/bio.c ****     
 510              		.loc 1 35 5 is_stmt 1 view .LVU126
 511 0028 3246     		mov	r2, r6
 512 002a 0021     		movs	r1, #0
 513 002c FFF7FEFF 		bl	memset
 514              	.LVL46:
  38:./block/bio.c ****         bio->bi_io_vec = bio->bi_inline_vecs;
 515              		.loc 1 38 5 view .LVU127
  38:./block/bio.c ****         bio->bi_io_vec = bio->bi_inline_vecs;
 516              		.loc 1 38 8 is_stmt 0 view .LVU128
ARM GAS  /tmp/cc0b6Xiw.s 			page 31


 517 0030 BAF1000F 		cmp	r10, #0
 518 0034 1DD0     		beq	.L24
  39:./block/bio.c ****         bio->bi_max_vecs = nr_iovecs;
 519              		.loc 1 39 9 is_stmt 1 view .LVU129
  39:./block/bio.c ****         bio->bi_max_vecs = nr_iovecs;
 520              		.loc 1 39 26 is_stmt 0 view .LVU130
 521 0036 04F14403 		add	r3, r4, #68
  39:./block/bio.c ****         bio->bi_max_vecs = nr_iovecs;
 522              		.loc 1 39 24 view .LVU131
 523 003a E363     		str	r3, [r4, #60]
  40:./block/bio.c ****     }
 524              		.loc 1 40 9 is_stmt 1 view .LVU132
  40:./block/bio.c ****     }
 525              		.loc 1 40 26 is_stmt 0 view .LVU133
 526 003c E586     		strh	r5, [r4, #54]	@ movhi
 527              	.LVL47:
 528              	.L25:
  50:./block/bio.c ****     bio->bi_bdev = NULL;
 529              		.loc 1 50 5 is_stmt 1 view .LVU134
  50:./block/bio.c ****     bio->bi_bdev = NULL;
 530              		.loc 1 50 18 is_stmt 0 view .LVU135
 531 003e 0023     		movs	r3, #0
 532 0040 2360     		str	r3, [r4]
  51:./block/bio.c ****     bio->bi_opf = 0;
 533              		.loc 1 51 5 is_stmt 1 view .LVU136
  51:./block/bio.c ****     bio->bi_opf = 0;
 534              		.loc 1 51 18 is_stmt 0 view .LVU137
 535 0042 6360     		str	r3, [r4, #4]
  52:./block/bio.c ****     bio->bi_flags = 0;
 536              		.loc 1 52 5 is_stmt 1 view .LVU138
  52:./block/bio.c ****     bio->bi_flags = 0;
 537              		.loc 1 52 17 is_stmt 0 view .LVU139
 538 0044 A360     		str	r3, [r4, #8]
  53:./block/bio.c ****     bio->bi_status = BLK_STS_OK;
 539              		.loc 1 53 5 is_stmt 1 view .LVU140
  53:./block/bio.c ****     bio->bi_status = BLK_STS_OK;
 540              		.loc 1 53 19 is_stmt 0 view .LVU141
 541 0046 A381     		strh	r3, [r4, #12]	@ movhi
  54:./block/bio.c ****     atomic_set(&bio->__bi_remaining, 1);
 542              		.loc 1 54 5 is_stmt 1 view .LVU142
  54:./block/bio.c ****     atomic_set(&bio->__bi_remaining, 1);
 543              		.loc 1 54 20 is_stmt 0 view .LVU143
 544 0048 6374     		strb	r3, [r4, #17]
  55:./block/bio.c ****     atomic_set(&bio->__bi_cnt, 1);
 545              		.loc 1 55 5 is_stmt 1 view .LVU144
 546              	.LVL48:
 547              	.LBB88:
 548              	.LBI88:
 549              		.file 7 "./include/linux/atomic/atomic-instrumented.h"
   1:./include/linux/atomic/atomic-instrumented.h **** // SPDX-License-Identifier: GPL-2.0
   2:./include/linux/atomic/atomic-instrumented.h **** 
   3:./include/linux/atomic/atomic-instrumented.h **** // Generated by scripts/atomic/gen-atomic-instrumented.sh 
   4:./include/linux/atomic/atomic-instrumented.h **** // DO NOT MODIFY THIS FILE DIRECTLY
   5:./include/linux/atomic/atomic-instrumented.h **** 
   6:./include/linux/atomic/atomic-instrumented.h **** /*
   7:./include/linux/atomic/atomic-instrumented.h ****  * This file provoides atomic operations with explicit instrumentation (e.g.
   8:./include/linux/atomic/atomic-instrumented.h ****  * KASAN, KCSAN), which should be used unless it is necessary to avoid
ARM GAS  /tmp/cc0b6Xiw.s 			page 32


   9:./include/linux/atomic/atomic-instrumented.h ****  * instrumentation. Where it is necessary to aovid instrumenation, the
  10:./include/linux/atomic/atomic-instrumented.h ****  * raw_atomic*() operations should be used.
  11:./include/linux/atomic/atomic-instrumented.h ****  */
  12:./include/linux/atomic/atomic-instrumented.h **** #ifndef _LINUX_ATOMIC_INSTRUMENTED_H
  13:./include/linux/atomic/atomic-instrumented.h **** #define _LINUX_ATOMIC_INSTRUMENTED_H
  14:./include/linux/atomic/atomic-instrumented.h **** 
  15:./include/linux/atomic/atomic-instrumented.h **** #include <linux/build_bug.h>
  16:./include/linux/atomic/atomic-instrumented.h **** #include <linux/compiler.h>
  17:./include/linux/atomic/atomic-instrumented.h **** #include <linux/instrumented.h>
  18:./include/linux/atomic/atomic-instrumented.h **** 
  19:./include/linux/atomic/atomic-instrumented.h **** /**
  20:./include/linux/atomic/atomic-instrumented.h ****  * atomic_read() - atomic load with relaxed ordering
  21:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
  22:./include/linux/atomic/atomic-instrumented.h ****  *
  23:./include/linux/atomic/atomic-instrumented.h ****  * Atomically loads the value of @v with relaxed ordering.
  24:./include/linux/atomic/atomic-instrumented.h ****  *
  25:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_read() there.
  26:./include/linux/atomic/atomic-instrumented.h ****  *
  27:./include/linux/atomic/atomic-instrumented.h ****  * Return: The value loaded from @v.
  28:./include/linux/atomic/atomic-instrumented.h ****  */
  29:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
  30:./include/linux/atomic/atomic-instrumented.h **** atomic_read(const atomic_t *v)
  31:./include/linux/atomic/atomic-instrumented.h **** {
  32:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read(v, sizeof(*v));
  33:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_read(v);
  34:./include/linux/atomic/atomic-instrumented.h **** }
  35:./include/linux/atomic/atomic-instrumented.h **** 
  36:./include/linux/atomic/atomic-instrumented.h **** /**
  37:./include/linux/atomic/atomic-instrumented.h ****  * atomic_read_acquire() - atomic load with acquire ordering
  38:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
  39:./include/linux/atomic/atomic-instrumented.h ****  *
  40:./include/linux/atomic/atomic-instrumented.h ****  * Atomically loads the value of @v with acquire ordering.
  41:./include/linux/atomic/atomic-instrumented.h ****  *
  42:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_read_acquire() there.
  43:./include/linux/atomic/atomic-instrumented.h ****  *
  44:./include/linux/atomic/atomic-instrumented.h ****  * Return: The value loaded from @v.
  45:./include/linux/atomic/atomic-instrumented.h ****  */
  46:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
  47:./include/linux/atomic/atomic-instrumented.h **** atomic_read_acquire(const atomic_t *v)
  48:./include/linux/atomic/atomic-instrumented.h **** {
  49:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read(v, sizeof(*v));
  50:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_read_acquire(v);
  51:./include/linux/atomic/atomic-instrumented.h **** }
  52:./include/linux/atomic/atomic-instrumented.h **** 
  53:./include/linux/atomic/atomic-instrumented.h **** /**
  54:./include/linux/atomic/atomic-instrumented.h ****  * atomic_set() - atomic set with relaxed ordering
  55:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
  56:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to assign
  57:./include/linux/atomic/atomic-instrumented.h ****  *
  58:./include/linux/atomic/atomic-instrumented.h ****  * Atomically sets @v to @i with relaxed ordering.
  59:./include/linux/atomic/atomic-instrumented.h ****  *
  60:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_set() there.
  61:./include/linux/atomic/atomic-instrumented.h ****  *
  62:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
  63:./include/linux/atomic/atomic-instrumented.h ****  */
  64:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
  65:./include/linux/atomic/atomic-instrumented.h **** atomic_set(atomic_t *v, int i)
ARM GAS  /tmp/cc0b6Xiw.s 			page 33


 550              		.loc 7 65 1 view .LVU145
  66:./include/linux/atomic/atomic-instrumented.h **** {
  67:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_write(v, sizeof(*v));
 551              		.loc 7 67 2 view .LVU146
  68:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_set(v, i);
 552              		.loc 7 68 2 view .LVU147
 553              	.LBB89:
 554              	.LBI89:
 555              		.file 8 "./include/linux/atomic/atomic-arch-fallback.h"
   1:./include/linux/atomic/atomic-arch-fallback.h **** // SPDX-License-Identifier: GPL-2.0
   2:./include/linux/atomic/atomic-arch-fallback.h **** 
   3:./include/linux/atomic/atomic-arch-fallback.h **** // Generated by scripts/atomic/gen-atomic-fallback.sh
   4:./include/linux/atomic/atomic-arch-fallback.h **** // DO NOT MODIFY THIS FILE DIRECTLY
   5:./include/linux/atomic/atomic-arch-fallback.h **** 
   6:./include/linux/atomic/atomic-arch-fallback.h **** #ifndef _LINUX_ATOMIC_FALLBACK_H
   7:./include/linux/atomic/atomic-arch-fallback.h **** #define _LINUX_ATOMIC_FALLBACK_H
   8:./include/linux/atomic/atomic-arch-fallback.h **** 
   9:./include/linux/atomic/atomic-arch-fallback.h **** #include <linux/compiler.h>
  10:./include/linux/atomic/atomic-arch-fallback.h **** 
  11:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_xchg)
  12:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg arch_xchg
  13:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_xchg_relaxed)
  14:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg(...) \
  15:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_fence(arch_xchg, __VA_ARGS__)
  16:./include/linux/atomic/atomic-arch-fallback.h **** #else
  17:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_xchg_not_implemented(void);
  18:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg(...) raw_xchg_not_implemented()
  19:./include/linux/atomic/atomic-arch-fallback.h **** #endif
  20:./include/linux/atomic/atomic-arch-fallback.h **** 
  21:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_xchg_acquire)
  22:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_acquire arch_xchg_acquire
  23:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_xchg_relaxed)
  24:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_acquire(...) \
  25:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_acquire(arch_xchg, __VA_ARGS__)
  26:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_xchg)
  27:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_acquire arch_xchg
  28:./include/linux/atomic/atomic-arch-fallback.h **** #else
  29:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_xchg_acquire_not_implemented(void);
  30:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_acquire(...) raw_xchg_acquire_not_implemented()
  31:./include/linux/atomic/atomic-arch-fallback.h **** #endif
  32:./include/linux/atomic/atomic-arch-fallback.h **** 
  33:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_xchg_release)
  34:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_release arch_xchg_release
  35:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_xchg_relaxed)
  36:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_release(...) \
  37:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_release(arch_xchg, __VA_ARGS__)
  38:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_xchg)
  39:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_release arch_xchg
  40:./include/linux/atomic/atomic-arch-fallback.h **** #else
  41:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_xchg_release_not_implemented(void);
  42:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_release(...) raw_xchg_release_not_implemented()
  43:./include/linux/atomic/atomic-arch-fallback.h **** #endif
  44:./include/linux/atomic/atomic-arch-fallback.h **** 
  45:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_xchg_relaxed)
  46:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_relaxed arch_xchg_relaxed
  47:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_xchg)
  48:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_relaxed arch_xchg
ARM GAS  /tmp/cc0b6Xiw.s 			page 34


  49:./include/linux/atomic/atomic-arch-fallback.h **** #else
  50:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_xchg_relaxed_not_implemented(void);
  51:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_xchg_relaxed(...) raw_xchg_relaxed_not_implemented()
  52:./include/linux/atomic/atomic-arch-fallback.h **** #endif
  53:./include/linux/atomic/atomic-arch-fallback.h **** 
  54:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg)
  55:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg arch_cmpxchg
  56:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg_relaxed)
  57:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg(...) \
  58:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_fence(arch_cmpxchg, __VA_ARGS__)
  59:./include/linux/atomic/atomic-arch-fallback.h **** #else
  60:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg_not_implemented(void);
  61:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg(...) raw_cmpxchg_not_implemented()
  62:./include/linux/atomic/atomic-arch-fallback.h **** #endif
  63:./include/linux/atomic/atomic-arch-fallback.h **** 
  64:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg_acquire)
  65:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_acquire arch_cmpxchg_acquire
  66:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg_relaxed)
  67:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_acquire(...) \
  68:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_acquire(arch_cmpxchg, __VA_ARGS__)
  69:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg)
  70:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_acquire arch_cmpxchg
  71:./include/linux/atomic/atomic-arch-fallback.h **** #else
  72:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg_acquire_not_implemented(void);
  73:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_acquire(...) raw_cmpxchg_acquire_not_implemented()
  74:./include/linux/atomic/atomic-arch-fallback.h **** #endif
  75:./include/linux/atomic/atomic-arch-fallback.h **** 
  76:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg_release)
  77:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_release arch_cmpxchg_release
  78:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg_relaxed)
  79:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_release(...) \
  80:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_release(arch_cmpxchg, __VA_ARGS__)
  81:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg)
  82:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_release arch_cmpxchg
  83:./include/linux/atomic/atomic-arch-fallback.h **** #else
  84:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg_release_not_implemented(void);
  85:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_release(...) raw_cmpxchg_release_not_implemented()
  86:./include/linux/atomic/atomic-arch-fallback.h **** #endif
  87:./include/linux/atomic/atomic-arch-fallback.h **** 
  88:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg_relaxed)
  89:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_relaxed arch_cmpxchg_relaxed
  90:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg)
  91:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_relaxed arch_cmpxchg
  92:./include/linux/atomic/atomic-arch-fallback.h **** #else
  93:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg_relaxed_not_implemented(void);
  94:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_relaxed(...) raw_cmpxchg_relaxed_not_implemented()
  95:./include/linux/atomic/atomic-arch-fallback.h **** #endif
  96:./include/linux/atomic/atomic-arch-fallback.h **** 
  97:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg64)
  98:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64 arch_cmpxchg64
  99:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg64_relaxed)
 100:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64(...) \
 101:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_fence(arch_cmpxchg64, __VA_ARGS__)
 102:./include/linux/atomic/atomic-arch-fallback.h **** #else
 103:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg64_not_implemented(void);
 104:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64(...) raw_cmpxchg64_not_implemented()
 105:./include/linux/atomic/atomic-arch-fallback.h **** #endif
ARM GAS  /tmp/cc0b6Xiw.s 			page 35


 106:./include/linux/atomic/atomic-arch-fallback.h **** 
 107:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg64_acquire)
 108:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_acquire arch_cmpxchg64_acquire
 109:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg64_relaxed)
 110:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_acquire(...) \
 111:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_acquire(arch_cmpxchg64, __VA_ARGS__)
 112:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg64)
 113:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_acquire arch_cmpxchg64
 114:./include/linux/atomic/atomic-arch-fallback.h **** #else
 115:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg64_acquire_not_implemented(void);
 116:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_acquire(...) raw_cmpxchg64_acquire_not_implemented()
 117:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 118:./include/linux/atomic/atomic-arch-fallback.h **** 
 119:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg64_release)
 120:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_release arch_cmpxchg64_release
 121:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg64_relaxed)
 122:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_release(...) \
 123:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_release(arch_cmpxchg64, __VA_ARGS__)
 124:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg64)
 125:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_release arch_cmpxchg64
 126:./include/linux/atomic/atomic-arch-fallback.h **** #else
 127:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg64_release_not_implemented(void);
 128:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_release(...) raw_cmpxchg64_release_not_implemented()
 129:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 130:./include/linux/atomic/atomic-arch-fallback.h **** 
 131:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg64_relaxed)
 132:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_relaxed arch_cmpxchg64_relaxed
 133:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg64)
 134:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_relaxed arch_cmpxchg64
 135:./include/linux/atomic/atomic-arch-fallback.h **** #else
 136:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg64_relaxed_not_implemented(void);
 137:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_relaxed(...) raw_cmpxchg64_relaxed_not_implemented()
 138:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 139:./include/linux/atomic/atomic-arch-fallback.h **** 
 140:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg128)
 141:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128 arch_cmpxchg128
 142:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg128_relaxed)
 143:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128(...) \
 144:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_fence(arch_cmpxchg128, __VA_ARGS__)
 145:./include/linux/atomic/atomic-arch-fallback.h **** #else
 146:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg128_not_implemented(void);
 147:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128(...) raw_cmpxchg128_not_implemented()
 148:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 149:./include/linux/atomic/atomic-arch-fallback.h **** 
 150:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg128_acquire)
 151:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_acquire arch_cmpxchg128_acquire
 152:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg128_relaxed)
 153:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_acquire(...) \
 154:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_acquire(arch_cmpxchg128, __VA_ARGS__)
 155:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg128)
 156:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_acquire arch_cmpxchg128
 157:./include/linux/atomic/atomic-arch-fallback.h **** #else
 158:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg128_acquire_not_implemented(void);
 159:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_acquire(...) raw_cmpxchg128_acquire_not_implemented()
 160:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 161:./include/linux/atomic/atomic-arch-fallback.h **** 
 162:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg128_release)
ARM GAS  /tmp/cc0b6Xiw.s 			page 36


 163:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_release arch_cmpxchg128_release
 164:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg128_relaxed)
 165:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_release(...) \
 166:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_release(arch_cmpxchg128, __VA_ARGS__)
 167:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg128)
 168:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_release arch_cmpxchg128
 169:./include/linux/atomic/atomic-arch-fallback.h **** #else
 170:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg128_release_not_implemented(void);
 171:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_release(...) raw_cmpxchg128_release_not_implemented()
 172:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 173:./include/linux/atomic/atomic-arch-fallback.h **** 
 174:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_cmpxchg128_relaxed)
 175:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_relaxed arch_cmpxchg128_relaxed
 176:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_cmpxchg128)
 177:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_relaxed arch_cmpxchg128
 178:./include/linux/atomic/atomic-arch-fallback.h **** #else
 179:./include/linux/atomic/atomic-arch-fallback.h **** extern void raw_cmpxchg128_relaxed_not_implemented(void);
 180:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_relaxed(...) raw_cmpxchg128_relaxed_not_implemented()
 181:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 182:./include/linux/atomic/atomic-arch-fallback.h **** 
 183:./include/linux/atomic/atomic-arch-fallback.h **** 
 184:./include/linux/atomic/atomic-arch-fallback.h **** 
 185:./include/linux/atomic/atomic-arch-fallback.h **** 
 186:./include/linux/atomic/atomic-arch-fallback.h **** 
 187:./include/linux/atomic/atomic-arch-fallback.h **** 
 188:./include/linux/atomic/atomic-arch-fallback.h **** 
 189:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg)
 190:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg arch_try_cmpxchg
 191:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg_relaxed)
 192:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg(...) \
 193:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_fence(arch_try_cmpxchg, __VA_ARGS__)
 194:./include/linux/atomic/atomic-arch-fallback.h **** #else
 195:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg(_ptr, _oldp, _new) \
 196:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 197:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 198:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg((_ptr), ___o, (_new)); \
 199:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 200:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 201:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 202:./include/linux/atomic/atomic-arch-fallback.h **** })
 203:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 204:./include/linux/atomic/atomic-arch-fallback.h **** 
 205:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg_acquire)
 206:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_acquire arch_try_cmpxchg_acquire
 207:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg_relaxed)
 208:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_acquire(...) \
 209:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_acquire(arch_try_cmpxchg, __VA_ARGS__)
 210:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg)
 211:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_acquire arch_try_cmpxchg
 212:./include/linux/atomic/atomic-arch-fallback.h **** #else
 213:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_acquire(_ptr, _oldp, _new) \
 214:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 215:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 216:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg_acquire((_ptr), ___o, (_new)); \
 217:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 218:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 219:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
ARM GAS  /tmp/cc0b6Xiw.s 			page 37


 220:./include/linux/atomic/atomic-arch-fallback.h **** })
 221:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 222:./include/linux/atomic/atomic-arch-fallback.h **** 
 223:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg_release)
 224:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_release arch_try_cmpxchg_release
 225:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg_relaxed)
 226:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_release(...) \
 227:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_release(arch_try_cmpxchg, __VA_ARGS__)
 228:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg)
 229:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_release arch_try_cmpxchg
 230:./include/linux/atomic/atomic-arch-fallback.h **** #else
 231:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_release(_ptr, _oldp, _new) \
 232:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 233:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 234:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg_release((_ptr), ___o, (_new)); \
 235:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 236:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 237:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 238:./include/linux/atomic/atomic-arch-fallback.h **** })
 239:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 240:./include/linux/atomic/atomic-arch-fallback.h **** 
 241:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg_relaxed)
 242:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_relaxed arch_try_cmpxchg_relaxed
 243:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg)
 244:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_relaxed arch_try_cmpxchg
 245:./include/linux/atomic/atomic-arch-fallback.h **** #else
 246:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_relaxed(_ptr, _oldp, _new) \
 247:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 248:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 249:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg_relaxed((_ptr), ___o, (_new)); \
 250:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 251:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 252:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 253:./include/linux/atomic/atomic-arch-fallback.h **** })
 254:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 255:./include/linux/atomic/atomic-arch-fallback.h **** 
 256:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg64)
 257:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64 arch_try_cmpxchg64
 258:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg64_relaxed)
 259:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64(...) \
 260:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_fence(arch_try_cmpxchg64, __VA_ARGS__)
 261:./include/linux/atomic/atomic-arch-fallback.h **** #else
 262:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64(_ptr, _oldp, _new) \
 263:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 264:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 265:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg64((_ptr), ___o, (_new)); \
 266:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 267:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 268:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 269:./include/linux/atomic/atomic-arch-fallback.h **** })
 270:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 271:./include/linux/atomic/atomic-arch-fallback.h **** 
 272:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg64_acquire)
 273:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_acquire arch_try_cmpxchg64_acquire
 274:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg64_relaxed)
 275:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_acquire(...) \
 276:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_acquire(arch_try_cmpxchg64, __VA_ARGS__)
ARM GAS  /tmp/cc0b6Xiw.s 			page 38


 277:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg64)
 278:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_acquire arch_try_cmpxchg64
 279:./include/linux/atomic/atomic-arch-fallback.h **** #else
 280:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_acquire(_ptr, _oldp, _new) \
 281:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 282:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 283:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg64_acquire((_ptr), ___o, (_new)); \
 284:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 285:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 286:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 287:./include/linux/atomic/atomic-arch-fallback.h **** })
 288:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 289:./include/linux/atomic/atomic-arch-fallback.h **** 
 290:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg64_release)
 291:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_release arch_try_cmpxchg64_release
 292:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg64_relaxed)
 293:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_release(...) \
 294:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_release(arch_try_cmpxchg64, __VA_ARGS__)
 295:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg64)
 296:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_release arch_try_cmpxchg64
 297:./include/linux/atomic/atomic-arch-fallback.h **** #else
 298:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_release(_ptr, _oldp, _new) \
 299:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 300:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 301:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg64_release((_ptr), ___o, (_new)); \
 302:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 303:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 304:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 305:./include/linux/atomic/atomic-arch-fallback.h **** })
 306:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 307:./include/linux/atomic/atomic-arch-fallback.h **** 
 308:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg64_relaxed)
 309:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_relaxed arch_try_cmpxchg64_relaxed
 310:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg64)
 311:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_relaxed arch_try_cmpxchg64
 312:./include/linux/atomic/atomic-arch-fallback.h **** #else
 313:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_relaxed(_ptr, _oldp, _new) \
 314:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 315:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 316:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg64_relaxed((_ptr), ___o, (_new)); \
 317:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 318:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 319:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 320:./include/linux/atomic/atomic-arch-fallback.h **** })
 321:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 322:./include/linux/atomic/atomic-arch-fallback.h **** 
 323:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg128)
 324:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128 arch_try_cmpxchg128
 325:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg128_relaxed)
 326:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128(...) \
 327:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_fence(arch_try_cmpxchg128, __VA_ARGS__)
 328:./include/linux/atomic/atomic-arch-fallback.h **** #else
 329:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128(_ptr, _oldp, _new) \
 330:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 331:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 332:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg128((_ptr), ___o, (_new)); \
 333:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
ARM GAS  /tmp/cc0b6Xiw.s 			page 39


 334:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 335:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 336:./include/linux/atomic/atomic-arch-fallback.h **** })
 337:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 338:./include/linux/atomic/atomic-arch-fallback.h **** 
 339:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg128_acquire)
 340:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_acquire arch_try_cmpxchg128_acquire
 341:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg128_relaxed)
 342:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_acquire(...) \
 343:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_acquire(arch_try_cmpxchg128, __VA_ARGS__)
 344:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg128)
 345:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_acquire arch_try_cmpxchg128
 346:./include/linux/atomic/atomic-arch-fallback.h **** #else
 347:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_acquire(_ptr, _oldp, _new) \
 348:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 349:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 350:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg128_acquire((_ptr), ___o, (_new)); \
 351:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 352:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 353:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 354:./include/linux/atomic/atomic-arch-fallback.h **** })
 355:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 356:./include/linux/atomic/atomic-arch-fallback.h **** 
 357:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg128_release)
 358:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_release arch_try_cmpxchg128_release
 359:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg128_relaxed)
 360:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_release(...) \
 361:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_op_release(arch_try_cmpxchg128, __VA_ARGS__)
 362:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg128)
 363:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_release arch_try_cmpxchg128
 364:./include/linux/atomic/atomic-arch-fallback.h **** #else
 365:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_release(_ptr, _oldp, _new) \
 366:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 367:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 368:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg128_release((_ptr), ___o, (_new)); \
 369:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 370:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 371:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 372:./include/linux/atomic/atomic-arch-fallback.h **** })
 373:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 374:./include/linux/atomic/atomic-arch-fallback.h **** 
 375:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_try_cmpxchg128_relaxed)
 376:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_relaxed arch_try_cmpxchg128_relaxed
 377:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_try_cmpxchg128)
 378:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_relaxed arch_try_cmpxchg128
 379:./include/linux/atomic/atomic-arch-fallback.h **** #else
 380:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_relaxed(_ptr, _oldp, _new) \
 381:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 382:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 383:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg128_relaxed((_ptr), ___o, (_new)); \
 384:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 385:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 386:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 387:./include/linux/atomic/atomic-arch-fallback.h **** })
 388:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 389:./include/linux/atomic/atomic-arch-fallback.h **** 
 390:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg_local arch_cmpxchg_local
ARM GAS  /tmp/cc0b6Xiw.s 			page 40


 391:./include/linux/atomic/atomic-arch-fallback.h **** 
 392:./include/linux/atomic/atomic-arch-fallback.h **** #ifdef arch_try_cmpxchg_local
 393:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_local arch_try_cmpxchg_local
 394:./include/linux/atomic/atomic-arch-fallback.h **** #else
 395:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg_local(_ptr, _oldp, _new) \
 396:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 397:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 398:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg_local((_ptr), ___o, (_new)); \
 399:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 400:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 401:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 402:./include/linux/atomic/atomic-arch-fallback.h **** })
 403:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 404:./include/linux/atomic/atomic-arch-fallback.h **** 
 405:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg64_local arch_cmpxchg64_local
 406:./include/linux/atomic/atomic-arch-fallback.h **** 
 407:./include/linux/atomic/atomic-arch-fallback.h **** #ifdef arch_try_cmpxchg64_local
 408:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_local arch_try_cmpxchg64_local
 409:./include/linux/atomic/atomic-arch-fallback.h **** #else
 410:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg64_local(_ptr, _oldp, _new) \
 411:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 412:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 413:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg64_local((_ptr), ___o, (_new)); \
 414:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 415:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 416:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 417:./include/linux/atomic/atomic-arch-fallback.h **** })
 418:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 419:./include/linux/atomic/atomic-arch-fallback.h **** 
 420:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_cmpxchg128_local arch_cmpxchg128_local
 421:./include/linux/atomic/atomic-arch-fallback.h **** 
 422:./include/linux/atomic/atomic-arch-fallback.h **** #ifdef arch_try_cmpxchg128_local
 423:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_local arch_try_cmpxchg128_local
 424:./include/linux/atomic/atomic-arch-fallback.h **** #else
 425:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_try_cmpxchg128_local(_ptr, _oldp, _new) \
 426:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 427:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 428:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_cmpxchg128_local((_ptr), ___o, (_new)); \
 429:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 430:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 431:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 432:./include/linux/atomic/atomic-arch-fallback.h **** })
 433:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 434:./include/linux/atomic/atomic-arch-fallback.h **** 
 435:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_sync_cmpxchg arch_sync_cmpxchg
 436:./include/linux/atomic/atomic-arch-fallback.h **** 
 437:./include/linux/atomic/atomic-arch-fallback.h **** #ifdef arch_sync_try_cmpxchg
 438:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_sync_try_cmpxchg arch_sync_try_cmpxchg
 439:./include/linux/atomic/atomic-arch-fallback.h **** #else
 440:./include/linux/atomic/atomic-arch-fallback.h **** #define raw_sync_try_cmpxchg(_ptr, _oldp, _new) \
 441:./include/linux/atomic/atomic-arch-fallback.h **** ({ \
 442:./include/linux/atomic/atomic-arch-fallback.h **** 	typeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \
 443:./include/linux/atomic/atomic-arch-fallback.h **** 	___r = raw_sync_cmpxchg((_ptr), ___o, (_new)); \
 444:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(___r != ___o)) \
 445:./include/linux/atomic/atomic-arch-fallback.h **** 		*___op = ___r; \
 446:./include/linux/atomic/atomic-arch-fallback.h **** 	likely(___r == ___o); \
 447:./include/linux/atomic/atomic-arch-fallback.h **** })
ARM GAS  /tmp/cc0b6Xiw.s 			page 41


 448:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 449:./include/linux/atomic/atomic-arch-fallback.h **** 
 450:./include/linux/atomic/atomic-arch-fallback.h **** /**
 451:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_read() - atomic load with relaxed ordering
 452:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 453:./include/linux/atomic/atomic-arch-fallback.h ****  *
 454:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically loads the value of @v with relaxed ordering.
 455:./include/linux/atomic/atomic-arch-fallback.h ****  *
 456:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_read() elsewhere.
 457:./include/linux/atomic/atomic-arch-fallback.h ****  *
 458:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The value loaded from @v.
 459:./include/linux/atomic/atomic-arch-fallback.h ****  */
 460:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 461:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_read(const atomic_t *v)
 462:./include/linux/atomic/atomic-arch-fallback.h **** {
 463:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_read(v);
 464:./include/linux/atomic/atomic-arch-fallback.h **** }
 465:./include/linux/atomic/atomic-arch-fallback.h **** 
 466:./include/linux/atomic/atomic-arch-fallback.h **** /**
 467:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_read_acquire() - atomic load with acquire ordering
 468:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 469:./include/linux/atomic/atomic-arch-fallback.h ****  *
 470:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically loads the value of @v with acquire ordering.
 471:./include/linux/atomic/atomic-arch-fallback.h ****  *
 472:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_read_acquire() elsewhere.
 473:./include/linux/atomic/atomic-arch-fallback.h ****  *
 474:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The value loaded from @v.
 475:./include/linux/atomic/atomic-arch-fallback.h ****  */
 476:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 477:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_read_acquire(const atomic_t *v)
 478:./include/linux/atomic/atomic-arch-fallback.h **** {
 479:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_read_acquire)
 480:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_read_acquire(v);
 481:./include/linux/atomic/atomic-arch-fallback.h **** #else
 482:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
 483:./include/linux/atomic/atomic-arch-fallback.h **** 
 484:./include/linux/atomic/atomic-arch-fallback.h **** 	if (__native_word(atomic_t)) {
 485:./include/linux/atomic/atomic-arch-fallback.h **** 		ret = smp_load_acquire(&(v)->counter);
 486:./include/linux/atomic/atomic-arch-fallback.h **** 	} else {
 487:./include/linux/atomic/atomic-arch-fallback.h **** 		ret = raw_atomic_read(v);
 488:./include/linux/atomic/atomic-arch-fallback.h **** 		__atomic_acquire_fence();
 489:./include/linux/atomic/atomic-arch-fallback.h **** 	}
 490:./include/linux/atomic/atomic-arch-fallback.h **** 
 491:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 492:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 493:./include/linux/atomic/atomic-arch-fallback.h **** }
 494:./include/linux/atomic/atomic-arch-fallback.h **** 
 495:./include/linux/atomic/atomic-arch-fallback.h **** /**
 496:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_set() - atomic set with relaxed ordering
 497:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 498:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to assign
 499:./include/linux/atomic/atomic-arch-fallback.h ****  *
 500:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically sets @v to @i with relaxed ordering.
 501:./include/linux/atomic/atomic-arch-fallback.h ****  *
 502:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_set() elsewhere.
 503:./include/linux/atomic/atomic-arch-fallback.h ****  *
 504:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
ARM GAS  /tmp/cc0b6Xiw.s 			page 42


 505:./include/linux/atomic/atomic-arch-fallback.h ****  */
 506:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
 507:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_set(atomic_t *v, int i)
 556              		.loc 8 507 1 view .LVU148
 557              	.LBB90:
 508:./include/linux/atomic/atomic-arch-fallback.h **** {
 509:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_set(v, i);
 558              		.loc 8 509 2 view .LVU149
 559              		.loc 8 509 2 view .LVU150
 560              	.LBB91:
 561              		.loc 8 509 2 view .LVU151
 562              		.loc 8 509 2 view .LVU152
 563              	.LBE91:
 564              		.loc 8 509 2 discriminator 2 view .LVU153
 565              		.loc 8 509 2 discriminator 2 view .LVU154
 566              		.loc 8 509 2 discriminator 2 view .LVU155
 567 004a 0122     		movs	r2, #1
 568 004c 6261     		str	r2, [r4, #20]
 569              		.loc 8 509 2 discriminator 2 view .LVU156
 570              		.loc 8 509 2 discriminator 2 view .LVU157
 571              	.LVL49:
 572              		.loc 8 509 2 is_stmt 0 discriminator 2 view .LVU158
 573              	.LBE90:
 574              	.LBE89:
 575              	.LBE88:
  56:./block/bio.c ****     bio->bi_iter.bi_sector = 0;
 576              		.loc 1 56 5 is_stmt 1 view .LVU159
 577              	.LBB92:
 578              	.LBI92:
  65:./include/linux/atomic/atomic-instrumented.h **** {
 579              		.loc 7 65 1 view .LVU160
  67:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_set(v, i);
 580              		.loc 7 67 2 view .LVU161
 581              		.loc 7 68 2 view .LVU162
 582              	.LBB93:
 583              	.LBI93:
 507:./include/linux/atomic/atomic-arch-fallback.h **** {
 584              		.loc 8 507 1 view .LVU163
 585              	.LBB94:
 586              		.loc 8 509 2 view .LVU164
 587              		.loc 8 509 2 view .LVU165
 588              	.LBB95:
 589              		.loc 8 509 2 view .LVU166
 590              		.loc 8 509 2 view .LVU167
 591              	.LBE95:
 592              		.loc 8 509 2 discriminator 2 view .LVU168
 593              		.loc 8 509 2 discriminator 2 view .LVU169
 594              		.loc 8 509 2 discriminator 2 view .LVU170
 595 004e A263     		str	r2, [r4, #56]
 596              		.loc 8 509 2 discriminator 2 view .LVU171
 597              		.loc 8 509 2 discriminator 2 view .LVU172
 598              	.LVL50:
 599              		.loc 8 509 2 is_stmt 0 discriminator 2 view .LVU173
 600              	.LBE94:
 601              	.LBE93:
 602              	.LBE92:
  57:./block/bio.c ****     bio->bi_iter.bi_size = 0;
ARM GAS  /tmp/cc0b6Xiw.s 			page 43


 603              		.loc 1 57 5 is_stmt 1 view .LVU174
  57:./block/bio.c ****     bio->bi_iter.bi_size = 0;
 604              		.loc 1 57 28 is_stmt 0 view .LVU175
 605 0050 A361     		str	r3, [r4, #24]
  58:./block/bio.c ****     bio->bi_iter.bi_idx = 0;
 606              		.loc 1 58 5 is_stmt 1 view .LVU176
  58:./block/bio.c ****     bio->bi_iter.bi_idx = 0;
 607              		.loc 1 58 26 is_stmt 0 view .LVU177
 608 0052 E361     		str	r3, [r4, #28]
  59:./block/bio.c ****     bio->bi_iter.bi_bvec_done = 0;
 609              		.loc 1 59 5 is_stmt 1 view .LVU178
  59:./block/bio.c ****     bio->bi_iter.bi_bvec_done = 0;
 610              		.loc 1 59 25 is_stmt 0 view .LVU179
 611 0054 2362     		str	r3, [r4, #32]
  60:./block/bio.c ****     bio->bi_vcnt = 0;
 612              		.loc 1 60 5 is_stmt 1 view .LVU180
  60:./block/bio.c ****     bio->bi_vcnt = 0;
 613              		.loc 1 60 31 is_stmt 0 view .LVU181
 614 0056 6362     		str	r3, [r4, #36]
  61:./block/bio.c ****     bio->bi_end_io = NULL;
 615              		.loc 1 61 5 is_stmt 1 view .LVU182
  61:./block/bio.c ****     bio->bi_end_io = NULL;
 616              		.loc 1 61 18 is_stmt 0 view .LVU183
 617 0058 A386     		strh	r3, [r4, #52]	@ movhi
  62:./block/bio.c ****     bio->bi_private = NULL;
 618              		.loc 1 62 5 is_stmt 1 view .LVU184
  62:./block/bio.c ****     bio->bi_private = NULL;
 619              		.loc 1 62 20 is_stmt 0 view .LVU185
 620 005a E362     		str	r3, [r4, #44]
  63:./block/bio.c ****     bio->bi_bdev = bdev;
 621              		.loc 1 63 5 is_stmt 1 view .LVU186
  63:./block/bio.c ****     bio->bi_bdev = bdev;
 622              		.loc 1 63 21 is_stmt 0 view .LVU187
 623 005c 2363     		str	r3, [r4, #48]
  64:./block/bio.c ****     bio->bi_opf = opf;
 624              		.loc 1 64 5 is_stmt 1 view .LVU188
  64:./block/bio.c ****     bio->bi_opf = opf;
 625              		.loc 1 64 18 is_stmt 0 view .LVU189
 626 005e C4F80480 		str	r8, [r4, #4]
  65:./block/bio.c ****     return bio;
 627              		.loc 1 65 5 is_stmt 1 view .LVU190
  65:./block/bio.c ****     return bio;
 628              		.loc 1 65 17 is_stmt 0 view .LVU191
 629 0062 A760     		str	r7, [r4, #8]
  66:./block/bio.c **** }
 630              		.loc 1 66 5 is_stmt 1 view .LVU192
 631              	.LVL51:
 632              	.L21:
  67:./block/bio.c ****  
 633              		.loc 1 67 1 is_stmt 0 view .LVU193
 634 0064 2046     		mov	r0, r4
 635 0066 BDE8F087 		pop	{r4, r5, r6, r7, r8, r9, r10, pc}
 636              	.LVL52:
 637              	.L27:
  20:./block/bio.c **** 
 638              		.loc 1 20 10 view .LVU194
 639 006a 4FF0000A 		mov	r10, #0
ARM GAS  /tmp/cc0b6Xiw.s 			page 44


  27:./block/bio.c ****     
 640              		.loc 1 27 18 view .LVU195
 641 006e 4426     		movs	r6, #68
 642 0070 D4E7     		b	.L22
 643              	.LVL53:
 644              	.L24:
  43:./block/bio.c ****         if (!bio->bi_io_vec) {
 645              		.loc 1 43 9 is_stmt 1 view .LVU196
 646              	.LBB96:
 647              	.LBI96:
 381:./include/linux/slab.h **** }
 382:./include/linux/slab.h **** 
 383:./include/linux/slab.h **** static void inline kfree(const void *ptr){
 384:./include/linux/slab.h **** 	__sfree__((void*)ptr);
 385:./include/linux/slab.h **** }
 386:./include/linux/slab.h **** 
 387:./include/linux/slab.h **** 
 388:./include/linux/slab.h **** 
 389:./include/linux/slab.h **** static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags){
 648              		.loc 6 389 21 view .LVU197
 649              	.LBB97:
 390:./include/linux/slab.h **** 	return kmalloc(n * size, flags);
 650              		.loc 6 390 2 view .LVU198
 651              		.loc 6 390 9 is_stmt 0 view .LVU199
 652 0072 05EB4500 		add	r0, r5, r5, lsl #1
 653              	.LVL54:
 654              	.LBB98:
 655              	.LBI98:
 379:./include/linux/slab.h **** 	return __smalloc__((u32)size,flags);
 656              		.loc 6 379 21 is_stmt 1 view .LVU200
 657              	.LBB99:
 380:./include/linux/slab.h **** }
 658              		.loc 6 380 2 view .LVU201
 380:./include/linux/slab.h **** }
 659              		.loc 6 380 9 is_stmt 0 view .LVU202
 660 0076 4946     		mov	r1, r9
 661 0078 8000     		lsls	r0, r0, #2
 662 007a FFF7FEFF 		bl	__smalloc__
 663              	.LVL55:
 664 007e 0646     		mov	r6, r0
 665              	.LVL56:
 380:./include/linux/slab.h **** }
 666              		.loc 6 380 9 view .LVU203
 667              	.LBE99:
 668              	.LBE98:
 669              	.LBE97:
 670              	.LBE96:
  43:./block/bio.c ****         if (!bio->bi_io_vec) {
 671              		.loc 1 43 24 discriminator 1 view .LVU204
 672 0080 E063     		str	r0, [r4, #60]
  44:./block/bio.c ****             kfree(bio);
 673              		.loc 1 44 9 is_stmt 1 view .LVU205
  44:./block/bio.c ****             kfree(bio);
 674              		.loc 1 44 12 is_stmt 0 view .LVU206
 675 0082 08B1     		cbz	r0, .L29
  48:./block/bio.c ****     }
 676              		.loc 1 48 9 is_stmt 1 view .LVU207
ARM GAS  /tmp/cc0b6Xiw.s 			page 45


  48:./block/bio.c ****     }
 677              		.loc 1 48 26 is_stmt 0 view .LVU208
 678 0084 E586     		strh	r5, [r4, #54]	@ movhi
 679 0086 DAE7     		b	.L25
 680              	.L29:
  45:./block/bio.c ****             return NULL;
 681              		.loc 1 45 13 is_stmt 1 view .LVU209
 682              	.LVL57:
 683              	.LBB100:
 684              	.LBI100:
 383:./include/linux/slab.h **** 	__sfree__((void*)ptr);
 685              		.loc 6 383 20 view .LVU210
 686              	.LBB101:
 384:./include/linux/slab.h **** }
 687              		.loc 6 384 2 view .LVU211
 688 0088 2046     		mov	r0, r4
 689 008a FFF7FEFF 		bl	__sfree__
 690              	.LVL58:
 384:./include/linux/slab.h **** }
 691              		.loc 6 384 2 is_stmt 0 view .LVU212
 692              	.LBE101:
 693              	.LBE100:
  46:./block/bio.c ****         }
 694              		.loc 1 46 13 is_stmt 1 view .LVU213
  46:./block/bio.c ****         }
 695              		.loc 1 46 20 is_stmt 0 view .LVU214
 696 008e 3446     		mov	r4, r6
 697              	.LVL59:
  46:./block/bio.c ****         }
 698              		.loc 1 46 20 view .LVU215
 699 0090 E8E7     		b	.L21
 700              		.cfi_endproc
 701              	.LFE1036:
 703              		.section	.text.bio_put,"ax",%progbits
 704              		.align	1
 705              		.global	bio_put
 706              		.syntax unified
 707              		.thumb
 708              		.thumb_func
 710              	bio_put:
 711              	.LVL60:
 712              	.LFB1037:
  70:./block/bio.c ****     if(!bio)return;
 713              		.loc 1 70 1 is_stmt 1 view -0
 714              		.cfi_startproc
 715              		@ args = 0, pretend = 0, frame = 0
 716              		@ frame_needed = 0, uses_anonymous_args = 0
  71:./block/bio.c ****     if (atomic_dec_and_test(&bio->__bi_cnt)) {
 717              		.loc 1 71 5 view .LVU217
  71:./block/bio.c ****     if (atomic_dec_and_test(&bio->__bi_cnt)) {
 718              		.loc 1 71 7 is_stmt 0 view .LVU218
 719 0000 C0B1     		cbz	r0, .L34
  70:./block/bio.c ****     if(!bio)return;
 720              		.loc 1 70 1 view .LVU219
 721 0002 10B5     		push	{r4, lr}
 722              	.LCFI11:
 723              		.cfi_def_cfa_offset 8
ARM GAS  /tmp/cc0b6Xiw.s 			page 46


 724              		.cfi_offset 4, -8
 725              		.cfi_offset 14, -4
 726 0004 0446     		mov	r4, r0
  72:./block/bio.c ****         if (bio->bi_io_vec != bio->bi_inline_vecs) {
 727              		.loc 1 72 5 is_stmt 1 view .LVU220
  72:./block/bio.c ****         if (bio->bi_io_vec != bio->bi_inline_vecs) {
 728              		.loc 1 72 9 is_stmt 0 view .LVU221
 729 0006 00F13803 		add	r3, r0, #56
 730              	.LBB102:
 731              	.LBI102:
  69:./include/linux/atomic/atomic-instrumented.h **** }
  70:./include/linux/atomic/atomic-instrumented.h **** 
  71:./include/linux/atomic/atomic-instrumented.h **** /**
  72:./include/linux/atomic/atomic-instrumented.h ****  * atomic_set_release() - atomic set with release ordering
  73:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
  74:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to assign
  75:./include/linux/atomic/atomic-instrumented.h ****  *
  76:./include/linux/atomic/atomic-instrumented.h ****  * Atomically sets @v to @i with release ordering.
  77:./include/linux/atomic/atomic-instrumented.h ****  *
  78:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_set_release() there.
  79:./include/linux/atomic/atomic-instrumented.h ****  *
  80:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
  81:./include/linux/atomic/atomic-instrumented.h ****  */
  82:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
  83:./include/linux/atomic/atomic-instrumented.h **** atomic_set_release(atomic_t *v, int i)
  84:./include/linux/atomic/atomic-instrumented.h **** {
  85:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
  86:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_write(v, sizeof(*v));
  87:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_set_release(v, i);
  88:./include/linux/atomic/atomic-instrumented.h **** }
  89:./include/linux/atomic/atomic-instrumented.h **** 
  90:./include/linux/atomic/atomic-instrumented.h **** /**
  91:./include/linux/atomic/atomic-instrumented.h ****  * atomic_add() - atomic add with relaxed ordering
  92:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
  93:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
  94:./include/linux/atomic/atomic-instrumented.h ****  *
  95:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with relaxed ordering.
  96:./include/linux/atomic/atomic-instrumented.h ****  *
  97:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_add() there.
  98:./include/linux/atomic/atomic-instrumented.h ****  *
  99:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
 100:./include/linux/atomic/atomic-instrumented.h ****  */
 101:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
 102:./include/linux/atomic/atomic-instrumented.h **** atomic_add(int i, atomic_t *v)
 103:./include/linux/atomic/atomic-instrumented.h **** {
 104:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 105:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_add(i, v);
 106:./include/linux/atomic/atomic-instrumented.h **** }
 107:./include/linux/atomic/atomic-instrumented.h **** 
 108:./include/linux/atomic/atomic-instrumented.h **** /**
 109:./include/linux/atomic/atomic-instrumented.h ****  * atomic_add_return() - atomic add with full ordering
 110:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
 111:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 112:./include/linux/atomic/atomic-instrumented.h ****  *
 113:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with full ordering.
 114:./include/linux/atomic/atomic-instrumented.h ****  *
 115:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_add_return() there.
ARM GAS  /tmp/cc0b6Xiw.s 			page 47


 116:./include/linux/atomic/atomic-instrumented.h ****  *
 117:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 118:./include/linux/atomic/atomic-instrumented.h ****  */
 119:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 120:./include/linux/atomic/atomic-instrumented.h **** atomic_add_return(int i, atomic_t *v)
 121:./include/linux/atomic/atomic-instrumented.h **** {
 122:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 123:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 124:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_add_return(i, v);
 125:./include/linux/atomic/atomic-instrumented.h **** }
 126:./include/linux/atomic/atomic-instrumented.h **** 
 127:./include/linux/atomic/atomic-instrumented.h **** /**
 128:./include/linux/atomic/atomic-instrumented.h ****  * atomic_add_return_acquire() - atomic add with acquire ordering
 129:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
 130:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 131:./include/linux/atomic/atomic-instrumented.h ****  *
 132:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with acquire ordering.
 133:./include/linux/atomic/atomic-instrumented.h ****  *
 134:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_add_return_acquire() there.
 135:./include/linux/atomic/atomic-instrumented.h ****  *
 136:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 137:./include/linux/atomic/atomic-instrumented.h ****  */
 138:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 139:./include/linux/atomic/atomic-instrumented.h **** atomic_add_return_acquire(int i, atomic_t *v)
 140:./include/linux/atomic/atomic-instrumented.h **** {
 141:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 142:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_add_return_acquire(i, v);
 143:./include/linux/atomic/atomic-instrumented.h **** }
 144:./include/linux/atomic/atomic-instrumented.h **** 
 145:./include/linux/atomic/atomic-instrumented.h **** /**
 146:./include/linux/atomic/atomic-instrumented.h ****  * atomic_add_return_release() - atomic add with release ordering
 147:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
 148:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 149:./include/linux/atomic/atomic-instrumented.h ****  *
 150:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with release ordering.
 151:./include/linux/atomic/atomic-instrumented.h ****  *
 152:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_add_return_release() there.
 153:./include/linux/atomic/atomic-instrumented.h ****  *
 154:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 155:./include/linux/atomic/atomic-instrumented.h ****  */
 156:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 157:./include/linux/atomic/atomic-instrumented.h **** atomic_add_return_release(int i, atomic_t *v)
 158:./include/linux/atomic/atomic-instrumented.h **** {
 159:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 160:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 161:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_add_return_release(i, v);
 162:./include/linux/atomic/atomic-instrumented.h **** }
 163:./include/linux/atomic/atomic-instrumented.h **** 
 164:./include/linux/atomic/atomic-instrumented.h **** /**
 165:./include/linux/atomic/atomic-instrumented.h ****  * atomic_add_return_relaxed() - atomic add with relaxed ordering
 166:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
 167:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 168:./include/linux/atomic/atomic-instrumented.h ****  *
 169:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with relaxed ordering.
 170:./include/linux/atomic/atomic-instrumented.h ****  *
 171:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_add_return_relaxed() there.
 172:./include/linux/atomic/atomic-instrumented.h ****  *
ARM GAS  /tmp/cc0b6Xiw.s 			page 48


 173:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 174:./include/linux/atomic/atomic-instrumented.h ****  */
 175:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 176:./include/linux/atomic/atomic-instrumented.h **** atomic_add_return_relaxed(int i, atomic_t *v)
 177:./include/linux/atomic/atomic-instrumented.h **** {
 178:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 179:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_add_return_relaxed(i, v);
 180:./include/linux/atomic/atomic-instrumented.h **** }
 181:./include/linux/atomic/atomic-instrumented.h **** 
 182:./include/linux/atomic/atomic-instrumented.h **** /**
 183:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_add() - atomic add with full ordering
 184:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
 185:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 186:./include/linux/atomic/atomic-instrumented.h ****  *
 187:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with full ordering.
 188:./include/linux/atomic/atomic-instrumented.h ****  *
 189:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_add() there.
 190:./include/linux/atomic/atomic-instrumented.h ****  *
 191:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 192:./include/linux/atomic/atomic-instrumented.h ****  */
 193:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 194:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_add(int i, atomic_t *v)
 195:./include/linux/atomic/atomic-instrumented.h **** {
 196:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 197:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 198:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_add(i, v);
 199:./include/linux/atomic/atomic-instrumented.h **** }
 200:./include/linux/atomic/atomic-instrumented.h **** 
 201:./include/linux/atomic/atomic-instrumented.h **** /**
 202:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_add_acquire() - atomic add with acquire ordering
 203:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
 204:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 205:./include/linux/atomic/atomic-instrumented.h ****  *
 206:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with acquire ordering.
 207:./include/linux/atomic/atomic-instrumented.h ****  *
 208:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_add_acquire() there.
 209:./include/linux/atomic/atomic-instrumented.h ****  *
 210:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 211:./include/linux/atomic/atomic-instrumented.h ****  */
 212:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 213:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_add_acquire(int i, atomic_t *v)
 214:./include/linux/atomic/atomic-instrumented.h **** {
 215:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 216:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_add_acquire(i, v);
 217:./include/linux/atomic/atomic-instrumented.h **** }
 218:./include/linux/atomic/atomic-instrumented.h **** 
 219:./include/linux/atomic/atomic-instrumented.h **** /**
 220:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_add_release() - atomic add with release ordering
 221:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
 222:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 223:./include/linux/atomic/atomic-instrumented.h ****  *
 224:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with release ordering.
 225:./include/linux/atomic/atomic-instrumented.h ****  *
 226:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_add_release() there.
 227:./include/linux/atomic/atomic-instrumented.h ****  *
 228:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 229:./include/linux/atomic/atomic-instrumented.h ****  */
ARM GAS  /tmp/cc0b6Xiw.s 			page 49


 230:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 231:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_add_release(int i, atomic_t *v)
 232:./include/linux/atomic/atomic-instrumented.h **** {
 233:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 234:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 235:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_add_release(i, v);
 236:./include/linux/atomic/atomic-instrumented.h **** }
 237:./include/linux/atomic/atomic-instrumented.h **** 
 238:./include/linux/atomic/atomic-instrumented.h **** /**
 239:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_add_relaxed() - atomic add with relaxed ordering
 240:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to add
 241:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 242:./include/linux/atomic/atomic-instrumented.h ****  *
 243:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + @i) with relaxed ordering.
 244:./include/linux/atomic/atomic-instrumented.h ****  *
 245:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_add_relaxed() there.
 246:./include/linux/atomic/atomic-instrumented.h ****  *
 247:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 248:./include/linux/atomic/atomic-instrumented.h ****  */
 249:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 250:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_add_relaxed(int i, atomic_t *v)
 251:./include/linux/atomic/atomic-instrumented.h **** {
 252:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 253:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_add_relaxed(i, v);
 254:./include/linux/atomic/atomic-instrumented.h **** }
 255:./include/linux/atomic/atomic-instrumented.h **** 
 256:./include/linux/atomic/atomic-instrumented.h **** /**
 257:./include/linux/atomic/atomic-instrumented.h ****  * atomic_sub() - atomic subtract with relaxed ordering
 258:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 259:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 260:./include/linux/atomic/atomic-instrumented.h ****  *
 261:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with relaxed ordering.
 262:./include/linux/atomic/atomic-instrumented.h ****  *
 263:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_sub() there.
 264:./include/linux/atomic/atomic-instrumented.h ****  *
 265:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
 266:./include/linux/atomic/atomic-instrumented.h ****  */
 267:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
 268:./include/linux/atomic/atomic-instrumented.h **** atomic_sub(int i, atomic_t *v)
 269:./include/linux/atomic/atomic-instrumented.h **** {
 270:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 271:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_sub(i, v);
 272:./include/linux/atomic/atomic-instrumented.h **** }
 273:./include/linux/atomic/atomic-instrumented.h **** 
 274:./include/linux/atomic/atomic-instrumented.h **** /**
 275:./include/linux/atomic/atomic-instrumented.h ****  * atomic_sub_return() - atomic subtract with full ordering
 276:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 277:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 278:./include/linux/atomic/atomic-instrumented.h ****  *
 279:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with full ordering.
 280:./include/linux/atomic/atomic-instrumented.h ****  *
 281:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_sub_return() there.
 282:./include/linux/atomic/atomic-instrumented.h ****  *
 283:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 284:./include/linux/atomic/atomic-instrumented.h ****  */
 285:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 286:./include/linux/atomic/atomic-instrumented.h **** atomic_sub_return(int i, atomic_t *v)
ARM GAS  /tmp/cc0b6Xiw.s 			page 50


 287:./include/linux/atomic/atomic-instrumented.h **** {
 288:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 289:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 290:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_sub_return(i, v);
 291:./include/linux/atomic/atomic-instrumented.h **** }
 292:./include/linux/atomic/atomic-instrumented.h **** 
 293:./include/linux/atomic/atomic-instrumented.h **** /**
 294:./include/linux/atomic/atomic-instrumented.h ****  * atomic_sub_return_acquire() - atomic subtract with acquire ordering
 295:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 296:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 297:./include/linux/atomic/atomic-instrumented.h ****  *
 298:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with acquire ordering.
 299:./include/linux/atomic/atomic-instrumented.h ****  *
 300:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_sub_return_acquire() there.
 301:./include/linux/atomic/atomic-instrumented.h ****  *
 302:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 303:./include/linux/atomic/atomic-instrumented.h ****  */
 304:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 305:./include/linux/atomic/atomic-instrumented.h **** atomic_sub_return_acquire(int i, atomic_t *v)
 306:./include/linux/atomic/atomic-instrumented.h **** {
 307:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 308:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_sub_return_acquire(i, v);
 309:./include/linux/atomic/atomic-instrumented.h **** }
 310:./include/linux/atomic/atomic-instrumented.h **** 
 311:./include/linux/atomic/atomic-instrumented.h **** /**
 312:./include/linux/atomic/atomic-instrumented.h ****  * atomic_sub_return_release() - atomic subtract with release ordering
 313:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 314:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 315:./include/linux/atomic/atomic-instrumented.h ****  *
 316:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with release ordering.
 317:./include/linux/atomic/atomic-instrumented.h ****  *
 318:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_sub_return_release() there.
 319:./include/linux/atomic/atomic-instrumented.h ****  *
 320:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 321:./include/linux/atomic/atomic-instrumented.h ****  */
 322:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 323:./include/linux/atomic/atomic-instrumented.h **** atomic_sub_return_release(int i, atomic_t *v)
 324:./include/linux/atomic/atomic-instrumented.h **** {
 325:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 326:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 327:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_sub_return_release(i, v);
 328:./include/linux/atomic/atomic-instrumented.h **** }
 329:./include/linux/atomic/atomic-instrumented.h **** 
 330:./include/linux/atomic/atomic-instrumented.h **** /**
 331:./include/linux/atomic/atomic-instrumented.h ****  * atomic_sub_return_relaxed() - atomic subtract with relaxed ordering
 332:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 333:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 334:./include/linux/atomic/atomic-instrumented.h ****  *
 335:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with relaxed ordering.
 336:./include/linux/atomic/atomic-instrumented.h ****  *
 337:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_sub_return_relaxed() there.
 338:./include/linux/atomic/atomic-instrumented.h ****  *
 339:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 340:./include/linux/atomic/atomic-instrumented.h ****  */
 341:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 342:./include/linux/atomic/atomic-instrumented.h **** atomic_sub_return_relaxed(int i, atomic_t *v)
 343:./include/linux/atomic/atomic-instrumented.h **** {
ARM GAS  /tmp/cc0b6Xiw.s 			page 51


 344:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 345:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_sub_return_relaxed(i, v);
 346:./include/linux/atomic/atomic-instrumented.h **** }
 347:./include/linux/atomic/atomic-instrumented.h **** 
 348:./include/linux/atomic/atomic-instrumented.h **** /**
 349:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_sub() - atomic subtract with full ordering
 350:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 351:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 352:./include/linux/atomic/atomic-instrumented.h ****  *
 353:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with full ordering.
 354:./include/linux/atomic/atomic-instrumented.h ****  *
 355:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_sub() there.
 356:./include/linux/atomic/atomic-instrumented.h ****  *
 357:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 358:./include/linux/atomic/atomic-instrumented.h ****  */
 359:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 360:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_sub(int i, atomic_t *v)
 361:./include/linux/atomic/atomic-instrumented.h **** {
 362:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 363:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 364:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_sub(i, v);
 365:./include/linux/atomic/atomic-instrumented.h **** }
 366:./include/linux/atomic/atomic-instrumented.h **** 
 367:./include/linux/atomic/atomic-instrumented.h **** /**
 368:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_sub_acquire() - atomic subtract with acquire ordering
 369:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 370:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 371:./include/linux/atomic/atomic-instrumented.h ****  *
 372:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with acquire ordering.
 373:./include/linux/atomic/atomic-instrumented.h ****  *
 374:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_sub_acquire() there.
 375:./include/linux/atomic/atomic-instrumented.h ****  *
 376:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 377:./include/linux/atomic/atomic-instrumented.h ****  */
 378:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 379:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_sub_acquire(int i, atomic_t *v)
 380:./include/linux/atomic/atomic-instrumented.h **** {
 381:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 382:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_sub_acquire(i, v);
 383:./include/linux/atomic/atomic-instrumented.h **** }
 384:./include/linux/atomic/atomic-instrumented.h **** 
 385:./include/linux/atomic/atomic-instrumented.h **** /**
 386:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_sub_release() - atomic subtract with release ordering
 387:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 388:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 389:./include/linux/atomic/atomic-instrumented.h ****  *
 390:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with release ordering.
 391:./include/linux/atomic/atomic-instrumented.h ****  *
 392:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_sub_release() there.
 393:./include/linux/atomic/atomic-instrumented.h ****  *
 394:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 395:./include/linux/atomic/atomic-instrumented.h ****  */
 396:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 397:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_sub_release(int i, atomic_t *v)
 398:./include/linux/atomic/atomic-instrumented.h **** {
 399:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 400:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
ARM GAS  /tmp/cc0b6Xiw.s 			page 52


 401:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_sub_release(i, v);
 402:./include/linux/atomic/atomic-instrumented.h **** }
 403:./include/linux/atomic/atomic-instrumented.h **** 
 404:./include/linux/atomic/atomic-instrumented.h **** /**
 405:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_sub_relaxed() - atomic subtract with relaxed ordering
 406:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
 407:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 408:./include/linux/atomic/atomic-instrumented.h ****  *
 409:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with relaxed ordering.
 410:./include/linux/atomic/atomic-instrumented.h ****  *
 411:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_sub_relaxed() there.
 412:./include/linux/atomic/atomic-instrumented.h ****  *
 413:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 414:./include/linux/atomic/atomic-instrumented.h ****  */
 415:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 416:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_sub_relaxed(int i, atomic_t *v)
 417:./include/linux/atomic/atomic-instrumented.h **** {
 418:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 419:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_sub_relaxed(i, v);
 420:./include/linux/atomic/atomic-instrumented.h **** }
 421:./include/linux/atomic/atomic-instrumented.h **** 
 422:./include/linux/atomic/atomic-instrumented.h **** /**
 423:./include/linux/atomic/atomic-instrumented.h ****  * atomic_inc() - atomic increment with relaxed ordering
 424:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 425:./include/linux/atomic/atomic-instrumented.h ****  *
 426:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with relaxed ordering.
 427:./include/linux/atomic/atomic-instrumented.h ****  *
 428:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_inc() there.
 429:./include/linux/atomic/atomic-instrumented.h ****  *
 430:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
 431:./include/linux/atomic/atomic-instrumented.h ****  */
 432:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
 433:./include/linux/atomic/atomic-instrumented.h **** atomic_inc(atomic_t *v)
 434:./include/linux/atomic/atomic-instrumented.h **** {
 435:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 436:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_inc(v);
 437:./include/linux/atomic/atomic-instrumented.h **** }
 438:./include/linux/atomic/atomic-instrumented.h **** 
 439:./include/linux/atomic/atomic-instrumented.h **** /**
 440:./include/linux/atomic/atomic-instrumented.h ****  * atomic_inc_return() - atomic increment with full ordering
 441:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 442:./include/linux/atomic/atomic-instrumented.h ****  *
 443:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with full ordering.
 444:./include/linux/atomic/atomic-instrumented.h ****  *
 445:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_inc_return() there.
 446:./include/linux/atomic/atomic-instrumented.h ****  *
 447:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 448:./include/linux/atomic/atomic-instrumented.h ****  */
 449:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 450:./include/linux/atomic/atomic-instrumented.h **** atomic_inc_return(atomic_t *v)
 451:./include/linux/atomic/atomic-instrumented.h **** {
 452:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 453:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 454:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_inc_return(v);
 455:./include/linux/atomic/atomic-instrumented.h **** }
 456:./include/linux/atomic/atomic-instrumented.h **** 
 457:./include/linux/atomic/atomic-instrumented.h **** /**
ARM GAS  /tmp/cc0b6Xiw.s 			page 53


 458:./include/linux/atomic/atomic-instrumented.h ****  * atomic_inc_return_acquire() - atomic increment with acquire ordering
 459:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 460:./include/linux/atomic/atomic-instrumented.h ****  *
 461:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with acquire ordering.
 462:./include/linux/atomic/atomic-instrumented.h ****  *
 463:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_inc_return_acquire() there.
 464:./include/linux/atomic/atomic-instrumented.h ****  *
 465:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 466:./include/linux/atomic/atomic-instrumented.h ****  */
 467:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 468:./include/linux/atomic/atomic-instrumented.h **** atomic_inc_return_acquire(atomic_t *v)
 469:./include/linux/atomic/atomic-instrumented.h **** {
 470:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 471:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_inc_return_acquire(v);
 472:./include/linux/atomic/atomic-instrumented.h **** }
 473:./include/linux/atomic/atomic-instrumented.h **** 
 474:./include/linux/atomic/atomic-instrumented.h **** /**
 475:./include/linux/atomic/atomic-instrumented.h ****  * atomic_inc_return_release() - atomic increment with release ordering
 476:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 477:./include/linux/atomic/atomic-instrumented.h ****  *
 478:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with release ordering.
 479:./include/linux/atomic/atomic-instrumented.h ****  *
 480:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_inc_return_release() there.
 481:./include/linux/atomic/atomic-instrumented.h ****  *
 482:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 483:./include/linux/atomic/atomic-instrumented.h ****  */
 484:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 485:./include/linux/atomic/atomic-instrumented.h **** atomic_inc_return_release(atomic_t *v)
 486:./include/linux/atomic/atomic-instrumented.h **** {
 487:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 488:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 489:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_inc_return_release(v);
 490:./include/linux/atomic/atomic-instrumented.h **** }
 491:./include/linux/atomic/atomic-instrumented.h **** 
 492:./include/linux/atomic/atomic-instrumented.h **** /**
 493:./include/linux/atomic/atomic-instrumented.h ****  * atomic_inc_return_relaxed() - atomic increment with relaxed ordering
 494:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 495:./include/linux/atomic/atomic-instrumented.h ****  *
 496:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with relaxed ordering.
 497:./include/linux/atomic/atomic-instrumented.h ****  *
 498:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_inc_return_relaxed() there.
 499:./include/linux/atomic/atomic-instrumented.h ****  *
 500:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 501:./include/linux/atomic/atomic-instrumented.h ****  */
 502:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 503:./include/linux/atomic/atomic-instrumented.h **** atomic_inc_return_relaxed(atomic_t *v)
 504:./include/linux/atomic/atomic-instrumented.h **** {
 505:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 506:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_inc_return_relaxed(v);
 507:./include/linux/atomic/atomic-instrumented.h **** }
 508:./include/linux/atomic/atomic-instrumented.h **** 
 509:./include/linux/atomic/atomic-instrumented.h **** /**
 510:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_inc() - atomic increment with full ordering
 511:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 512:./include/linux/atomic/atomic-instrumented.h ****  *
 513:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with full ordering.
 514:./include/linux/atomic/atomic-instrumented.h ****  *
ARM GAS  /tmp/cc0b6Xiw.s 			page 54


 515:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_inc() there.
 516:./include/linux/atomic/atomic-instrumented.h ****  *
 517:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 518:./include/linux/atomic/atomic-instrumented.h ****  */
 519:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 520:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_inc(atomic_t *v)
 521:./include/linux/atomic/atomic-instrumented.h **** {
 522:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 523:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 524:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_inc(v);
 525:./include/linux/atomic/atomic-instrumented.h **** }
 526:./include/linux/atomic/atomic-instrumented.h **** 
 527:./include/linux/atomic/atomic-instrumented.h **** /**
 528:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_inc_acquire() - atomic increment with acquire ordering
 529:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 530:./include/linux/atomic/atomic-instrumented.h ****  *
 531:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with acquire ordering.
 532:./include/linux/atomic/atomic-instrumented.h ****  *
 533:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_inc_acquire() there.
 534:./include/linux/atomic/atomic-instrumented.h ****  *
 535:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 536:./include/linux/atomic/atomic-instrumented.h ****  */
 537:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 538:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_inc_acquire(atomic_t *v)
 539:./include/linux/atomic/atomic-instrumented.h **** {
 540:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 541:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_inc_acquire(v);
 542:./include/linux/atomic/atomic-instrumented.h **** }
 543:./include/linux/atomic/atomic-instrumented.h **** 
 544:./include/linux/atomic/atomic-instrumented.h **** /**
 545:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_inc_release() - atomic increment with release ordering
 546:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 547:./include/linux/atomic/atomic-instrumented.h ****  *
 548:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with release ordering.
 549:./include/linux/atomic/atomic-instrumented.h ****  *
 550:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_inc_release() there.
 551:./include/linux/atomic/atomic-instrumented.h ****  *
 552:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 553:./include/linux/atomic/atomic-instrumented.h ****  */
 554:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 555:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_inc_release(atomic_t *v)
 556:./include/linux/atomic/atomic-instrumented.h **** {
 557:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 558:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 559:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_inc_release(v);
 560:./include/linux/atomic/atomic-instrumented.h **** }
 561:./include/linux/atomic/atomic-instrumented.h **** 
 562:./include/linux/atomic/atomic-instrumented.h **** /**
 563:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_inc_relaxed() - atomic increment with relaxed ordering
 564:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 565:./include/linux/atomic/atomic-instrumented.h ****  *
 566:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v + 1) with relaxed ordering.
 567:./include/linux/atomic/atomic-instrumented.h ****  *
 568:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_inc_relaxed() there.
 569:./include/linux/atomic/atomic-instrumented.h ****  *
 570:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 571:./include/linux/atomic/atomic-instrumented.h ****  */
ARM GAS  /tmp/cc0b6Xiw.s 			page 55


 572:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 573:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_inc_relaxed(atomic_t *v)
 574:./include/linux/atomic/atomic-instrumented.h **** {
 575:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 576:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_inc_relaxed(v);
 577:./include/linux/atomic/atomic-instrumented.h **** }
 578:./include/linux/atomic/atomic-instrumented.h **** 
 579:./include/linux/atomic/atomic-instrumented.h **** /**
 580:./include/linux/atomic/atomic-instrumented.h ****  * atomic_dec() - atomic decrement with relaxed ordering
 581:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 582:./include/linux/atomic/atomic-instrumented.h ****  *
 583:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with relaxed ordering.
 584:./include/linux/atomic/atomic-instrumented.h ****  *
 585:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_dec() there.
 586:./include/linux/atomic/atomic-instrumented.h ****  *
 587:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
 588:./include/linux/atomic/atomic-instrumented.h ****  */
 589:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
 590:./include/linux/atomic/atomic-instrumented.h **** atomic_dec(atomic_t *v)
 591:./include/linux/atomic/atomic-instrumented.h **** {
 592:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 593:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_dec(v);
 594:./include/linux/atomic/atomic-instrumented.h **** }
 595:./include/linux/atomic/atomic-instrumented.h **** 
 596:./include/linux/atomic/atomic-instrumented.h **** /**
 597:./include/linux/atomic/atomic-instrumented.h ****  * atomic_dec_return() - atomic decrement with full ordering
 598:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 599:./include/linux/atomic/atomic-instrumented.h ****  *
 600:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with full ordering.
 601:./include/linux/atomic/atomic-instrumented.h ****  *
 602:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_dec_return() there.
 603:./include/linux/atomic/atomic-instrumented.h ****  *
 604:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 605:./include/linux/atomic/atomic-instrumented.h ****  */
 606:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 607:./include/linux/atomic/atomic-instrumented.h **** atomic_dec_return(atomic_t *v)
 608:./include/linux/atomic/atomic-instrumented.h **** {
 609:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 610:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 611:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_dec_return(v);
 612:./include/linux/atomic/atomic-instrumented.h **** }
 613:./include/linux/atomic/atomic-instrumented.h **** 
 614:./include/linux/atomic/atomic-instrumented.h **** /**
 615:./include/linux/atomic/atomic-instrumented.h ****  * atomic_dec_return_acquire() - atomic decrement with acquire ordering
 616:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 617:./include/linux/atomic/atomic-instrumented.h ****  *
 618:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with acquire ordering.
 619:./include/linux/atomic/atomic-instrumented.h ****  *
 620:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_dec_return_acquire() there.
 621:./include/linux/atomic/atomic-instrumented.h ****  *
 622:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 623:./include/linux/atomic/atomic-instrumented.h ****  */
 624:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 625:./include/linux/atomic/atomic-instrumented.h **** atomic_dec_return_acquire(atomic_t *v)
 626:./include/linux/atomic/atomic-instrumented.h **** {
 627:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 628:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_dec_return_acquire(v);
ARM GAS  /tmp/cc0b6Xiw.s 			page 56


 629:./include/linux/atomic/atomic-instrumented.h **** }
 630:./include/linux/atomic/atomic-instrumented.h **** 
 631:./include/linux/atomic/atomic-instrumented.h **** /**
 632:./include/linux/atomic/atomic-instrumented.h ****  * atomic_dec_return_release() - atomic decrement with release ordering
 633:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 634:./include/linux/atomic/atomic-instrumented.h ****  *
 635:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with release ordering.
 636:./include/linux/atomic/atomic-instrumented.h ****  *
 637:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_dec_return_release() there.
 638:./include/linux/atomic/atomic-instrumented.h ****  *
 639:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 640:./include/linux/atomic/atomic-instrumented.h ****  */
 641:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 642:./include/linux/atomic/atomic-instrumented.h **** atomic_dec_return_release(atomic_t *v)
 643:./include/linux/atomic/atomic-instrumented.h **** {
 644:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 645:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 646:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_dec_return_release(v);
 647:./include/linux/atomic/atomic-instrumented.h **** }
 648:./include/linux/atomic/atomic-instrumented.h **** 
 649:./include/linux/atomic/atomic-instrumented.h **** /**
 650:./include/linux/atomic/atomic-instrumented.h ****  * atomic_dec_return_relaxed() - atomic decrement with relaxed ordering
 651:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 652:./include/linux/atomic/atomic-instrumented.h ****  *
 653:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with relaxed ordering.
 654:./include/linux/atomic/atomic-instrumented.h ****  *
 655:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_dec_return_relaxed() there.
 656:./include/linux/atomic/atomic-instrumented.h ****  *
 657:./include/linux/atomic/atomic-instrumented.h ****  * Return: The updated value of @v.
 658:./include/linux/atomic/atomic-instrumented.h ****  */
 659:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 660:./include/linux/atomic/atomic-instrumented.h **** atomic_dec_return_relaxed(atomic_t *v)
 661:./include/linux/atomic/atomic-instrumented.h **** {
 662:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 663:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_dec_return_relaxed(v);
 664:./include/linux/atomic/atomic-instrumented.h **** }
 665:./include/linux/atomic/atomic-instrumented.h **** 
 666:./include/linux/atomic/atomic-instrumented.h **** /**
 667:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_dec() - atomic decrement with full ordering
 668:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 669:./include/linux/atomic/atomic-instrumented.h ****  *
 670:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with full ordering.
 671:./include/linux/atomic/atomic-instrumented.h ****  *
 672:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_dec() there.
 673:./include/linux/atomic/atomic-instrumented.h ****  *
 674:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 675:./include/linux/atomic/atomic-instrumented.h ****  */
 676:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 677:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_dec(atomic_t *v)
 678:./include/linux/atomic/atomic-instrumented.h **** {
 679:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 680:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 681:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_dec(v);
 682:./include/linux/atomic/atomic-instrumented.h **** }
 683:./include/linux/atomic/atomic-instrumented.h **** 
 684:./include/linux/atomic/atomic-instrumented.h **** /**
 685:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_dec_acquire() - atomic decrement with acquire ordering
ARM GAS  /tmp/cc0b6Xiw.s 			page 57


 686:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 687:./include/linux/atomic/atomic-instrumented.h ****  *
 688:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with acquire ordering.
 689:./include/linux/atomic/atomic-instrumented.h ****  *
 690:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_dec_acquire() there.
 691:./include/linux/atomic/atomic-instrumented.h ****  *
 692:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 693:./include/linux/atomic/atomic-instrumented.h ****  */
 694:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 695:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_dec_acquire(atomic_t *v)
 696:./include/linux/atomic/atomic-instrumented.h **** {
 697:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 698:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_dec_acquire(v);
 699:./include/linux/atomic/atomic-instrumented.h **** }
 700:./include/linux/atomic/atomic-instrumented.h **** 
 701:./include/linux/atomic/atomic-instrumented.h **** /**
 702:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_dec_release() - atomic decrement with release ordering
 703:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 704:./include/linux/atomic/atomic-instrumented.h ****  *
 705:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with release ordering.
 706:./include/linux/atomic/atomic-instrumented.h ****  *
 707:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_dec_release() there.
 708:./include/linux/atomic/atomic-instrumented.h ****  *
 709:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 710:./include/linux/atomic/atomic-instrumented.h ****  */
 711:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 712:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_dec_release(atomic_t *v)
 713:./include/linux/atomic/atomic-instrumented.h **** {
 714:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 715:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 716:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_dec_release(v);
 717:./include/linux/atomic/atomic-instrumented.h **** }
 718:./include/linux/atomic/atomic-instrumented.h **** 
 719:./include/linux/atomic/atomic-instrumented.h **** /**
 720:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_dec_relaxed() - atomic decrement with relaxed ordering
 721:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 722:./include/linux/atomic/atomic-instrumented.h ****  *
 723:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with relaxed ordering.
 724:./include/linux/atomic/atomic-instrumented.h ****  *
 725:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_dec_relaxed() there.
 726:./include/linux/atomic/atomic-instrumented.h ****  *
 727:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 728:./include/linux/atomic/atomic-instrumented.h ****  */
 729:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 730:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_dec_relaxed(atomic_t *v)
 731:./include/linux/atomic/atomic-instrumented.h **** {
 732:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 733:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_dec_relaxed(v);
 734:./include/linux/atomic/atomic-instrumented.h **** }
 735:./include/linux/atomic/atomic-instrumented.h **** 
 736:./include/linux/atomic/atomic-instrumented.h **** /**
 737:./include/linux/atomic/atomic-instrumented.h ****  * atomic_and() - atomic bitwise AND with relaxed ordering
 738:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 739:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 740:./include/linux/atomic/atomic-instrumented.h ****  *
 741:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & @i) with relaxed ordering.
 742:./include/linux/atomic/atomic-instrumented.h ****  *
ARM GAS  /tmp/cc0b6Xiw.s 			page 58


 743:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_and() there.
 744:./include/linux/atomic/atomic-instrumented.h ****  *
 745:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
 746:./include/linux/atomic/atomic-instrumented.h ****  */
 747:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
 748:./include/linux/atomic/atomic-instrumented.h **** atomic_and(int i, atomic_t *v)
 749:./include/linux/atomic/atomic-instrumented.h **** {
 750:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 751:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_and(i, v);
 752:./include/linux/atomic/atomic-instrumented.h **** }
 753:./include/linux/atomic/atomic-instrumented.h **** 
 754:./include/linux/atomic/atomic-instrumented.h **** /**
 755:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_and() - atomic bitwise AND with full ordering
 756:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 757:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 758:./include/linux/atomic/atomic-instrumented.h ****  *
 759:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & @i) with full ordering.
 760:./include/linux/atomic/atomic-instrumented.h ****  *
 761:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_and() there.
 762:./include/linux/atomic/atomic-instrumented.h ****  *
 763:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 764:./include/linux/atomic/atomic-instrumented.h ****  */
 765:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 766:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_and(int i, atomic_t *v)
 767:./include/linux/atomic/atomic-instrumented.h **** {
 768:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 769:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 770:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_and(i, v);
 771:./include/linux/atomic/atomic-instrumented.h **** }
 772:./include/linux/atomic/atomic-instrumented.h **** 
 773:./include/linux/atomic/atomic-instrumented.h **** /**
 774:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_and_acquire() - atomic bitwise AND with acquire ordering
 775:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 776:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 777:./include/linux/atomic/atomic-instrumented.h ****  *
 778:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & @i) with acquire ordering.
 779:./include/linux/atomic/atomic-instrumented.h ****  *
 780:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_and_acquire() there.
 781:./include/linux/atomic/atomic-instrumented.h ****  *
 782:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 783:./include/linux/atomic/atomic-instrumented.h ****  */
 784:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 785:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_and_acquire(int i, atomic_t *v)
 786:./include/linux/atomic/atomic-instrumented.h **** {
 787:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 788:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_and_acquire(i, v);
 789:./include/linux/atomic/atomic-instrumented.h **** }
 790:./include/linux/atomic/atomic-instrumented.h **** 
 791:./include/linux/atomic/atomic-instrumented.h **** /**
 792:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_and_release() - atomic bitwise AND with release ordering
 793:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 794:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 795:./include/linux/atomic/atomic-instrumented.h ****  *
 796:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & @i) with release ordering.
 797:./include/linux/atomic/atomic-instrumented.h ****  *
 798:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_and_release() there.
 799:./include/linux/atomic/atomic-instrumented.h ****  *
ARM GAS  /tmp/cc0b6Xiw.s 			page 59


 800:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 801:./include/linux/atomic/atomic-instrumented.h ****  */
 802:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 803:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_and_release(int i, atomic_t *v)
 804:./include/linux/atomic/atomic-instrumented.h **** {
 805:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 806:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 807:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_and_release(i, v);
 808:./include/linux/atomic/atomic-instrumented.h **** }
 809:./include/linux/atomic/atomic-instrumented.h **** 
 810:./include/linux/atomic/atomic-instrumented.h **** /**
 811:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_and_relaxed() - atomic bitwise AND with relaxed ordering
 812:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 813:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 814:./include/linux/atomic/atomic-instrumented.h ****  *
 815:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & @i) with relaxed ordering.
 816:./include/linux/atomic/atomic-instrumented.h ****  *
 817:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_and_relaxed() there.
 818:./include/linux/atomic/atomic-instrumented.h ****  *
 819:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 820:./include/linux/atomic/atomic-instrumented.h ****  */
 821:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 822:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_and_relaxed(int i, atomic_t *v)
 823:./include/linux/atomic/atomic-instrumented.h **** {
 824:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 825:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_and_relaxed(i, v);
 826:./include/linux/atomic/atomic-instrumented.h **** }
 827:./include/linux/atomic/atomic-instrumented.h **** 
 828:./include/linux/atomic/atomic-instrumented.h **** /**
 829:./include/linux/atomic/atomic-instrumented.h ****  * atomic_andnot() - atomic bitwise AND NOT with relaxed ordering
 830:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 831:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 832:./include/linux/atomic/atomic-instrumented.h ****  *
 833:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & ~@i) with relaxed ordering.
 834:./include/linux/atomic/atomic-instrumented.h ****  *
 835:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_andnot() there.
 836:./include/linux/atomic/atomic-instrumented.h ****  *
 837:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
 838:./include/linux/atomic/atomic-instrumented.h ****  */
 839:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
 840:./include/linux/atomic/atomic-instrumented.h **** atomic_andnot(int i, atomic_t *v)
 841:./include/linux/atomic/atomic-instrumented.h **** {
 842:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 843:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_andnot(i, v);
 844:./include/linux/atomic/atomic-instrumented.h **** }
 845:./include/linux/atomic/atomic-instrumented.h **** 
 846:./include/linux/atomic/atomic-instrumented.h **** /**
 847:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_andnot() - atomic bitwise AND NOT with full ordering
 848:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 849:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 850:./include/linux/atomic/atomic-instrumented.h ****  *
 851:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & ~@i) with full ordering.
 852:./include/linux/atomic/atomic-instrumented.h ****  *
 853:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_andnot() there.
 854:./include/linux/atomic/atomic-instrumented.h ****  *
 855:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 856:./include/linux/atomic/atomic-instrumented.h ****  */
ARM GAS  /tmp/cc0b6Xiw.s 			page 60


 857:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 858:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_andnot(int i, atomic_t *v)
 859:./include/linux/atomic/atomic-instrumented.h **** {
 860:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 861:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 862:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_andnot(i, v);
 863:./include/linux/atomic/atomic-instrumented.h **** }
 864:./include/linux/atomic/atomic-instrumented.h **** 
 865:./include/linux/atomic/atomic-instrumented.h **** /**
 866:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_andnot_acquire() - atomic bitwise AND NOT with acquire ordering
 867:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 868:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 869:./include/linux/atomic/atomic-instrumented.h ****  *
 870:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & ~@i) with acquire ordering.
 871:./include/linux/atomic/atomic-instrumented.h ****  *
 872:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_andnot_acquire() there.
 873:./include/linux/atomic/atomic-instrumented.h ****  *
 874:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 875:./include/linux/atomic/atomic-instrumented.h ****  */
 876:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 877:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_andnot_acquire(int i, atomic_t *v)
 878:./include/linux/atomic/atomic-instrumented.h **** {
 879:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 880:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_andnot_acquire(i, v);
 881:./include/linux/atomic/atomic-instrumented.h **** }
 882:./include/linux/atomic/atomic-instrumented.h **** 
 883:./include/linux/atomic/atomic-instrumented.h **** /**
 884:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_andnot_release() - atomic bitwise AND NOT with release ordering
 885:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 886:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 887:./include/linux/atomic/atomic-instrumented.h ****  *
 888:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & ~@i) with release ordering.
 889:./include/linux/atomic/atomic-instrumented.h ****  *
 890:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_andnot_release() there.
 891:./include/linux/atomic/atomic-instrumented.h ****  *
 892:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 893:./include/linux/atomic/atomic-instrumented.h ****  */
 894:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 895:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_andnot_release(int i, atomic_t *v)
 896:./include/linux/atomic/atomic-instrumented.h **** {
 897:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 898:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 899:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_andnot_release(i, v);
 900:./include/linux/atomic/atomic-instrumented.h **** }
 901:./include/linux/atomic/atomic-instrumented.h **** 
 902:./include/linux/atomic/atomic-instrumented.h **** /**
 903:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_andnot_relaxed() - atomic bitwise AND NOT with relaxed ordering
 904:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 905:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 906:./include/linux/atomic/atomic-instrumented.h ****  *
 907:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v & ~@i) with relaxed ordering.
 908:./include/linux/atomic/atomic-instrumented.h ****  *
 909:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_andnot_relaxed() there.
 910:./include/linux/atomic/atomic-instrumented.h ****  *
 911:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 912:./include/linux/atomic/atomic-instrumented.h ****  */
 913:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
ARM GAS  /tmp/cc0b6Xiw.s 			page 61


 914:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_andnot_relaxed(int i, atomic_t *v)
 915:./include/linux/atomic/atomic-instrumented.h **** {
 916:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 917:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_andnot_relaxed(i, v);
 918:./include/linux/atomic/atomic-instrumented.h **** }
 919:./include/linux/atomic/atomic-instrumented.h **** 
 920:./include/linux/atomic/atomic-instrumented.h **** /**
 921:./include/linux/atomic/atomic-instrumented.h ****  * atomic_or() - atomic bitwise OR with relaxed ordering
 922:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 923:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 924:./include/linux/atomic/atomic-instrumented.h ****  *
 925:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v | @i) with relaxed ordering.
 926:./include/linux/atomic/atomic-instrumented.h ****  *
 927:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_or() there.
 928:./include/linux/atomic/atomic-instrumented.h ****  *
 929:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
 930:./include/linux/atomic/atomic-instrumented.h ****  */
 931:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
 932:./include/linux/atomic/atomic-instrumented.h **** atomic_or(int i, atomic_t *v)
 933:./include/linux/atomic/atomic-instrumented.h **** {
 934:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 935:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_or(i, v);
 936:./include/linux/atomic/atomic-instrumented.h **** }
 937:./include/linux/atomic/atomic-instrumented.h **** 
 938:./include/linux/atomic/atomic-instrumented.h **** /**
 939:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_or() - atomic bitwise OR with full ordering
 940:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 941:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 942:./include/linux/atomic/atomic-instrumented.h ****  *
 943:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v | @i) with full ordering.
 944:./include/linux/atomic/atomic-instrumented.h ****  *
 945:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_or() there.
 946:./include/linux/atomic/atomic-instrumented.h ****  *
 947:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 948:./include/linux/atomic/atomic-instrumented.h ****  */
 949:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 950:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_or(int i, atomic_t *v)
 951:./include/linux/atomic/atomic-instrumented.h **** {
 952:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 953:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 954:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_or(i, v);
 955:./include/linux/atomic/atomic-instrumented.h **** }
 956:./include/linux/atomic/atomic-instrumented.h **** 
 957:./include/linux/atomic/atomic-instrumented.h **** /**
 958:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_or_acquire() - atomic bitwise OR with acquire ordering
 959:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 960:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 961:./include/linux/atomic/atomic-instrumented.h ****  *
 962:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v | @i) with acquire ordering.
 963:./include/linux/atomic/atomic-instrumented.h ****  *
 964:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_or_acquire() there.
 965:./include/linux/atomic/atomic-instrumented.h ****  *
 966:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 967:./include/linux/atomic/atomic-instrumented.h ****  */
 968:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 969:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_or_acquire(int i, atomic_t *v)
 970:./include/linux/atomic/atomic-instrumented.h **** {
ARM GAS  /tmp/cc0b6Xiw.s 			page 62


 971:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 972:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_or_acquire(i, v);
 973:./include/linux/atomic/atomic-instrumented.h **** }
 974:./include/linux/atomic/atomic-instrumented.h **** 
 975:./include/linux/atomic/atomic-instrumented.h **** /**
 976:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_or_release() - atomic bitwise OR with release ordering
 977:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 978:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 979:./include/linux/atomic/atomic-instrumented.h ****  *
 980:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v | @i) with release ordering.
 981:./include/linux/atomic/atomic-instrumented.h ****  *
 982:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_or_release() there.
 983:./include/linux/atomic/atomic-instrumented.h ****  *
 984:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
 985:./include/linux/atomic/atomic-instrumented.h ****  */
 986:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
 987:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_or_release(int i, atomic_t *v)
 988:./include/linux/atomic/atomic-instrumented.h **** {
 989:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
 990:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 991:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_or_release(i, v);
 992:./include/linux/atomic/atomic-instrumented.h **** }
 993:./include/linux/atomic/atomic-instrumented.h **** 
 994:./include/linux/atomic/atomic-instrumented.h **** /**
 995:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_or_relaxed() - atomic bitwise OR with relaxed ordering
 996:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
 997:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
 998:./include/linux/atomic/atomic-instrumented.h ****  *
 999:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v | @i) with relaxed ordering.
1000:./include/linux/atomic/atomic-instrumented.h ****  *
1001:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_or_relaxed() there.
1002:./include/linux/atomic/atomic-instrumented.h ****  *
1003:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1004:./include/linux/atomic/atomic-instrumented.h ****  */
1005:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1006:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_or_relaxed(int i, atomic_t *v)
1007:./include/linux/atomic/atomic-instrumented.h **** {
1008:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1009:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_or_relaxed(i, v);
1010:./include/linux/atomic/atomic-instrumented.h **** }
1011:./include/linux/atomic/atomic-instrumented.h **** 
1012:./include/linux/atomic/atomic-instrumented.h **** /**
1013:./include/linux/atomic/atomic-instrumented.h ****  * atomic_xor() - atomic bitwise XOR with relaxed ordering
1014:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
1015:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1016:./include/linux/atomic/atomic-instrumented.h ****  *
1017:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v ^ @i) with relaxed ordering.
1018:./include/linux/atomic/atomic-instrumented.h ****  *
1019:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_xor() there.
1020:./include/linux/atomic/atomic-instrumented.h ****  *
1021:./include/linux/atomic/atomic-instrumented.h ****  * Return: Nothing.
1022:./include/linux/atomic/atomic-instrumented.h ****  */
1023:./include/linux/atomic/atomic-instrumented.h **** static __always_inline void
1024:./include/linux/atomic/atomic-instrumented.h **** atomic_xor(int i, atomic_t *v)
1025:./include/linux/atomic/atomic-instrumented.h **** {
1026:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1027:./include/linux/atomic/atomic-instrumented.h **** 	raw_atomic_xor(i, v);
ARM GAS  /tmp/cc0b6Xiw.s 			page 63


1028:./include/linux/atomic/atomic-instrumented.h **** }
1029:./include/linux/atomic/atomic-instrumented.h **** 
1030:./include/linux/atomic/atomic-instrumented.h **** /**
1031:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_xor() - atomic bitwise XOR with full ordering
1032:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
1033:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1034:./include/linux/atomic/atomic-instrumented.h ****  *
1035:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v ^ @i) with full ordering.
1036:./include/linux/atomic/atomic-instrumented.h ****  *
1037:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_xor() there.
1038:./include/linux/atomic/atomic-instrumented.h ****  *
1039:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1040:./include/linux/atomic/atomic-instrumented.h ****  */
1041:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1042:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_xor(int i, atomic_t *v)
1043:./include/linux/atomic/atomic-instrumented.h **** {
1044:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
1045:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1046:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_xor(i, v);
1047:./include/linux/atomic/atomic-instrumented.h **** }
1048:./include/linux/atomic/atomic-instrumented.h **** 
1049:./include/linux/atomic/atomic-instrumented.h **** /**
1050:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_xor_acquire() - atomic bitwise XOR with acquire ordering
1051:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
1052:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1053:./include/linux/atomic/atomic-instrumented.h ****  *
1054:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v ^ @i) with acquire ordering.
1055:./include/linux/atomic/atomic-instrumented.h ****  *
1056:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_xor_acquire() there.
1057:./include/linux/atomic/atomic-instrumented.h ****  *
1058:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1059:./include/linux/atomic/atomic-instrumented.h ****  */
1060:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1061:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_xor_acquire(int i, atomic_t *v)
1062:./include/linux/atomic/atomic-instrumented.h **** {
1063:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1064:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_xor_acquire(i, v);
1065:./include/linux/atomic/atomic-instrumented.h **** }
1066:./include/linux/atomic/atomic-instrumented.h **** 
1067:./include/linux/atomic/atomic-instrumented.h **** /**
1068:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_xor_release() - atomic bitwise XOR with release ordering
1069:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
1070:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1071:./include/linux/atomic/atomic-instrumented.h ****  *
1072:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v ^ @i) with release ordering.
1073:./include/linux/atomic/atomic-instrumented.h ****  *
1074:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_xor_release() there.
1075:./include/linux/atomic/atomic-instrumented.h ****  *
1076:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1077:./include/linux/atomic/atomic-instrumented.h ****  */
1078:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1079:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_xor_release(int i, atomic_t *v)
1080:./include/linux/atomic/atomic-instrumented.h **** {
1081:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
1082:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1083:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_xor_release(i, v);
1084:./include/linux/atomic/atomic-instrumented.h **** }
ARM GAS  /tmp/cc0b6Xiw.s 			page 64


1085:./include/linux/atomic/atomic-instrumented.h **** 
1086:./include/linux/atomic/atomic-instrumented.h **** /**
1087:./include/linux/atomic/atomic-instrumented.h ****  * atomic_fetch_xor_relaxed() - atomic bitwise XOR with relaxed ordering
1088:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value
1089:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1090:./include/linux/atomic/atomic-instrumented.h ****  *
1091:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v ^ @i) with relaxed ordering.
1092:./include/linux/atomic/atomic-instrumented.h ****  *
1093:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_fetch_xor_relaxed() there.
1094:./include/linux/atomic/atomic-instrumented.h ****  *
1095:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1096:./include/linux/atomic/atomic-instrumented.h ****  */
1097:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1098:./include/linux/atomic/atomic-instrumented.h **** atomic_fetch_xor_relaxed(int i, atomic_t *v)
1099:./include/linux/atomic/atomic-instrumented.h **** {
1100:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1101:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_fetch_xor_relaxed(i, v);
1102:./include/linux/atomic/atomic-instrumented.h **** }
1103:./include/linux/atomic/atomic-instrumented.h **** 
1104:./include/linux/atomic/atomic-instrumented.h **** /**
1105:./include/linux/atomic/atomic-instrumented.h ****  * atomic_xchg() - atomic exchange with full ordering
1106:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1107:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1108:./include/linux/atomic/atomic-instrumented.h ****  *
1109:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to @new with full ordering.
1110:./include/linux/atomic/atomic-instrumented.h ****  *
1111:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_xchg() there.
1112:./include/linux/atomic/atomic-instrumented.h ****  *
1113:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1114:./include/linux/atomic/atomic-instrumented.h ****  */
1115:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1116:./include/linux/atomic/atomic-instrumented.h **** atomic_xchg(atomic_t *v, int new)
1117:./include/linux/atomic/atomic-instrumented.h **** {
1118:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
1119:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1120:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_xchg(v, new);
1121:./include/linux/atomic/atomic-instrumented.h **** }
1122:./include/linux/atomic/atomic-instrumented.h **** 
1123:./include/linux/atomic/atomic-instrumented.h **** /**
1124:./include/linux/atomic/atomic-instrumented.h ****  * atomic_xchg_acquire() - atomic exchange with acquire ordering
1125:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1126:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1127:./include/linux/atomic/atomic-instrumented.h ****  *
1128:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to @new with acquire ordering.
1129:./include/linux/atomic/atomic-instrumented.h ****  *
1130:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_xchg_acquire() there.
1131:./include/linux/atomic/atomic-instrumented.h ****  *
1132:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1133:./include/linux/atomic/atomic-instrumented.h ****  */
1134:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1135:./include/linux/atomic/atomic-instrumented.h **** atomic_xchg_acquire(atomic_t *v, int new)
1136:./include/linux/atomic/atomic-instrumented.h **** {
1137:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1138:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_xchg_acquire(v, new);
1139:./include/linux/atomic/atomic-instrumented.h **** }
1140:./include/linux/atomic/atomic-instrumented.h **** 
1141:./include/linux/atomic/atomic-instrumented.h **** /**
ARM GAS  /tmp/cc0b6Xiw.s 			page 65


1142:./include/linux/atomic/atomic-instrumented.h ****  * atomic_xchg_release() - atomic exchange with release ordering
1143:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1144:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1145:./include/linux/atomic/atomic-instrumented.h ****  *
1146:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to @new with release ordering.
1147:./include/linux/atomic/atomic-instrumented.h ****  *
1148:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_xchg_release() there.
1149:./include/linux/atomic/atomic-instrumented.h ****  *
1150:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1151:./include/linux/atomic/atomic-instrumented.h ****  */
1152:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1153:./include/linux/atomic/atomic-instrumented.h **** atomic_xchg_release(atomic_t *v, int new)
1154:./include/linux/atomic/atomic-instrumented.h **** {
1155:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
1156:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1157:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_xchg_release(v, new);
1158:./include/linux/atomic/atomic-instrumented.h **** }
1159:./include/linux/atomic/atomic-instrumented.h **** 
1160:./include/linux/atomic/atomic-instrumented.h **** /**
1161:./include/linux/atomic/atomic-instrumented.h ****  * atomic_xchg_relaxed() - atomic exchange with relaxed ordering
1162:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1163:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1164:./include/linux/atomic/atomic-instrumented.h ****  *
1165:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to @new with relaxed ordering.
1166:./include/linux/atomic/atomic-instrumented.h ****  *
1167:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_xchg_relaxed() there.
1168:./include/linux/atomic/atomic-instrumented.h ****  *
1169:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1170:./include/linux/atomic/atomic-instrumented.h ****  */
1171:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1172:./include/linux/atomic/atomic-instrumented.h **** atomic_xchg_relaxed(atomic_t *v, int new)
1173:./include/linux/atomic/atomic-instrumented.h **** {
1174:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1175:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_xchg_relaxed(v, new);
1176:./include/linux/atomic/atomic-instrumented.h **** }
1177:./include/linux/atomic/atomic-instrumented.h **** 
1178:./include/linux/atomic/atomic-instrumented.h **** /**
1179:./include/linux/atomic/atomic-instrumented.h ****  * atomic_cmpxchg() - atomic compare and exchange with full ordering
1180:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1181:./include/linux/atomic/atomic-instrumented.h ****  * @old: int value to compare with
1182:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1183:./include/linux/atomic/atomic-instrumented.h ****  *
1184:./include/linux/atomic/atomic-instrumented.h ****  * If (@v == @old), atomically updates @v to @new with full ordering.
1185:./include/linux/atomic/atomic-instrumented.h ****  * Otherwise, @v is not modified and relaxed ordering is provided.
1186:./include/linux/atomic/atomic-instrumented.h ****  *
1187:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_cmpxchg() there.
1188:./include/linux/atomic/atomic-instrumented.h ****  *
1189:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1190:./include/linux/atomic/atomic-instrumented.h ****  */
1191:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1192:./include/linux/atomic/atomic-instrumented.h **** atomic_cmpxchg(atomic_t *v, int old, int new)
1193:./include/linux/atomic/atomic-instrumented.h **** {
1194:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
1195:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1196:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_cmpxchg(v, old, new);
1197:./include/linux/atomic/atomic-instrumented.h **** }
1198:./include/linux/atomic/atomic-instrumented.h **** 
ARM GAS  /tmp/cc0b6Xiw.s 			page 66


1199:./include/linux/atomic/atomic-instrumented.h **** /**
1200:./include/linux/atomic/atomic-instrumented.h ****  * atomic_cmpxchg_acquire() - atomic compare and exchange with acquire ordering
1201:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1202:./include/linux/atomic/atomic-instrumented.h ****  * @old: int value to compare with
1203:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1204:./include/linux/atomic/atomic-instrumented.h ****  *
1205:./include/linux/atomic/atomic-instrumented.h ****  * If (@v == @old), atomically updates @v to @new with acquire ordering.
1206:./include/linux/atomic/atomic-instrumented.h ****  * Otherwise, @v is not modified and relaxed ordering is provided.
1207:./include/linux/atomic/atomic-instrumented.h ****  *
1208:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_cmpxchg_acquire() there.
1209:./include/linux/atomic/atomic-instrumented.h ****  *
1210:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1211:./include/linux/atomic/atomic-instrumented.h ****  */
1212:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1213:./include/linux/atomic/atomic-instrumented.h **** atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
1214:./include/linux/atomic/atomic-instrumented.h **** {
1215:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1216:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_cmpxchg_acquire(v, old, new);
1217:./include/linux/atomic/atomic-instrumented.h **** }
1218:./include/linux/atomic/atomic-instrumented.h **** 
1219:./include/linux/atomic/atomic-instrumented.h **** /**
1220:./include/linux/atomic/atomic-instrumented.h ****  * atomic_cmpxchg_release() - atomic compare and exchange with release ordering
1221:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1222:./include/linux/atomic/atomic-instrumented.h ****  * @old: int value to compare with
1223:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1224:./include/linux/atomic/atomic-instrumented.h ****  *
1225:./include/linux/atomic/atomic-instrumented.h ****  * If (@v == @old), atomically updates @v to @new with release ordering.
1226:./include/linux/atomic/atomic-instrumented.h ****  * Otherwise, @v is not modified and relaxed ordering is provided.
1227:./include/linux/atomic/atomic-instrumented.h ****  *
1228:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_cmpxchg_release() there.
1229:./include/linux/atomic/atomic-instrumented.h ****  *
1230:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1231:./include/linux/atomic/atomic-instrumented.h ****  */
1232:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1233:./include/linux/atomic/atomic-instrumented.h **** atomic_cmpxchg_release(atomic_t *v, int old, int new)
1234:./include/linux/atomic/atomic-instrumented.h **** {
1235:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
1236:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1237:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_cmpxchg_release(v, old, new);
1238:./include/linux/atomic/atomic-instrumented.h **** }
1239:./include/linux/atomic/atomic-instrumented.h **** 
1240:./include/linux/atomic/atomic-instrumented.h **** /**
1241:./include/linux/atomic/atomic-instrumented.h ****  * atomic_cmpxchg_relaxed() - atomic compare and exchange with relaxed ordering
1242:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1243:./include/linux/atomic/atomic-instrumented.h ****  * @old: int value to compare with
1244:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1245:./include/linux/atomic/atomic-instrumented.h ****  *
1246:./include/linux/atomic/atomic-instrumented.h ****  * If (@v == @old), atomically updates @v to @new with relaxed ordering.
1247:./include/linux/atomic/atomic-instrumented.h ****  * Otherwise, @v is not modified and relaxed ordering is provided.
1248:./include/linux/atomic/atomic-instrumented.h ****  *
1249:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_cmpxchg_relaxed() there.
1250:./include/linux/atomic/atomic-instrumented.h ****  *
1251:./include/linux/atomic/atomic-instrumented.h ****  * Return: The original value of @v.
1252:./include/linux/atomic/atomic-instrumented.h ****  */
1253:./include/linux/atomic/atomic-instrumented.h **** static __always_inline int
1254:./include/linux/atomic/atomic-instrumented.h **** atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)
1255:./include/linux/atomic/atomic-instrumented.h **** {
ARM GAS  /tmp/cc0b6Xiw.s 			page 67


1256:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1257:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_cmpxchg_relaxed(v, old, new);
1258:./include/linux/atomic/atomic-instrumented.h **** }
1259:./include/linux/atomic/atomic-instrumented.h **** 
1260:./include/linux/atomic/atomic-instrumented.h **** /**
1261:./include/linux/atomic/atomic-instrumented.h ****  * atomic_try_cmpxchg() - atomic compare and exchange with full ordering
1262:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1263:./include/linux/atomic/atomic-instrumented.h ****  * @old: pointer to int value to compare with
1264:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1265:./include/linux/atomic/atomic-instrumented.h ****  *
1266:./include/linux/atomic/atomic-instrumented.h ****  * If (@v == @old), atomically updates @v to @new with full ordering.
1267:./include/linux/atomic/atomic-instrumented.h ****  * Otherwise, @v is not modified, @old is updated to the current value of @v,
1268:./include/linux/atomic/atomic-instrumented.h ****  * and relaxed ordering is provided.
1269:./include/linux/atomic/atomic-instrumented.h ****  *
1270:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_try_cmpxchg() there.
1271:./include/linux/atomic/atomic-instrumented.h ****  *
1272:./include/linux/atomic/atomic-instrumented.h ****  * Return: @true if the exchange occured, @false otherwise.
1273:./include/linux/atomic/atomic-instrumented.h ****  */
1274:./include/linux/atomic/atomic-instrumented.h **** static __always_inline bool
1275:./include/linux/atomic/atomic-instrumented.h **** atomic_try_cmpxchg(atomic_t *v, int *old, int new)
1276:./include/linux/atomic/atomic-instrumented.h **** {
1277:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
1278:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1279:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(old, sizeof(*old));
1280:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_try_cmpxchg(v, old, new);
1281:./include/linux/atomic/atomic-instrumented.h **** }
1282:./include/linux/atomic/atomic-instrumented.h **** 
1283:./include/linux/atomic/atomic-instrumented.h **** /**
1284:./include/linux/atomic/atomic-instrumented.h ****  * atomic_try_cmpxchg_acquire() - atomic compare and exchange with acquire ordering
1285:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1286:./include/linux/atomic/atomic-instrumented.h ****  * @old: pointer to int value to compare with
1287:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1288:./include/linux/atomic/atomic-instrumented.h ****  *
1289:./include/linux/atomic/atomic-instrumented.h ****  * If (@v == @old), atomically updates @v to @new with acquire ordering.
1290:./include/linux/atomic/atomic-instrumented.h ****  * Otherwise, @v is not modified, @old is updated to the current value of @v,
1291:./include/linux/atomic/atomic-instrumented.h ****  * and relaxed ordering is provided.
1292:./include/linux/atomic/atomic-instrumented.h ****  *
1293:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_try_cmpxchg_acquire() there.
1294:./include/linux/atomic/atomic-instrumented.h ****  *
1295:./include/linux/atomic/atomic-instrumented.h ****  * Return: @true if the exchange occured, @false otherwise.
1296:./include/linux/atomic/atomic-instrumented.h ****  */
1297:./include/linux/atomic/atomic-instrumented.h **** static __always_inline bool
1298:./include/linux/atomic/atomic-instrumented.h **** atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
1299:./include/linux/atomic/atomic-instrumented.h **** {
1300:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1301:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(old, sizeof(*old));
1302:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_try_cmpxchg_acquire(v, old, new);
1303:./include/linux/atomic/atomic-instrumented.h **** }
1304:./include/linux/atomic/atomic-instrumented.h **** 
1305:./include/linux/atomic/atomic-instrumented.h **** /**
1306:./include/linux/atomic/atomic-instrumented.h ****  * atomic_try_cmpxchg_release() - atomic compare and exchange with release ordering
1307:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1308:./include/linux/atomic/atomic-instrumented.h ****  * @old: pointer to int value to compare with
1309:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1310:./include/linux/atomic/atomic-instrumented.h ****  *
1311:./include/linux/atomic/atomic-instrumented.h ****  * If (@v == @old), atomically updates @v to @new with release ordering.
1312:./include/linux/atomic/atomic-instrumented.h ****  * Otherwise, @v is not modified, @old is updated to the current value of @v,
ARM GAS  /tmp/cc0b6Xiw.s 			page 68


1313:./include/linux/atomic/atomic-instrumented.h ****  * and relaxed ordering is provided.
1314:./include/linux/atomic/atomic-instrumented.h ****  *
1315:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_try_cmpxchg_release() there.
1316:./include/linux/atomic/atomic-instrumented.h ****  *
1317:./include/linux/atomic/atomic-instrumented.h ****  * Return: @true if the exchange occured, @false otherwise.
1318:./include/linux/atomic/atomic-instrumented.h ****  */
1319:./include/linux/atomic/atomic-instrumented.h **** static __always_inline bool
1320:./include/linux/atomic/atomic-instrumented.h **** atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)
1321:./include/linux/atomic/atomic-instrumented.h **** {
1322:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_release();
1323:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1324:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(old, sizeof(*old));
1325:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_try_cmpxchg_release(v, old, new);
1326:./include/linux/atomic/atomic-instrumented.h **** }
1327:./include/linux/atomic/atomic-instrumented.h **** 
1328:./include/linux/atomic/atomic-instrumented.h **** /**
1329:./include/linux/atomic/atomic-instrumented.h ****  * atomic_try_cmpxchg_relaxed() - atomic compare and exchange with relaxed ordering
1330:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1331:./include/linux/atomic/atomic-instrumented.h ****  * @old: pointer to int value to compare with
1332:./include/linux/atomic/atomic-instrumented.h ****  * @new: int value to assign
1333:./include/linux/atomic/atomic-instrumented.h ****  *
1334:./include/linux/atomic/atomic-instrumented.h ****  * If (@v == @old), atomically updates @v to @new with relaxed ordering.
1335:./include/linux/atomic/atomic-instrumented.h ****  * Otherwise, @v is not modified, @old is updated to the current value of @v,
1336:./include/linux/atomic/atomic-instrumented.h ****  * and relaxed ordering is provided.
1337:./include/linux/atomic/atomic-instrumented.h ****  *
1338:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_try_cmpxchg_relaxed() there.
1339:./include/linux/atomic/atomic-instrumented.h ****  *
1340:./include/linux/atomic/atomic-instrumented.h ****  * Return: @true if the exchange occured, @false otherwise.
1341:./include/linux/atomic/atomic-instrumented.h ****  */
1342:./include/linux/atomic/atomic-instrumented.h **** static __always_inline bool
1343:./include/linux/atomic/atomic-instrumented.h **** atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)
1344:./include/linux/atomic/atomic-instrumented.h **** {
1345:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1346:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(old, sizeof(*old));
1347:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_try_cmpxchg_relaxed(v, old, new);
1348:./include/linux/atomic/atomic-instrumented.h **** }
1349:./include/linux/atomic/atomic-instrumented.h **** 
1350:./include/linux/atomic/atomic-instrumented.h **** /**
1351:./include/linux/atomic/atomic-instrumented.h ****  * atomic_sub_and_test() - atomic subtract and test if zero with full ordering
1352:./include/linux/atomic/atomic-instrumented.h ****  * @i: int value to subtract
1353:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1354:./include/linux/atomic/atomic-instrumented.h ****  *
1355:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - @i) with full ordering.
1356:./include/linux/atomic/atomic-instrumented.h ****  *
1357:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_sub_and_test() there.
1358:./include/linux/atomic/atomic-instrumented.h ****  *
1359:./include/linux/atomic/atomic-instrumented.h ****  * Return: @true if the resulting value of @v is zero, @false otherwise.
1360:./include/linux/atomic/atomic-instrumented.h ****  */
1361:./include/linux/atomic/atomic-instrumented.h **** static __always_inline bool
1362:./include/linux/atomic/atomic-instrumented.h **** atomic_sub_and_test(int i, atomic_t *v)
1363:./include/linux/atomic/atomic-instrumented.h **** {
1364:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
1365:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
1366:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_sub_and_test(i, v);
1367:./include/linux/atomic/atomic-instrumented.h **** }
1368:./include/linux/atomic/atomic-instrumented.h **** 
1369:./include/linux/atomic/atomic-instrumented.h **** /**
ARM GAS  /tmp/cc0b6Xiw.s 			page 69


1370:./include/linux/atomic/atomic-instrumented.h ****  * atomic_dec_and_test() - atomic decrement and test if zero with full ordering
1371:./include/linux/atomic/atomic-instrumented.h ****  * @v: pointer to atomic_t
1372:./include/linux/atomic/atomic-instrumented.h ****  *
1373:./include/linux/atomic/atomic-instrumented.h ****  * Atomically updates @v to (@v - 1) with full ordering.
1374:./include/linux/atomic/atomic-instrumented.h ****  *
1375:./include/linux/atomic/atomic-instrumented.h ****  * Unsafe to use in noinstr code; use raw_atomic_dec_and_test() there.
1376:./include/linux/atomic/atomic-instrumented.h ****  *
1377:./include/linux/atomic/atomic-instrumented.h ****  * Return: @true if the resulting value of @v is zero, @false otherwise.
1378:./include/linux/atomic/atomic-instrumented.h ****  */
1379:./include/linux/atomic/atomic-instrumented.h **** static __always_inline bool
1380:./include/linux/atomic/atomic-instrumented.h **** atomic_dec_and_test(atomic_t *v)
 732              		.loc 7 1380 1 is_stmt 1 view .LVU222
1381:./include/linux/atomic/atomic-instrumented.h **** {
1382:./include/linux/atomic/atomic-instrumented.h **** 	kcsan_mb();
 733              		.loc 7 1382 2 view .LVU223
 734              		.loc 7 1382 2 view .LVU224
 735              		.loc 7 1382 2 view .LVU225
1383:./include/linux/atomic/atomic-instrumented.h **** 	instrument_atomic_read_write(v, sizeof(*v));
 736              		.loc 7 1383 2 view .LVU226
1384:./include/linux/atomic/atomic-instrumented.h **** 	return raw_atomic_dec_and_test(v);
 737              		.loc 7 1384 2 view .LVU227
 738              	.LBB103:
 739              	.LBI103:
 510:./include/linux/atomic/atomic-arch-fallback.h **** }
 511:./include/linux/atomic/atomic-arch-fallback.h **** 
 512:./include/linux/atomic/atomic-arch-fallback.h **** /**
 513:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_set_release() - atomic set with release ordering
 514:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 515:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to assign
 516:./include/linux/atomic/atomic-arch-fallback.h ****  *
 517:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically sets @v to @i with release ordering.
 518:./include/linux/atomic/atomic-arch-fallback.h ****  *
 519:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_set_release() elsewhere.
 520:./include/linux/atomic/atomic-arch-fallback.h ****  *
 521:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
 522:./include/linux/atomic/atomic-arch-fallback.h ****  */
 523:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
 524:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_set_release(atomic_t *v, int i)
 525:./include/linux/atomic/atomic-arch-fallback.h **** {
 526:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_set_release)
 527:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_set_release(v, i);
 528:./include/linux/atomic/atomic-arch-fallback.h **** #else
 529:./include/linux/atomic/atomic-arch-fallback.h **** 	if (__native_word(atomic_t)) {
 530:./include/linux/atomic/atomic-arch-fallback.h **** 		smp_store_release(&(v)->counter, i);
 531:./include/linux/atomic/atomic-arch-fallback.h **** 	} else {
 532:./include/linux/atomic/atomic-arch-fallback.h **** 		__atomic_release_fence();
 533:./include/linux/atomic/atomic-arch-fallback.h **** 		raw_atomic_set(v, i);
 534:./include/linux/atomic/atomic-arch-fallback.h **** 	}
 535:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 536:./include/linux/atomic/atomic-arch-fallback.h **** }
 537:./include/linux/atomic/atomic-arch-fallback.h **** 
 538:./include/linux/atomic/atomic-arch-fallback.h **** /**
 539:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_add() - atomic add with relaxed ordering
 540:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
 541:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 542:./include/linux/atomic/atomic-arch-fallback.h ****  *
 543:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with relaxed ordering.
ARM GAS  /tmp/cc0b6Xiw.s 			page 70


 544:./include/linux/atomic/atomic-arch-fallback.h ****  *
 545:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_add() elsewhere.
 546:./include/linux/atomic/atomic-arch-fallback.h ****  *
 547:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
 548:./include/linux/atomic/atomic-arch-fallback.h ****  */
 549:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
 550:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_add(int i, atomic_t *v)
 551:./include/linux/atomic/atomic-arch-fallback.h **** {
 552:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_add(i, v);
 553:./include/linux/atomic/atomic-arch-fallback.h **** }
 554:./include/linux/atomic/atomic-arch-fallback.h **** 
 555:./include/linux/atomic/atomic-arch-fallback.h **** /**
 556:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_add_return() - atomic add with full ordering
 557:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
 558:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 559:./include/linux/atomic/atomic-arch-fallback.h ****  *
 560:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with full ordering.
 561:./include/linux/atomic/atomic-arch-fallback.h ****  *
 562:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_add_return() elsewhere.
 563:./include/linux/atomic/atomic-arch-fallback.h ****  *
 564:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
 565:./include/linux/atomic/atomic-arch-fallback.h ****  */
 566:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 567:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_add_return(int i, atomic_t *v)
 568:./include/linux/atomic/atomic-arch-fallback.h **** {
 569:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_add_return)
 570:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_add_return(i, v);
 571:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_add_return_relaxed)
 572:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
 573:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
 574:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_add_return_relaxed(i, v);
 575:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
 576:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 577:./include/linux/atomic/atomic-arch-fallback.h **** #else
 578:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_add_return"
 579:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 580:./include/linux/atomic/atomic-arch-fallback.h **** }
 581:./include/linux/atomic/atomic-arch-fallback.h **** 
 582:./include/linux/atomic/atomic-arch-fallback.h **** /**
 583:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_add_return_acquire() - atomic add with acquire ordering
 584:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
 585:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 586:./include/linux/atomic/atomic-arch-fallback.h ****  *
 587:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with acquire ordering.
 588:./include/linux/atomic/atomic-arch-fallback.h ****  *
 589:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_add_return_acquire() elsewhere.
 590:./include/linux/atomic/atomic-arch-fallback.h ****  *
 591:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
 592:./include/linux/atomic/atomic-arch-fallback.h ****  */
 593:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 594:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_add_return_acquire(int i, atomic_t *v)
 595:./include/linux/atomic/atomic-arch-fallback.h **** {
 596:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_add_return_acquire)
 597:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_add_return_acquire(i, v);
 598:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_add_return_relaxed)
 599:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_add_return_relaxed(i, v);
 600:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
ARM GAS  /tmp/cc0b6Xiw.s 			page 71


 601:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 602:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_add_return)
 603:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_add_return(i, v);
 604:./include/linux/atomic/atomic-arch-fallback.h **** #else
 605:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_add_return_acquire"
 606:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 607:./include/linux/atomic/atomic-arch-fallback.h **** }
 608:./include/linux/atomic/atomic-arch-fallback.h **** 
 609:./include/linux/atomic/atomic-arch-fallback.h **** /**
 610:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_add_return_release() - atomic add with release ordering
 611:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
 612:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 613:./include/linux/atomic/atomic-arch-fallback.h ****  *
 614:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with release ordering.
 615:./include/linux/atomic/atomic-arch-fallback.h ****  *
 616:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_add_return_release() elsewhere.
 617:./include/linux/atomic/atomic-arch-fallback.h ****  *
 618:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
 619:./include/linux/atomic/atomic-arch-fallback.h ****  */
 620:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 621:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_add_return_release(int i, atomic_t *v)
 622:./include/linux/atomic/atomic-arch-fallback.h **** {
 623:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_add_return_release)
 624:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_add_return_release(i, v);
 625:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_add_return_relaxed)
 626:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
 627:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_add_return_relaxed(i, v);
 628:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_add_return)
 629:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_add_return(i, v);
 630:./include/linux/atomic/atomic-arch-fallback.h **** #else
 631:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_add_return_release"
 632:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 633:./include/linux/atomic/atomic-arch-fallback.h **** }
 634:./include/linux/atomic/atomic-arch-fallback.h **** 
 635:./include/linux/atomic/atomic-arch-fallback.h **** /**
 636:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_add_return_relaxed() - atomic add with relaxed ordering
 637:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
 638:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 639:./include/linux/atomic/atomic-arch-fallback.h ****  *
 640:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with relaxed ordering.
 641:./include/linux/atomic/atomic-arch-fallback.h ****  *
 642:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_add_return_relaxed() elsewhere.
 643:./include/linux/atomic/atomic-arch-fallback.h ****  *
 644:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
 645:./include/linux/atomic/atomic-arch-fallback.h ****  */
 646:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 647:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_add_return_relaxed(int i, atomic_t *v)
 648:./include/linux/atomic/atomic-arch-fallback.h **** {
 649:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_add_return_relaxed)
 650:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_add_return_relaxed(i, v);
 651:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_add_return)
 652:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_add_return(i, v);
 653:./include/linux/atomic/atomic-arch-fallback.h **** #else
 654:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_add_return_relaxed"
 655:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 656:./include/linux/atomic/atomic-arch-fallback.h **** }
 657:./include/linux/atomic/atomic-arch-fallback.h **** 
ARM GAS  /tmp/cc0b6Xiw.s 			page 72


 658:./include/linux/atomic/atomic-arch-fallback.h **** /**
 659:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_add() - atomic add with full ordering
 660:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
 661:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 662:./include/linux/atomic/atomic-arch-fallback.h ****  *
 663:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with full ordering.
 664:./include/linux/atomic/atomic-arch-fallback.h ****  *
 665:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_add() elsewhere.
 666:./include/linux/atomic/atomic-arch-fallback.h ****  *
 667:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
 668:./include/linux/atomic/atomic-arch-fallback.h ****  */
 669:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 670:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_add(int i, atomic_t *v)
 671:./include/linux/atomic/atomic-arch-fallback.h **** {
 672:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_add)
 673:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_add(i, v);
 674:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_add_relaxed)
 675:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
 676:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
 677:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_fetch_add_relaxed(i, v);
 678:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
 679:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 680:./include/linux/atomic/atomic-arch-fallback.h **** #else
 681:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_add"
 682:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 683:./include/linux/atomic/atomic-arch-fallback.h **** }
 684:./include/linux/atomic/atomic-arch-fallback.h **** 
 685:./include/linux/atomic/atomic-arch-fallback.h **** /**
 686:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_add_acquire() - atomic add with acquire ordering
 687:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
 688:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 689:./include/linux/atomic/atomic-arch-fallback.h ****  *
 690:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with acquire ordering.
 691:./include/linux/atomic/atomic-arch-fallback.h ****  *
 692:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_add_acquire() elsewhere.
 693:./include/linux/atomic/atomic-arch-fallback.h ****  *
 694:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
 695:./include/linux/atomic/atomic-arch-fallback.h ****  */
 696:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 697:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_add_acquire(int i, atomic_t *v)
 698:./include/linux/atomic/atomic-arch-fallback.h **** {
 699:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_add_acquire)
 700:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_add_acquire(i, v);
 701:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_add_relaxed)
 702:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_fetch_add_relaxed(i, v);
 703:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
 704:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 705:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_add)
 706:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_add(i, v);
 707:./include/linux/atomic/atomic-arch-fallback.h **** #else
 708:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_add_acquire"
 709:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 710:./include/linux/atomic/atomic-arch-fallback.h **** }
 711:./include/linux/atomic/atomic-arch-fallback.h **** 
 712:./include/linux/atomic/atomic-arch-fallback.h **** /**
 713:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_add_release() - atomic add with release ordering
 714:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
ARM GAS  /tmp/cc0b6Xiw.s 			page 73


 715:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 716:./include/linux/atomic/atomic-arch-fallback.h ****  *
 717:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with release ordering.
 718:./include/linux/atomic/atomic-arch-fallback.h ****  *
 719:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_add_release() elsewhere.
 720:./include/linux/atomic/atomic-arch-fallback.h ****  *
 721:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
 722:./include/linux/atomic/atomic-arch-fallback.h ****  */
 723:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 724:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_add_release(int i, atomic_t *v)
 725:./include/linux/atomic/atomic-arch-fallback.h **** {
 726:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_add_release)
 727:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_add_release(i, v);
 728:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_add_relaxed)
 729:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
 730:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_add_relaxed(i, v);
 731:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_add)
 732:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_add(i, v);
 733:./include/linux/atomic/atomic-arch-fallback.h **** #else
 734:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_add_release"
 735:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 736:./include/linux/atomic/atomic-arch-fallback.h **** }
 737:./include/linux/atomic/atomic-arch-fallback.h **** 
 738:./include/linux/atomic/atomic-arch-fallback.h **** /**
 739:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_add_relaxed() - atomic add with relaxed ordering
 740:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to add
 741:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 742:./include/linux/atomic/atomic-arch-fallback.h ****  *
 743:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + @i) with relaxed ordering.
 744:./include/linux/atomic/atomic-arch-fallback.h ****  *
 745:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_add_relaxed() elsewhere.
 746:./include/linux/atomic/atomic-arch-fallback.h ****  *
 747:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
 748:./include/linux/atomic/atomic-arch-fallback.h ****  */
 749:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 750:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_add_relaxed(int i, atomic_t *v)
 751:./include/linux/atomic/atomic-arch-fallback.h **** {
 752:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_add_relaxed)
 753:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_add_relaxed(i, v);
 754:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_add)
 755:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_add(i, v);
 756:./include/linux/atomic/atomic-arch-fallback.h **** #else
 757:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_add_relaxed"
 758:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 759:./include/linux/atomic/atomic-arch-fallback.h **** }
 760:./include/linux/atomic/atomic-arch-fallback.h **** 
 761:./include/linux/atomic/atomic-arch-fallback.h **** /**
 762:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_sub() - atomic subtract with relaxed ordering
 763:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 764:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 765:./include/linux/atomic/atomic-arch-fallback.h ****  *
 766:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with relaxed ordering.
 767:./include/linux/atomic/atomic-arch-fallback.h ****  *
 768:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_sub() elsewhere.
 769:./include/linux/atomic/atomic-arch-fallback.h ****  *
 770:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
 771:./include/linux/atomic/atomic-arch-fallback.h ****  */
ARM GAS  /tmp/cc0b6Xiw.s 			page 74


 772:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
 773:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_sub(int i, atomic_t *v)
 774:./include/linux/atomic/atomic-arch-fallback.h **** {
 775:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_sub(i, v);
 776:./include/linux/atomic/atomic-arch-fallback.h **** }
 777:./include/linux/atomic/atomic-arch-fallback.h **** 
 778:./include/linux/atomic/atomic-arch-fallback.h **** /**
 779:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_sub_return() - atomic subtract with full ordering
 780:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 781:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 782:./include/linux/atomic/atomic-arch-fallback.h ****  *
 783:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with full ordering.
 784:./include/linux/atomic/atomic-arch-fallback.h ****  *
 785:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_sub_return() elsewhere.
 786:./include/linux/atomic/atomic-arch-fallback.h ****  *
 787:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
 788:./include/linux/atomic/atomic-arch-fallback.h ****  */
 789:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 790:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_sub_return(int i, atomic_t *v)
 791:./include/linux/atomic/atomic-arch-fallback.h **** {
 792:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_sub_return)
 793:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_return(i, v);
 794:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_sub_return_relaxed)
 795:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
 796:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
 797:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_sub_return_relaxed(i, v);
 798:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
 799:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 800:./include/linux/atomic/atomic-arch-fallback.h **** #else
 801:./include/linux/atomic/atomic-arch-fallback.h **** 	volatile int *p = (volatile int *)&v->counter;
 802:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = *p;
 803:./include/linux/atomic/atomic-arch-fallback.h **** 	*p -= i;
 804:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 805:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 806:./include/linux/atomic/atomic-arch-fallback.h **** }
 807:./include/linux/atomic/atomic-arch-fallback.h **** 
 808:./include/linux/atomic/atomic-arch-fallback.h **** 
 809:./include/linux/atomic/atomic-arch-fallback.h **** /**
 810:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_sub_return_acquire() - atomic subtract with acquire ordering
 811:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 812:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 813:./include/linux/atomic/atomic-arch-fallback.h ****  *
 814:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with acquire ordering.
 815:./include/linux/atomic/atomic-arch-fallback.h ****  *
 816:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_sub_return_acquire() elsewhere.
 817:./include/linux/atomic/atomic-arch-fallback.h ****  *
 818:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
 819:./include/linux/atomic/atomic-arch-fallback.h ****  */
 820:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 821:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_sub_return_acquire(int i, atomic_t *v)
 822:./include/linux/atomic/atomic-arch-fallback.h **** {
 823:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_sub_return_acquire)
 824:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_return_acquire(i, v);
 825:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_sub_return_relaxed)
 826:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_sub_return_relaxed(i, v);
 827:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
 828:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
ARM GAS  /tmp/cc0b6Xiw.s 			page 75


 829:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_sub_return)
 830:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_return(i, v);
 831:./include/linux/atomic/atomic-arch-fallback.h **** #else
 832:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_sub_return_acquire"
 833:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 834:./include/linux/atomic/atomic-arch-fallback.h **** }
 835:./include/linux/atomic/atomic-arch-fallback.h **** 
 836:./include/linux/atomic/atomic-arch-fallback.h **** /**
 837:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_sub_return_release() - atomic subtract with release ordering
 838:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 839:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 840:./include/linux/atomic/atomic-arch-fallback.h ****  *
 841:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with release ordering.
 842:./include/linux/atomic/atomic-arch-fallback.h ****  *
 843:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_sub_return_release() elsewhere.
 844:./include/linux/atomic/atomic-arch-fallback.h ****  *
 845:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
 846:./include/linux/atomic/atomic-arch-fallback.h ****  */
 847:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 848:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_sub_return_release(int i, atomic_t *v)
 849:./include/linux/atomic/atomic-arch-fallback.h **** {
 850:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_sub_return_release)
 851:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_return_release(i, v);
 852:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_sub_return_relaxed)
 853:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
 854:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_return_relaxed(i, v);
 855:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_sub_return)
 856:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_return(i, v);
 857:./include/linux/atomic/atomic-arch-fallback.h **** #else
 858:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_sub_return_release"
 859:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 860:./include/linux/atomic/atomic-arch-fallback.h **** }
 861:./include/linux/atomic/atomic-arch-fallback.h **** 
 862:./include/linux/atomic/atomic-arch-fallback.h **** /**
 863:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_sub_return_relaxed() - atomic subtract with relaxed ordering
 864:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 865:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 866:./include/linux/atomic/atomic-arch-fallback.h ****  *
 867:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with relaxed ordering.
 868:./include/linux/atomic/atomic-arch-fallback.h ****  *
 869:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_sub_return_relaxed() elsewhere.
 870:./include/linux/atomic/atomic-arch-fallback.h ****  *
 871:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
 872:./include/linux/atomic/atomic-arch-fallback.h ****  */
 873:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 874:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_sub_return_relaxed(int i, atomic_t *v)
 875:./include/linux/atomic/atomic-arch-fallback.h **** {
 876:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_sub_return_relaxed)
 877:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_return_relaxed(i, v);
 878:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_sub_return)
 879:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_return(i, v);
 880:./include/linux/atomic/atomic-arch-fallback.h **** #else
 881:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_sub_return_relaxed"
 882:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 883:./include/linux/atomic/atomic-arch-fallback.h **** }
 884:./include/linux/atomic/atomic-arch-fallback.h **** 
 885:./include/linux/atomic/atomic-arch-fallback.h **** /**
ARM GAS  /tmp/cc0b6Xiw.s 			page 76


 886:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_sub() - atomic subtract with full ordering
 887:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 888:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 889:./include/linux/atomic/atomic-arch-fallback.h ****  *
 890:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with full ordering.
 891:./include/linux/atomic/atomic-arch-fallback.h ****  *
 892:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_sub() elsewhere.
 893:./include/linux/atomic/atomic-arch-fallback.h ****  *
 894:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
 895:./include/linux/atomic/atomic-arch-fallback.h ****  */
 896:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 897:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_sub(int i, atomic_t *v)
 898:./include/linux/atomic/atomic-arch-fallback.h **** {
 899:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_sub)
 900:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_sub(i, v);
 901:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_sub_relaxed)
 902:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
 903:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
 904:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_fetch_sub_relaxed(i, v);
 905:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
 906:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 907:./include/linux/atomic/atomic-arch-fallback.h **** #else
 908:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_sub"
 909:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 910:./include/linux/atomic/atomic-arch-fallback.h **** }
 911:./include/linux/atomic/atomic-arch-fallback.h **** 
 912:./include/linux/atomic/atomic-arch-fallback.h **** /**
 913:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_sub_acquire() - atomic subtract with acquire ordering
 914:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 915:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 916:./include/linux/atomic/atomic-arch-fallback.h ****  *
 917:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with acquire ordering.
 918:./include/linux/atomic/atomic-arch-fallback.h ****  *
 919:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_sub_acquire() elsewhere.
 920:./include/linux/atomic/atomic-arch-fallback.h ****  *
 921:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
 922:./include/linux/atomic/atomic-arch-fallback.h ****  */
 923:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 924:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_sub_acquire(int i, atomic_t *v)
 925:./include/linux/atomic/atomic-arch-fallback.h **** {
 926:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_sub_acquire)
 927:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_sub_acquire(i, v);
 928:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_sub_relaxed)
 929:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_fetch_sub_relaxed(i, v);
 930:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
 931:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 932:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_sub)
 933:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_sub(i, v);
 934:./include/linux/atomic/atomic-arch-fallback.h **** #else
 935:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_sub_acquire"
 936:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 937:./include/linux/atomic/atomic-arch-fallback.h **** }
 938:./include/linux/atomic/atomic-arch-fallback.h **** 
 939:./include/linux/atomic/atomic-arch-fallback.h **** /**
 940:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_sub_release() - atomic subtract with release ordering
 941:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 942:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
ARM GAS  /tmp/cc0b6Xiw.s 			page 77


 943:./include/linux/atomic/atomic-arch-fallback.h ****  *
 944:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with release ordering.
 945:./include/linux/atomic/atomic-arch-fallback.h ****  *
 946:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_sub_release() elsewhere.
 947:./include/linux/atomic/atomic-arch-fallback.h ****  *
 948:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
 949:./include/linux/atomic/atomic-arch-fallback.h ****  */
 950:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 951:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_sub_release(int i, atomic_t *v)
 952:./include/linux/atomic/atomic-arch-fallback.h **** {
 953:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_sub_release)
 954:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_sub_release(i, v);
 955:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_sub_relaxed)
 956:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
 957:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_sub_relaxed(i, v);
 958:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_sub)
 959:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_sub(i, v);
 960:./include/linux/atomic/atomic-arch-fallback.h **** #else
 961:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_sub_release"
 962:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 963:./include/linux/atomic/atomic-arch-fallback.h **** }
 964:./include/linux/atomic/atomic-arch-fallback.h **** 
 965:./include/linux/atomic/atomic-arch-fallback.h **** /**
 966:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_sub_relaxed() - atomic subtract with relaxed ordering
 967:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
 968:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 969:./include/linux/atomic/atomic-arch-fallback.h ****  *
 970:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with relaxed ordering.
 971:./include/linux/atomic/atomic-arch-fallback.h ****  *
 972:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_sub_relaxed() elsewhere.
 973:./include/linux/atomic/atomic-arch-fallback.h ****  *
 974:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
 975:./include/linux/atomic/atomic-arch-fallback.h ****  */
 976:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
 977:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_sub_relaxed(int i, atomic_t *v)
 978:./include/linux/atomic/atomic-arch-fallback.h **** {
 979:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_sub_relaxed)
 980:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_sub_relaxed(i, v);
 981:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_sub)
 982:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_sub(i, v);
 983:./include/linux/atomic/atomic-arch-fallback.h **** #else
 984:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_sub_relaxed"
 985:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 986:./include/linux/atomic/atomic-arch-fallback.h **** }
 987:./include/linux/atomic/atomic-arch-fallback.h **** 
 988:./include/linux/atomic/atomic-arch-fallback.h **** /**
 989:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_inc() - atomic increment with relaxed ordering
 990:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
 991:./include/linux/atomic/atomic-arch-fallback.h ****  *
 992:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with relaxed ordering.
 993:./include/linux/atomic/atomic-arch-fallback.h ****  *
 994:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_inc() elsewhere.
 995:./include/linux/atomic/atomic-arch-fallback.h ****  *
 996:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
 997:./include/linux/atomic/atomic-arch-fallback.h ****  */
 998:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
 999:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_inc(atomic_t *v)
ARM GAS  /tmp/cc0b6Xiw.s 			page 78


1000:./include/linux/atomic/atomic-arch-fallback.h **** {
1001:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_inc)
1002:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_inc(v);
1003:./include/linux/atomic/atomic-arch-fallback.h **** #else
1004:./include/linux/atomic/atomic-arch-fallback.h **** 	raw_atomic_add(1, v);
1005:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1006:./include/linux/atomic/atomic-arch-fallback.h **** }
1007:./include/linux/atomic/atomic-arch-fallback.h **** 
1008:./include/linux/atomic/atomic-arch-fallback.h **** /**
1009:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_inc_return() - atomic increment with full ordering
1010:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1011:./include/linux/atomic/atomic-arch-fallback.h ****  *
1012:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with full ordering.
1013:./include/linux/atomic/atomic-arch-fallback.h ****  *
1014:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_inc_return() elsewhere.
1015:./include/linux/atomic/atomic-arch-fallback.h ****  *
1016:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
1017:./include/linux/atomic/atomic-arch-fallback.h ****  */
1018:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1019:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_inc_return(atomic_t *v)
1020:./include/linux/atomic/atomic-arch-fallback.h **** {
1021:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_inc_return)
1022:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_inc_return(v);
1023:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_inc_return_relaxed)
1024:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
1025:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1026:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_inc_return_relaxed(v);
1027:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1028:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1029:./include/linux/atomic/atomic-arch-fallback.h **** #else
1030:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_add_return(1, v);
1031:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1032:./include/linux/atomic/atomic-arch-fallback.h **** }
1033:./include/linux/atomic/atomic-arch-fallback.h **** 
1034:./include/linux/atomic/atomic-arch-fallback.h **** /**
1035:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_inc_return_acquire() - atomic increment with acquire ordering
1036:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1037:./include/linux/atomic/atomic-arch-fallback.h ****  *
1038:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with acquire ordering.
1039:./include/linux/atomic/atomic-arch-fallback.h ****  *
1040:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_inc_return_acquire() elsewhere.
1041:./include/linux/atomic/atomic-arch-fallback.h ****  *
1042:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
1043:./include/linux/atomic/atomic-arch-fallback.h ****  */
1044:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1045:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_inc_return_acquire(atomic_t *v)
1046:./include/linux/atomic/atomic-arch-fallback.h **** {
1047:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_inc_return_acquire)
1048:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_inc_return_acquire(v);
1049:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_inc_return_relaxed)
1050:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_inc_return_relaxed(v);
1051:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1052:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1053:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_inc_return)
1054:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_inc_return(v);
1055:./include/linux/atomic/atomic-arch-fallback.h **** #else
1056:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_add_return_acquire(1, v);
ARM GAS  /tmp/cc0b6Xiw.s 			page 79


1057:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1058:./include/linux/atomic/atomic-arch-fallback.h **** }
1059:./include/linux/atomic/atomic-arch-fallback.h **** 
1060:./include/linux/atomic/atomic-arch-fallback.h **** /**
1061:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_inc_return_release() - atomic increment with release ordering
1062:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1063:./include/linux/atomic/atomic-arch-fallback.h ****  *
1064:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with release ordering.
1065:./include/linux/atomic/atomic-arch-fallback.h ****  *
1066:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_inc_return_release() elsewhere.
1067:./include/linux/atomic/atomic-arch-fallback.h ****  *
1068:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
1069:./include/linux/atomic/atomic-arch-fallback.h ****  */
1070:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1071:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_inc_return_release(atomic_t *v)
1072:./include/linux/atomic/atomic-arch-fallback.h **** {
1073:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_inc_return_release)
1074:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_inc_return_release(v);
1075:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_inc_return_relaxed)
1076:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
1077:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_inc_return_relaxed(v);
1078:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_inc_return)
1079:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_inc_return(v);
1080:./include/linux/atomic/atomic-arch-fallback.h **** #else
1081:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_add_return_release(1, v);
1082:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1083:./include/linux/atomic/atomic-arch-fallback.h **** }
1084:./include/linux/atomic/atomic-arch-fallback.h **** 
1085:./include/linux/atomic/atomic-arch-fallback.h **** /**
1086:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_inc_return_relaxed() - atomic increment with relaxed ordering
1087:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1088:./include/linux/atomic/atomic-arch-fallback.h ****  *
1089:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with relaxed ordering.
1090:./include/linux/atomic/atomic-arch-fallback.h ****  *
1091:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_inc_return_relaxed() elsewhere.
1092:./include/linux/atomic/atomic-arch-fallback.h ****  *
1093:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
1094:./include/linux/atomic/atomic-arch-fallback.h ****  */
1095:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1096:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_inc_return_relaxed(atomic_t *v)
1097:./include/linux/atomic/atomic-arch-fallback.h **** {
1098:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_inc_return_relaxed)
1099:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_inc_return_relaxed(v);
1100:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_inc_return)
1101:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_inc_return(v);
1102:./include/linux/atomic/atomic-arch-fallback.h **** #else
1103:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_add_return_relaxed(1, v);
1104:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1105:./include/linux/atomic/atomic-arch-fallback.h **** }
1106:./include/linux/atomic/atomic-arch-fallback.h **** 
1107:./include/linux/atomic/atomic-arch-fallback.h **** /**
1108:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_inc() - atomic increment with full ordering
1109:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1110:./include/linux/atomic/atomic-arch-fallback.h ****  *
1111:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with full ordering.
1112:./include/linux/atomic/atomic-arch-fallback.h ****  *
1113:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_inc() elsewhere.
ARM GAS  /tmp/cc0b6Xiw.s 			page 80


1114:./include/linux/atomic/atomic-arch-fallback.h ****  *
1115:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1116:./include/linux/atomic/atomic-arch-fallback.h ****  */
1117:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1118:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_inc(atomic_t *v)
1119:./include/linux/atomic/atomic-arch-fallback.h **** {
1120:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_inc)
1121:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_inc(v);
1122:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_inc_relaxed)
1123:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
1124:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1125:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_fetch_inc_relaxed(v);
1126:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1127:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1128:./include/linux/atomic/atomic-arch-fallback.h **** #else
1129:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_add(1, v);
1130:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1131:./include/linux/atomic/atomic-arch-fallback.h **** }
1132:./include/linux/atomic/atomic-arch-fallback.h **** 
1133:./include/linux/atomic/atomic-arch-fallback.h **** /**
1134:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_inc_acquire() - atomic increment with acquire ordering
1135:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1136:./include/linux/atomic/atomic-arch-fallback.h ****  *
1137:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with acquire ordering.
1138:./include/linux/atomic/atomic-arch-fallback.h ****  *
1139:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_inc_acquire() elsewhere.
1140:./include/linux/atomic/atomic-arch-fallback.h ****  *
1141:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1142:./include/linux/atomic/atomic-arch-fallback.h ****  */
1143:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1144:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_inc_acquire(atomic_t *v)
1145:./include/linux/atomic/atomic-arch-fallback.h **** {
1146:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_inc_acquire)
1147:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_inc_acquire(v);
1148:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_inc_relaxed)
1149:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_fetch_inc_relaxed(v);
1150:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1151:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1152:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_inc)
1153:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_inc(v);
1154:./include/linux/atomic/atomic-arch-fallback.h **** #else
1155:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_add_acquire(1, v);
1156:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1157:./include/linux/atomic/atomic-arch-fallback.h **** }
1158:./include/linux/atomic/atomic-arch-fallback.h **** 
1159:./include/linux/atomic/atomic-arch-fallback.h **** /**
1160:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_inc_release() - atomic increment with release ordering
1161:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1162:./include/linux/atomic/atomic-arch-fallback.h ****  *
1163:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with release ordering.
1164:./include/linux/atomic/atomic-arch-fallback.h ****  *
1165:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_inc_release() elsewhere.
1166:./include/linux/atomic/atomic-arch-fallback.h ****  *
1167:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1168:./include/linux/atomic/atomic-arch-fallback.h ****  */
1169:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1170:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_inc_release(atomic_t *v)
ARM GAS  /tmp/cc0b6Xiw.s 			page 81


1171:./include/linux/atomic/atomic-arch-fallback.h **** {
1172:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_inc_release)
1173:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_inc_release(v);
1174:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_inc_relaxed)
1175:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
1176:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_inc_relaxed(v);
1177:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_inc)
1178:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_inc(v);
1179:./include/linux/atomic/atomic-arch-fallback.h **** #else
1180:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_add_release(1, v);
1181:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1182:./include/linux/atomic/atomic-arch-fallback.h **** }
1183:./include/linux/atomic/atomic-arch-fallback.h **** 
1184:./include/linux/atomic/atomic-arch-fallback.h **** /**
1185:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_inc_relaxed() - atomic increment with relaxed ordering
1186:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1187:./include/linux/atomic/atomic-arch-fallback.h ****  *
1188:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v + 1) with relaxed ordering.
1189:./include/linux/atomic/atomic-arch-fallback.h ****  *
1190:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_inc_relaxed() elsewhere.
1191:./include/linux/atomic/atomic-arch-fallback.h ****  *
1192:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1193:./include/linux/atomic/atomic-arch-fallback.h ****  */
1194:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1195:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_inc_relaxed(atomic_t *v)
1196:./include/linux/atomic/atomic-arch-fallback.h **** {
1197:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_inc_relaxed)
1198:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_inc_relaxed(v);
1199:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_inc)
1200:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_inc(v);
1201:./include/linux/atomic/atomic-arch-fallback.h **** #else
1202:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_add_relaxed(1, v);
1203:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1204:./include/linux/atomic/atomic-arch-fallback.h **** }
1205:./include/linux/atomic/atomic-arch-fallback.h **** 
1206:./include/linux/atomic/atomic-arch-fallback.h **** /**
1207:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_dec() - atomic decrement with relaxed ordering
1208:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1209:./include/linux/atomic/atomic-arch-fallback.h ****  *
1210:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with relaxed ordering.
1211:./include/linux/atomic/atomic-arch-fallback.h ****  *
1212:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_dec() elsewhere.
1213:./include/linux/atomic/atomic-arch-fallback.h ****  *
1214:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
1215:./include/linux/atomic/atomic-arch-fallback.h ****  */
1216:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
1217:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_dec(atomic_t *v)
1218:./include/linux/atomic/atomic-arch-fallback.h **** {
1219:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_dec)
1220:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_dec(v);
1221:./include/linux/atomic/atomic-arch-fallback.h **** #else
1222:./include/linux/atomic/atomic-arch-fallback.h **** 	raw_atomic_sub(1, v);
1223:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1224:./include/linux/atomic/atomic-arch-fallback.h **** }
1225:./include/linux/atomic/atomic-arch-fallback.h **** 
1226:./include/linux/atomic/atomic-arch-fallback.h **** /**
1227:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_dec_return() - atomic decrement with full ordering
ARM GAS  /tmp/cc0b6Xiw.s 			page 82


1228:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1229:./include/linux/atomic/atomic-arch-fallback.h ****  *
1230:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with full ordering.
1231:./include/linux/atomic/atomic-arch-fallback.h ****  *
1232:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_dec_return() elsewhere.
1233:./include/linux/atomic/atomic-arch-fallback.h ****  *
1234:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
1235:./include/linux/atomic/atomic-arch-fallback.h ****  */
1236:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1237:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_dec_return(atomic_t *v)
1238:./include/linux/atomic/atomic-arch-fallback.h **** {
1239:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_dec_return)
1240:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_return(v);
1241:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_dec_return_relaxed)
1242:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
1243:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1244:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_dec_return_relaxed(v);
1245:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1246:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1247:./include/linux/atomic/atomic-arch-fallback.h **** #else
1248:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_sub_return(1, v);
1249:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1250:./include/linux/atomic/atomic-arch-fallback.h **** }
1251:./include/linux/atomic/atomic-arch-fallback.h **** 
1252:./include/linux/atomic/atomic-arch-fallback.h **** /**
1253:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_dec_return_acquire() - atomic decrement with acquire ordering
1254:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1255:./include/linux/atomic/atomic-arch-fallback.h ****  *
1256:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with acquire ordering.
1257:./include/linux/atomic/atomic-arch-fallback.h ****  *
1258:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_dec_return_acquire() elsewhere.
1259:./include/linux/atomic/atomic-arch-fallback.h ****  *
1260:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
1261:./include/linux/atomic/atomic-arch-fallback.h ****  */
1262:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1263:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_dec_return_acquire(atomic_t *v)
1264:./include/linux/atomic/atomic-arch-fallback.h **** {
1265:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_dec_return_acquire)
1266:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_return_acquire(v);
1267:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_dec_return_relaxed)
1268:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_dec_return_relaxed(v);
1269:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1270:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1271:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_dec_return)
1272:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_return(v);
1273:./include/linux/atomic/atomic-arch-fallback.h **** #else
1274:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_sub_return_acquire(1, v);
1275:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1276:./include/linux/atomic/atomic-arch-fallback.h **** }
1277:./include/linux/atomic/atomic-arch-fallback.h **** 
1278:./include/linux/atomic/atomic-arch-fallback.h **** /**
1279:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_dec_return_release() - atomic decrement with release ordering
1280:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1281:./include/linux/atomic/atomic-arch-fallback.h ****  *
1282:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with release ordering.
1283:./include/linux/atomic/atomic-arch-fallback.h ****  *
1284:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_dec_return_release() elsewhere.
ARM GAS  /tmp/cc0b6Xiw.s 			page 83


1285:./include/linux/atomic/atomic-arch-fallback.h ****  *
1286:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
1287:./include/linux/atomic/atomic-arch-fallback.h ****  */
1288:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1289:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_dec_return_release(atomic_t *v)
1290:./include/linux/atomic/atomic-arch-fallback.h **** {
1291:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_dec_return_release)
1292:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_return_release(v);
1293:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_dec_return_relaxed)
1294:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
1295:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_return_relaxed(v);
1296:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_dec_return)
1297:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_return(v);
1298:./include/linux/atomic/atomic-arch-fallback.h **** #else
1299:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_sub_return_release(1, v);
1300:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1301:./include/linux/atomic/atomic-arch-fallback.h **** }
1302:./include/linux/atomic/atomic-arch-fallback.h **** 
1303:./include/linux/atomic/atomic-arch-fallback.h **** /**
1304:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_dec_return_relaxed() - atomic decrement with relaxed ordering
1305:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1306:./include/linux/atomic/atomic-arch-fallback.h ****  *
1307:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with relaxed ordering.
1308:./include/linux/atomic/atomic-arch-fallback.h ****  *
1309:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_dec_return_relaxed() elsewhere.
1310:./include/linux/atomic/atomic-arch-fallback.h ****  *
1311:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The updated value of @v.
1312:./include/linux/atomic/atomic-arch-fallback.h ****  */
1313:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1314:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_dec_return_relaxed(atomic_t *v)
1315:./include/linux/atomic/atomic-arch-fallback.h **** {
1316:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_dec_return_relaxed)
1317:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_return_relaxed(v);
1318:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_dec_return)
1319:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_return(v);
1320:./include/linux/atomic/atomic-arch-fallback.h **** #else
1321:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_sub_return_relaxed(1, v);
1322:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1323:./include/linux/atomic/atomic-arch-fallback.h **** }
1324:./include/linux/atomic/atomic-arch-fallback.h **** 
1325:./include/linux/atomic/atomic-arch-fallback.h **** /**
1326:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_dec() - atomic decrement with full ordering
1327:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1328:./include/linux/atomic/atomic-arch-fallback.h ****  *
1329:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with full ordering.
1330:./include/linux/atomic/atomic-arch-fallback.h ****  *
1331:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_dec() elsewhere.
1332:./include/linux/atomic/atomic-arch-fallback.h ****  *
1333:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1334:./include/linux/atomic/atomic-arch-fallback.h ****  */
1335:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1336:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_dec(atomic_t *v)
1337:./include/linux/atomic/atomic-arch-fallback.h **** {
1338:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_dec)
1339:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_dec(v);
1340:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_dec_relaxed)
1341:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
ARM GAS  /tmp/cc0b6Xiw.s 			page 84


1342:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1343:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_fetch_dec_relaxed(v);
1344:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1345:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1346:./include/linux/atomic/atomic-arch-fallback.h **** #else
1347:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_sub(1, v);
1348:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1349:./include/linux/atomic/atomic-arch-fallback.h **** }
1350:./include/linux/atomic/atomic-arch-fallback.h **** 
1351:./include/linux/atomic/atomic-arch-fallback.h **** /**
1352:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_dec_acquire() - atomic decrement with acquire ordering
1353:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1354:./include/linux/atomic/atomic-arch-fallback.h ****  *
1355:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with acquire ordering.
1356:./include/linux/atomic/atomic-arch-fallback.h ****  *
1357:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_dec_acquire() elsewhere.
1358:./include/linux/atomic/atomic-arch-fallback.h ****  *
1359:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1360:./include/linux/atomic/atomic-arch-fallback.h ****  */
1361:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1362:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_dec_acquire(atomic_t *v)
1363:./include/linux/atomic/atomic-arch-fallback.h **** {
1364:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_dec_acquire)
1365:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_dec_acquire(v);
1366:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_dec_relaxed)
1367:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_fetch_dec_relaxed(v);
1368:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1369:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1370:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_dec)
1371:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_dec(v);
1372:./include/linux/atomic/atomic-arch-fallback.h **** #else
1373:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_sub_acquire(1, v);
1374:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1375:./include/linux/atomic/atomic-arch-fallback.h **** }
1376:./include/linux/atomic/atomic-arch-fallback.h **** 
1377:./include/linux/atomic/atomic-arch-fallback.h **** /**
1378:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_dec_release() - atomic decrement with release ordering
1379:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1380:./include/linux/atomic/atomic-arch-fallback.h ****  *
1381:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with release ordering.
1382:./include/linux/atomic/atomic-arch-fallback.h ****  *
1383:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_dec_release() elsewhere.
1384:./include/linux/atomic/atomic-arch-fallback.h ****  *
1385:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1386:./include/linux/atomic/atomic-arch-fallback.h ****  */
1387:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1388:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_dec_release(atomic_t *v)
1389:./include/linux/atomic/atomic-arch-fallback.h **** {
1390:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_dec_release)
1391:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_dec_release(v);
1392:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_dec_relaxed)
1393:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
1394:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_dec_relaxed(v);
1395:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_dec)
1396:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_dec(v);
1397:./include/linux/atomic/atomic-arch-fallback.h **** #else
1398:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_sub_release(1, v);
ARM GAS  /tmp/cc0b6Xiw.s 			page 85


1399:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1400:./include/linux/atomic/atomic-arch-fallback.h **** }
1401:./include/linux/atomic/atomic-arch-fallback.h **** 
1402:./include/linux/atomic/atomic-arch-fallback.h **** /**
1403:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_dec_relaxed() - atomic decrement with relaxed ordering
1404:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1405:./include/linux/atomic/atomic-arch-fallback.h ****  *
1406:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with relaxed ordering.
1407:./include/linux/atomic/atomic-arch-fallback.h ****  *
1408:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_dec_relaxed() elsewhere.
1409:./include/linux/atomic/atomic-arch-fallback.h ****  *
1410:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1411:./include/linux/atomic/atomic-arch-fallback.h ****  */
1412:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1413:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_dec_relaxed(atomic_t *v)
1414:./include/linux/atomic/atomic-arch-fallback.h **** {
1415:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_dec_relaxed)
1416:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_dec_relaxed(v);
1417:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_dec)
1418:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_dec(v);
1419:./include/linux/atomic/atomic-arch-fallback.h **** #else
1420:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_sub_relaxed(1, v);
1421:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1422:./include/linux/atomic/atomic-arch-fallback.h **** }
1423:./include/linux/atomic/atomic-arch-fallback.h **** 
1424:./include/linux/atomic/atomic-arch-fallback.h **** /**
1425:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_and() - atomic bitwise AND with relaxed ordering
1426:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1427:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1428:./include/linux/atomic/atomic-arch-fallback.h ****  *
1429:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & @i) with relaxed ordering.
1430:./include/linux/atomic/atomic-arch-fallback.h ****  *
1431:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_and() elsewhere.
1432:./include/linux/atomic/atomic-arch-fallback.h ****  *
1433:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
1434:./include/linux/atomic/atomic-arch-fallback.h ****  */
1435:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
1436:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_and(int i, atomic_t *v)
1437:./include/linux/atomic/atomic-arch-fallback.h **** {
1438:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_and(i, v);
1439:./include/linux/atomic/atomic-arch-fallback.h **** }
1440:./include/linux/atomic/atomic-arch-fallback.h **** 
1441:./include/linux/atomic/atomic-arch-fallback.h **** /**
1442:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_and() - atomic bitwise AND with full ordering
1443:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1444:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1445:./include/linux/atomic/atomic-arch-fallback.h ****  *
1446:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & @i) with full ordering.
1447:./include/linux/atomic/atomic-arch-fallback.h ****  *
1448:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_and() elsewhere.
1449:./include/linux/atomic/atomic-arch-fallback.h ****  *
1450:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1451:./include/linux/atomic/atomic-arch-fallback.h ****  */
1452:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1453:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_and(int i, atomic_t *v)
1454:./include/linux/atomic/atomic-arch-fallback.h **** {
1455:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_and)
ARM GAS  /tmp/cc0b6Xiw.s 			page 86


1456:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_and(i, v);
1457:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_and_relaxed)
1458:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
1459:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1460:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_fetch_and_relaxed(i, v);
1461:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1462:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1463:./include/linux/atomic/atomic-arch-fallback.h **** #else
1464:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_and"
1465:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1466:./include/linux/atomic/atomic-arch-fallback.h **** }
1467:./include/linux/atomic/atomic-arch-fallback.h **** 
1468:./include/linux/atomic/atomic-arch-fallback.h **** /**
1469:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_and_acquire() - atomic bitwise AND with acquire ordering
1470:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1471:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1472:./include/linux/atomic/atomic-arch-fallback.h ****  *
1473:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & @i) with acquire ordering.
1474:./include/linux/atomic/atomic-arch-fallback.h ****  *
1475:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_and_acquire() elsewhere.
1476:./include/linux/atomic/atomic-arch-fallback.h ****  *
1477:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1478:./include/linux/atomic/atomic-arch-fallback.h ****  */
1479:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1480:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_and_acquire(int i, atomic_t *v)
1481:./include/linux/atomic/atomic-arch-fallback.h **** {
1482:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_and_acquire)
1483:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_and_acquire(i, v);
1484:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_and_relaxed)
1485:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_fetch_and_relaxed(i, v);
1486:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1487:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1488:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_and)
1489:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_and(i, v);
1490:./include/linux/atomic/atomic-arch-fallback.h **** #else
1491:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_and_acquire"
1492:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1493:./include/linux/atomic/atomic-arch-fallback.h **** }
1494:./include/linux/atomic/atomic-arch-fallback.h **** 
1495:./include/linux/atomic/atomic-arch-fallback.h **** /**
1496:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_and_release() - atomic bitwise AND with release ordering
1497:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1498:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1499:./include/linux/atomic/atomic-arch-fallback.h ****  *
1500:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & @i) with release ordering.
1501:./include/linux/atomic/atomic-arch-fallback.h ****  *
1502:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_and_release() elsewhere.
1503:./include/linux/atomic/atomic-arch-fallback.h ****  *
1504:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1505:./include/linux/atomic/atomic-arch-fallback.h ****  */
1506:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1507:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_and_release(int i, atomic_t *v)
1508:./include/linux/atomic/atomic-arch-fallback.h **** {
1509:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_and_release)
1510:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_and_release(i, v);
1511:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_and_relaxed)
1512:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
ARM GAS  /tmp/cc0b6Xiw.s 			page 87


1513:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_and_relaxed(i, v);
1514:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_and)
1515:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_and(i, v);
1516:./include/linux/atomic/atomic-arch-fallback.h **** #else
1517:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_and_release"
1518:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1519:./include/linux/atomic/atomic-arch-fallback.h **** }
1520:./include/linux/atomic/atomic-arch-fallback.h **** 
1521:./include/linux/atomic/atomic-arch-fallback.h **** /**
1522:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_and_relaxed() - atomic bitwise AND with relaxed ordering
1523:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1524:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1525:./include/linux/atomic/atomic-arch-fallback.h ****  *
1526:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & @i) with relaxed ordering.
1527:./include/linux/atomic/atomic-arch-fallback.h ****  *
1528:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_and_relaxed() elsewhere.
1529:./include/linux/atomic/atomic-arch-fallback.h ****  *
1530:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1531:./include/linux/atomic/atomic-arch-fallback.h ****  */
1532:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1533:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_and_relaxed(int i, atomic_t *v)
1534:./include/linux/atomic/atomic-arch-fallback.h **** {
1535:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_and_relaxed)
1536:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_and_relaxed(i, v);
1537:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_and)
1538:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_and(i, v);
1539:./include/linux/atomic/atomic-arch-fallback.h **** #else
1540:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_and_relaxed"
1541:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1542:./include/linux/atomic/atomic-arch-fallback.h **** }
1543:./include/linux/atomic/atomic-arch-fallback.h **** 
1544:./include/linux/atomic/atomic-arch-fallback.h **** /**
1545:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_andnot() - atomic bitwise AND NOT with relaxed ordering
1546:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1547:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1548:./include/linux/atomic/atomic-arch-fallback.h ****  *
1549:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & ~@i) with relaxed ordering.
1550:./include/linux/atomic/atomic-arch-fallback.h ****  *
1551:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_andnot() elsewhere.
1552:./include/linux/atomic/atomic-arch-fallback.h ****  *
1553:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
1554:./include/linux/atomic/atomic-arch-fallback.h ****  */
1555:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
1556:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_andnot(int i, atomic_t *v)
1557:./include/linux/atomic/atomic-arch-fallback.h **** {
1558:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_andnot)
1559:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_andnot(i, v);
1560:./include/linux/atomic/atomic-arch-fallback.h **** #else
1561:./include/linux/atomic/atomic-arch-fallback.h **** 	raw_atomic_and(~i, v);
1562:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1563:./include/linux/atomic/atomic-arch-fallback.h **** }
1564:./include/linux/atomic/atomic-arch-fallback.h **** 
1565:./include/linux/atomic/atomic-arch-fallback.h **** /**
1566:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_andnot() - atomic bitwise AND NOT with full ordering
1567:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1568:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1569:./include/linux/atomic/atomic-arch-fallback.h ****  *
ARM GAS  /tmp/cc0b6Xiw.s 			page 88


1570:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & ~@i) with full ordering.
1571:./include/linux/atomic/atomic-arch-fallback.h ****  *
1572:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_andnot() elsewhere.
1573:./include/linux/atomic/atomic-arch-fallback.h ****  *
1574:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1575:./include/linux/atomic/atomic-arch-fallback.h ****  */
1576:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1577:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_andnot(int i, atomic_t *v)
1578:./include/linux/atomic/atomic-arch-fallback.h **** {
1579:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_andnot)
1580:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_andnot(i, v);
1581:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_andnot_relaxed)
1582:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
1583:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1584:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_fetch_andnot_relaxed(i, v);
1585:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1586:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1587:./include/linux/atomic/atomic-arch-fallback.h **** #else
1588:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_and(~i, v);
1589:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1590:./include/linux/atomic/atomic-arch-fallback.h **** }
1591:./include/linux/atomic/atomic-arch-fallback.h **** 
1592:./include/linux/atomic/atomic-arch-fallback.h **** /**
1593:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_andnot_acquire() - atomic bitwise AND NOT with acquire ordering
1594:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1595:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1596:./include/linux/atomic/atomic-arch-fallback.h ****  *
1597:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & ~@i) with acquire ordering.
1598:./include/linux/atomic/atomic-arch-fallback.h ****  *
1599:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_andnot_acquire() elsewhere.
1600:./include/linux/atomic/atomic-arch-fallback.h ****  *
1601:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1602:./include/linux/atomic/atomic-arch-fallback.h ****  */
1603:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1604:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_andnot_acquire(int i, atomic_t *v)
1605:./include/linux/atomic/atomic-arch-fallback.h **** {
1606:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_andnot_acquire)
1607:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_andnot_acquire(i, v);
1608:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_andnot_relaxed)
1609:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_fetch_andnot_relaxed(i, v);
1610:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1611:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1612:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_andnot)
1613:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_andnot(i, v);
1614:./include/linux/atomic/atomic-arch-fallback.h **** #else
1615:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_and_acquire(~i, v);
1616:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1617:./include/linux/atomic/atomic-arch-fallback.h **** }
1618:./include/linux/atomic/atomic-arch-fallback.h **** 
1619:./include/linux/atomic/atomic-arch-fallback.h **** /**
1620:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_andnot_release() - atomic bitwise AND NOT with release ordering
1621:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1622:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1623:./include/linux/atomic/atomic-arch-fallback.h ****  *
1624:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & ~@i) with release ordering.
1625:./include/linux/atomic/atomic-arch-fallback.h ****  *
1626:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_andnot_release() elsewhere.
ARM GAS  /tmp/cc0b6Xiw.s 			page 89


1627:./include/linux/atomic/atomic-arch-fallback.h ****  *
1628:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1629:./include/linux/atomic/atomic-arch-fallback.h ****  */
1630:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1631:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_andnot_release(int i, atomic_t *v)
1632:./include/linux/atomic/atomic-arch-fallback.h **** {
1633:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_andnot_release)
1634:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_andnot_release(i, v);
1635:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_andnot_relaxed)
1636:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
1637:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_andnot_relaxed(i, v);
1638:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_andnot)
1639:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_andnot(i, v);
1640:./include/linux/atomic/atomic-arch-fallback.h **** #else
1641:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_and_release(~i, v);
1642:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1643:./include/linux/atomic/atomic-arch-fallback.h **** }
1644:./include/linux/atomic/atomic-arch-fallback.h **** 
1645:./include/linux/atomic/atomic-arch-fallback.h **** /**
1646:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_andnot_relaxed() - atomic bitwise AND NOT with relaxed ordering
1647:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1648:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1649:./include/linux/atomic/atomic-arch-fallback.h ****  *
1650:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v & ~@i) with relaxed ordering.
1651:./include/linux/atomic/atomic-arch-fallback.h ****  *
1652:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_andnot_relaxed() elsewhere.
1653:./include/linux/atomic/atomic-arch-fallback.h ****  *
1654:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1655:./include/linux/atomic/atomic-arch-fallback.h ****  */
1656:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1657:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_andnot_relaxed(int i, atomic_t *v)
1658:./include/linux/atomic/atomic-arch-fallback.h **** {
1659:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_andnot_relaxed)
1660:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_andnot_relaxed(i, v);
1661:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_andnot)
1662:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_andnot(i, v);
1663:./include/linux/atomic/atomic-arch-fallback.h **** #else
1664:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_fetch_and_relaxed(~i, v);
1665:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1666:./include/linux/atomic/atomic-arch-fallback.h **** }
1667:./include/linux/atomic/atomic-arch-fallback.h **** 
1668:./include/linux/atomic/atomic-arch-fallback.h **** /**
1669:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_or() - atomic bitwise OR with relaxed ordering
1670:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1671:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1672:./include/linux/atomic/atomic-arch-fallback.h ****  *
1673:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v | @i) with relaxed ordering.
1674:./include/linux/atomic/atomic-arch-fallback.h ****  *
1675:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_or() elsewhere.
1676:./include/linux/atomic/atomic-arch-fallback.h ****  *
1677:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
1678:./include/linux/atomic/atomic-arch-fallback.h ****  */
1679:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
1680:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_or(int i, atomic_t *v)
1681:./include/linux/atomic/atomic-arch-fallback.h **** {
1682:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_or(i, v);
1683:./include/linux/atomic/atomic-arch-fallback.h **** }
ARM GAS  /tmp/cc0b6Xiw.s 			page 90


1684:./include/linux/atomic/atomic-arch-fallback.h **** 
1685:./include/linux/atomic/atomic-arch-fallback.h **** /**
1686:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_or() - atomic bitwise OR with full ordering
1687:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1688:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1689:./include/linux/atomic/atomic-arch-fallback.h ****  *
1690:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v | @i) with full ordering.
1691:./include/linux/atomic/atomic-arch-fallback.h ****  *
1692:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_or() elsewhere.
1693:./include/linux/atomic/atomic-arch-fallback.h ****  *
1694:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1695:./include/linux/atomic/atomic-arch-fallback.h ****  */
1696:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1697:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_or(int i, atomic_t *v)
1698:./include/linux/atomic/atomic-arch-fallback.h **** {
1699:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_or)
1700:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_or(i, v);
1701:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_or_relaxed)
1702:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
1703:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1704:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_fetch_or_relaxed(i, v);
1705:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1706:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1707:./include/linux/atomic/atomic-arch-fallback.h **** #else
1708:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_or"
1709:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1710:./include/linux/atomic/atomic-arch-fallback.h **** }
1711:./include/linux/atomic/atomic-arch-fallback.h **** 
1712:./include/linux/atomic/atomic-arch-fallback.h **** /**
1713:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_or_acquire() - atomic bitwise OR with acquire ordering
1714:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1715:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1716:./include/linux/atomic/atomic-arch-fallback.h ****  *
1717:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v | @i) with acquire ordering.
1718:./include/linux/atomic/atomic-arch-fallback.h ****  *
1719:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_or_acquire() elsewhere.
1720:./include/linux/atomic/atomic-arch-fallback.h ****  *
1721:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1722:./include/linux/atomic/atomic-arch-fallback.h ****  */
1723:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1724:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_or_acquire(int i, atomic_t *v)
1725:./include/linux/atomic/atomic-arch-fallback.h **** {
1726:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_or_acquire)
1727:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_or_acquire(i, v);
1728:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_or_relaxed)
1729:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_fetch_or_relaxed(i, v);
1730:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1731:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1732:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_or)
1733:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_or(i, v);
1734:./include/linux/atomic/atomic-arch-fallback.h **** #else
1735:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_or_acquire"
1736:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1737:./include/linux/atomic/atomic-arch-fallback.h **** }
1738:./include/linux/atomic/atomic-arch-fallback.h **** 
1739:./include/linux/atomic/atomic-arch-fallback.h **** /**
1740:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_or_release() - atomic bitwise OR with release ordering
ARM GAS  /tmp/cc0b6Xiw.s 			page 91


1741:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1742:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1743:./include/linux/atomic/atomic-arch-fallback.h ****  *
1744:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v | @i) with release ordering.
1745:./include/linux/atomic/atomic-arch-fallback.h ****  *
1746:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_or_release() elsewhere.
1747:./include/linux/atomic/atomic-arch-fallback.h ****  *
1748:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1749:./include/linux/atomic/atomic-arch-fallback.h ****  */
1750:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1751:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_or_release(int i, atomic_t *v)
1752:./include/linux/atomic/atomic-arch-fallback.h **** {
1753:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_or_release)
1754:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_or_release(i, v);
1755:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_or_relaxed)
1756:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
1757:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_or_relaxed(i, v);
1758:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_or)
1759:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_or(i, v);
1760:./include/linux/atomic/atomic-arch-fallback.h **** #else
1761:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_or_release"
1762:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1763:./include/linux/atomic/atomic-arch-fallback.h **** }
1764:./include/linux/atomic/atomic-arch-fallback.h **** 
1765:./include/linux/atomic/atomic-arch-fallback.h **** /**
1766:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_or_relaxed() - atomic bitwise OR with relaxed ordering
1767:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1768:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1769:./include/linux/atomic/atomic-arch-fallback.h ****  *
1770:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v | @i) with relaxed ordering.
1771:./include/linux/atomic/atomic-arch-fallback.h ****  *
1772:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_or_relaxed() elsewhere.
1773:./include/linux/atomic/atomic-arch-fallback.h ****  *
1774:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1775:./include/linux/atomic/atomic-arch-fallback.h ****  */
1776:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1777:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_or_relaxed(int i, atomic_t *v)
1778:./include/linux/atomic/atomic-arch-fallback.h **** {
1779:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_or_relaxed)
1780:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_or_relaxed(i, v);
1781:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_or)
1782:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_or(i, v);
1783:./include/linux/atomic/atomic-arch-fallback.h **** #else
1784:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_or_relaxed"
1785:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1786:./include/linux/atomic/atomic-arch-fallback.h **** }
1787:./include/linux/atomic/atomic-arch-fallback.h **** 
1788:./include/linux/atomic/atomic-arch-fallback.h **** /**
1789:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_xor() - atomic bitwise XOR with relaxed ordering
1790:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1791:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1792:./include/linux/atomic/atomic-arch-fallback.h ****  *
1793:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v ^ @i) with relaxed ordering.
1794:./include/linux/atomic/atomic-arch-fallback.h ****  *
1795:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_xor() elsewhere.
1796:./include/linux/atomic/atomic-arch-fallback.h ****  *
1797:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: Nothing.
ARM GAS  /tmp/cc0b6Xiw.s 			page 92


1798:./include/linux/atomic/atomic-arch-fallback.h ****  */
1799:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline void
1800:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_xor(int i, atomic_t *v)
1801:./include/linux/atomic/atomic-arch-fallback.h **** {
1802:./include/linux/atomic/atomic-arch-fallback.h **** 	arch_atomic_xor(i, v);
1803:./include/linux/atomic/atomic-arch-fallback.h **** }
1804:./include/linux/atomic/atomic-arch-fallback.h **** 
1805:./include/linux/atomic/atomic-arch-fallback.h **** /**
1806:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_xor() - atomic bitwise XOR with full ordering
1807:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1808:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1809:./include/linux/atomic/atomic-arch-fallback.h ****  *
1810:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v ^ @i) with full ordering.
1811:./include/linux/atomic/atomic-arch-fallback.h ****  *
1812:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_xor() elsewhere.
1813:./include/linux/atomic/atomic-arch-fallback.h ****  *
1814:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1815:./include/linux/atomic/atomic-arch-fallback.h ****  */
1816:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1817:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_xor(int i, atomic_t *v)
1818:./include/linux/atomic/atomic-arch-fallback.h **** {
1819:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_xor)
1820:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_xor(i, v);
1821:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_xor_relaxed)
1822:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
1823:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1824:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_fetch_xor_relaxed(i, v);
1825:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1826:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1827:./include/linux/atomic/atomic-arch-fallback.h **** #else
1828:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_xor"
1829:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1830:./include/linux/atomic/atomic-arch-fallback.h **** }
1831:./include/linux/atomic/atomic-arch-fallback.h **** 
1832:./include/linux/atomic/atomic-arch-fallback.h **** /**
1833:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_xor_acquire() - atomic bitwise XOR with acquire ordering
1834:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1835:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1836:./include/linux/atomic/atomic-arch-fallback.h ****  *
1837:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v ^ @i) with acquire ordering.
1838:./include/linux/atomic/atomic-arch-fallback.h ****  *
1839:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_xor_acquire() elsewhere.
1840:./include/linux/atomic/atomic-arch-fallback.h ****  *
1841:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1842:./include/linux/atomic/atomic-arch-fallback.h ****  */
1843:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1844:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_xor_acquire(int i, atomic_t *v)
1845:./include/linux/atomic/atomic-arch-fallback.h **** {
1846:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_xor_acquire)
1847:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_xor_acquire(i, v);
1848:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_xor_relaxed)
1849:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_fetch_xor_relaxed(i, v);
1850:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1851:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1852:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_xor)
1853:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_xor(i, v);
1854:./include/linux/atomic/atomic-arch-fallback.h **** #else
ARM GAS  /tmp/cc0b6Xiw.s 			page 93


1855:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_xor_acquire"
1856:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1857:./include/linux/atomic/atomic-arch-fallback.h **** }
1858:./include/linux/atomic/atomic-arch-fallback.h **** 
1859:./include/linux/atomic/atomic-arch-fallback.h **** /**
1860:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_xor_release() - atomic bitwise XOR with release ordering
1861:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1862:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1863:./include/linux/atomic/atomic-arch-fallback.h ****  *
1864:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v ^ @i) with release ordering.
1865:./include/linux/atomic/atomic-arch-fallback.h ****  *
1866:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_xor_release() elsewhere.
1867:./include/linux/atomic/atomic-arch-fallback.h ****  *
1868:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1869:./include/linux/atomic/atomic-arch-fallback.h ****  */
1870:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1871:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_xor_release(int i, atomic_t *v)
1872:./include/linux/atomic/atomic-arch-fallback.h **** {
1873:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_xor_release)
1874:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_xor_release(i, v);
1875:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_xor_relaxed)
1876:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
1877:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_xor_relaxed(i, v);
1878:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_xor)
1879:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_xor(i, v);
1880:./include/linux/atomic/atomic-arch-fallback.h **** #else
1881:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_xor_release"
1882:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1883:./include/linux/atomic/atomic-arch-fallback.h **** }
1884:./include/linux/atomic/atomic-arch-fallback.h **** 
1885:./include/linux/atomic/atomic-arch-fallback.h **** /**
1886:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_fetch_xor_relaxed() - atomic bitwise XOR with relaxed ordering
1887:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value
1888:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1889:./include/linux/atomic/atomic-arch-fallback.h ****  *
1890:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v ^ @i) with relaxed ordering.
1891:./include/linux/atomic/atomic-arch-fallback.h ****  *
1892:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_fetch_xor_relaxed() elsewhere.
1893:./include/linux/atomic/atomic-arch-fallback.h ****  *
1894:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1895:./include/linux/atomic/atomic-arch-fallback.h ****  */
1896:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1897:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_fetch_xor_relaxed(int i, atomic_t *v)
1898:./include/linux/atomic/atomic-arch-fallback.h **** {
1899:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_fetch_xor_relaxed)
1900:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_xor_relaxed(i, v);
1901:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_fetch_xor)
1902:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_fetch_xor(i, v);
1903:./include/linux/atomic/atomic-arch-fallback.h **** #else
1904:./include/linux/atomic/atomic-arch-fallback.h **** #error "Unable to define raw_atomic_fetch_xor_relaxed"
1905:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1906:./include/linux/atomic/atomic-arch-fallback.h **** }
1907:./include/linux/atomic/atomic-arch-fallback.h **** 
1908:./include/linux/atomic/atomic-arch-fallback.h **** /**
1909:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_xchg() - atomic exchange with full ordering
1910:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1911:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
ARM GAS  /tmp/cc0b6Xiw.s 			page 94


1912:./include/linux/atomic/atomic-arch-fallback.h ****  *
1913:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to @new with full ordering.
1914:./include/linux/atomic/atomic-arch-fallback.h ****  *
1915:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_xchg() elsewhere.
1916:./include/linux/atomic/atomic-arch-fallback.h ****  *
1917:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1918:./include/linux/atomic/atomic-arch-fallback.h ****  */
1919:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1920:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_xchg(atomic_t *v, int new)
1921:./include/linux/atomic/atomic-arch-fallback.h **** {
1922:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_xchg)
1923:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_xchg(v, new);
1924:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_xchg_relaxed)
1925:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
1926:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
1927:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_xchg_relaxed(v, new);
1928:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
1929:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1930:./include/linux/atomic/atomic-arch-fallback.h **** #else
1931:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_xchg(&v->counter, new);
1932:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1933:./include/linux/atomic/atomic-arch-fallback.h **** }
1934:./include/linux/atomic/atomic-arch-fallback.h **** 
1935:./include/linux/atomic/atomic-arch-fallback.h **** /**
1936:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_xchg_acquire() - atomic exchange with acquire ordering
1937:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1938:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
1939:./include/linux/atomic/atomic-arch-fallback.h ****  *
1940:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to @new with acquire ordering.
1941:./include/linux/atomic/atomic-arch-fallback.h ****  *
1942:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_xchg_acquire() elsewhere.
1943:./include/linux/atomic/atomic-arch-fallback.h ****  *
1944:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1945:./include/linux/atomic/atomic-arch-fallback.h ****  */
1946:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1947:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_xchg_acquire(atomic_t *v, int new)
1948:./include/linux/atomic/atomic-arch-fallback.h **** {
1949:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_xchg_acquire)
1950:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_xchg_acquire(v, new);
1951:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_xchg_relaxed)
1952:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_xchg_relaxed(v, new);
1953:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
1954:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
1955:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_xchg)
1956:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_xchg(v, new);
1957:./include/linux/atomic/atomic-arch-fallback.h **** #else
1958:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_xchg_acquire(&v->counter, new);
1959:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1960:./include/linux/atomic/atomic-arch-fallback.h **** }
1961:./include/linux/atomic/atomic-arch-fallback.h **** 
1962:./include/linux/atomic/atomic-arch-fallback.h **** /**
1963:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_xchg_release() - atomic exchange with release ordering
1964:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1965:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
1966:./include/linux/atomic/atomic-arch-fallback.h ****  *
1967:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to @new with release ordering.
1968:./include/linux/atomic/atomic-arch-fallback.h ****  *
ARM GAS  /tmp/cc0b6Xiw.s 			page 95


1969:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_xchg_release() elsewhere.
1970:./include/linux/atomic/atomic-arch-fallback.h ****  *
1971:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1972:./include/linux/atomic/atomic-arch-fallback.h ****  */
1973:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
1974:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_xchg_release(atomic_t *v, int new)
1975:./include/linux/atomic/atomic-arch-fallback.h **** {
1976:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_xchg_release)
1977:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_xchg_release(v, new);
1978:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_xchg_relaxed)
1979:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
1980:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_xchg_relaxed(v, new);
1981:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_xchg)
1982:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_xchg(v, new);
1983:./include/linux/atomic/atomic-arch-fallback.h **** #else
1984:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_xchg_release(&v->counter, new);
1985:./include/linux/atomic/atomic-arch-fallback.h **** #endif
1986:./include/linux/atomic/atomic-arch-fallback.h **** }
1987:./include/linux/atomic/atomic-arch-fallback.h **** 
1988:./include/linux/atomic/atomic-arch-fallback.h **** /**
1989:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_xchg_relaxed() - atomic exchange with relaxed ordering
1990:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
1991:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
1992:./include/linux/atomic/atomic-arch-fallback.h ****  *
1993:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to @new with relaxed ordering.
1994:./include/linux/atomic/atomic-arch-fallback.h ****  *
1995:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_xchg_relaxed() elsewhere.
1996:./include/linux/atomic/atomic-arch-fallback.h ****  *
1997:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
1998:./include/linux/atomic/atomic-arch-fallback.h ****  */
1999:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
2000:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_xchg_relaxed(atomic_t *v, int new)
2001:./include/linux/atomic/atomic-arch-fallback.h **** {
2002:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_xchg_relaxed)
2003:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_xchg_relaxed(v, new);
2004:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_xchg)
2005:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_xchg(v, new);
2006:./include/linux/atomic/atomic-arch-fallback.h **** #else
2007:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_xchg_relaxed(&v->counter, new);
2008:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2009:./include/linux/atomic/atomic-arch-fallback.h **** }
2010:./include/linux/atomic/atomic-arch-fallback.h **** 
2011:./include/linux/atomic/atomic-arch-fallback.h **** /**
2012:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_cmpxchg() - atomic compare and exchange with full ordering
2013:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2014:./include/linux/atomic/atomic-arch-fallback.h ****  * @old: int value to compare with
2015:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
2016:./include/linux/atomic/atomic-arch-fallback.h ****  *
2017:./include/linux/atomic/atomic-arch-fallback.h ****  * If (@v == @old), atomically updates @v to @new with full ordering.
2018:./include/linux/atomic/atomic-arch-fallback.h ****  * Otherwise, @v is not modified and relaxed ordering is provided.
2019:./include/linux/atomic/atomic-arch-fallback.h ****  *
2020:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_cmpxchg() elsewhere.
2021:./include/linux/atomic/atomic-arch-fallback.h ****  *
2022:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
2023:./include/linux/atomic/atomic-arch-fallback.h ****  */
2024:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
2025:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_cmpxchg(atomic_t *v, int old, int new)
ARM GAS  /tmp/cc0b6Xiw.s 			page 96


2026:./include/linux/atomic/atomic-arch-fallback.h **** {
2027:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_cmpxchg)
2028:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_cmpxchg(v, old, new);
2029:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_cmpxchg_relaxed)
2030:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret;
2031:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
2032:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_cmpxchg_relaxed(v, old, new);
2033:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
2034:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
2035:./include/linux/atomic/atomic-arch-fallback.h **** #else
2036:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_cmpxchg(&v->counter, old, new);
2037:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2038:./include/linux/atomic/atomic-arch-fallback.h **** }
2039:./include/linux/atomic/atomic-arch-fallback.h **** 
2040:./include/linux/atomic/atomic-arch-fallback.h **** /**
2041:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_cmpxchg_acquire() - atomic compare and exchange with acquire ordering
2042:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2043:./include/linux/atomic/atomic-arch-fallback.h ****  * @old: int value to compare with
2044:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
2045:./include/linux/atomic/atomic-arch-fallback.h ****  *
2046:./include/linux/atomic/atomic-arch-fallback.h ****  * If (@v == @old), atomically updates @v to @new with acquire ordering.
2047:./include/linux/atomic/atomic-arch-fallback.h ****  * Otherwise, @v is not modified and relaxed ordering is provided.
2048:./include/linux/atomic/atomic-arch-fallback.h ****  *
2049:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_cmpxchg_acquire() elsewhere.
2050:./include/linux/atomic/atomic-arch-fallback.h ****  *
2051:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
2052:./include/linux/atomic/atomic-arch-fallback.h ****  */
2053:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
2054:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
2055:./include/linux/atomic/atomic-arch-fallback.h **** {
2056:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_cmpxchg_acquire)
2057:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_cmpxchg_acquire(v, old, new);
2058:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_cmpxchg_relaxed)
2059:./include/linux/atomic/atomic-arch-fallback.h **** 	int ret = arch_atomic_cmpxchg_relaxed(v, old, new);
2060:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
2061:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
2062:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_cmpxchg)
2063:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_cmpxchg(v, old, new);
2064:./include/linux/atomic/atomic-arch-fallback.h **** #else
2065:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_cmpxchg_acquire(&v->counter, old, new);
2066:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2067:./include/linux/atomic/atomic-arch-fallback.h **** }
2068:./include/linux/atomic/atomic-arch-fallback.h **** 
2069:./include/linux/atomic/atomic-arch-fallback.h **** /**
2070:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_cmpxchg_release() - atomic compare and exchange with release ordering
2071:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2072:./include/linux/atomic/atomic-arch-fallback.h ****  * @old: int value to compare with
2073:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
2074:./include/linux/atomic/atomic-arch-fallback.h ****  *
2075:./include/linux/atomic/atomic-arch-fallback.h ****  * If (@v == @old), atomically updates @v to @new with release ordering.
2076:./include/linux/atomic/atomic-arch-fallback.h ****  * Otherwise, @v is not modified and relaxed ordering is provided.
2077:./include/linux/atomic/atomic-arch-fallback.h ****  *
2078:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_cmpxchg_release() elsewhere.
2079:./include/linux/atomic/atomic-arch-fallback.h ****  *
2080:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
2081:./include/linux/atomic/atomic-arch-fallback.h ****  */
2082:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
ARM GAS  /tmp/cc0b6Xiw.s 			page 97


2083:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_cmpxchg_release(atomic_t *v, int old, int new)
2084:./include/linux/atomic/atomic-arch-fallback.h **** {
2085:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_cmpxchg_release)
2086:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_cmpxchg_release(v, old, new);
2087:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_cmpxchg_relaxed)
2088:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
2089:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_cmpxchg_relaxed(v, old, new);
2090:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_cmpxchg)
2091:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_cmpxchg(v, old, new);
2092:./include/linux/atomic/atomic-arch-fallback.h **** #else
2093:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_cmpxchg_release(&v->counter, old, new);
2094:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2095:./include/linux/atomic/atomic-arch-fallback.h **** }
2096:./include/linux/atomic/atomic-arch-fallback.h **** 
2097:./include/linux/atomic/atomic-arch-fallback.h **** /**
2098:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_cmpxchg_relaxed() - atomic compare and exchange with relaxed ordering
2099:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2100:./include/linux/atomic/atomic-arch-fallback.h ****  * @old: int value to compare with
2101:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
2102:./include/linux/atomic/atomic-arch-fallback.h ****  *
2103:./include/linux/atomic/atomic-arch-fallback.h ****  * If (@v == @old), atomically updates @v to @new with relaxed ordering.
2104:./include/linux/atomic/atomic-arch-fallback.h ****  * Otherwise, @v is not modified and relaxed ordering is provided.
2105:./include/linux/atomic/atomic-arch-fallback.h ****  *
2106:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_cmpxchg_relaxed() elsewhere.
2107:./include/linux/atomic/atomic-arch-fallback.h ****  *
2108:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: The original value of @v.
2109:./include/linux/atomic/atomic-arch-fallback.h ****  */
2110:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline int
2111:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)
2112:./include/linux/atomic/atomic-arch-fallback.h **** {
2113:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_cmpxchg_relaxed)
2114:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_cmpxchg_relaxed(v, old, new);
2115:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_cmpxchg)
2116:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_cmpxchg(v, old, new);
2117:./include/linux/atomic/atomic-arch-fallback.h **** #else
2118:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_cmpxchg_relaxed(&v->counter, old, new);
2119:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2120:./include/linux/atomic/atomic-arch-fallback.h **** }
2121:./include/linux/atomic/atomic-arch-fallback.h **** 
2122:./include/linux/atomic/atomic-arch-fallback.h **** /**
2123:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_try_cmpxchg() - atomic compare and exchange with full ordering
2124:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2125:./include/linux/atomic/atomic-arch-fallback.h ****  * @old: pointer to int value to compare with
2126:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
2127:./include/linux/atomic/atomic-arch-fallback.h ****  *
2128:./include/linux/atomic/atomic-arch-fallback.h ****  * If (@v == @old), atomically updates @v to @new with full ordering.
2129:./include/linux/atomic/atomic-arch-fallback.h ****  * Otherwise, @v is not modified, @old is updated to the current value of @v,
2130:./include/linux/atomic/atomic-arch-fallback.h ****  * and relaxed ordering is provided.
2131:./include/linux/atomic/atomic-arch-fallback.h ****  *
2132:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_try_cmpxchg() elsewhere.
2133:./include/linux/atomic/atomic-arch-fallback.h ****  *
2134:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: @true if the exchange occured, @false otherwise.
2135:./include/linux/atomic/atomic-arch-fallback.h ****  */
2136:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline bool
2137:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_try_cmpxchg(atomic_t *v, int *old, int new)
2138:./include/linux/atomic/atomic-arch-fallback.h **** {
2139:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_try_cmpxchg)
ARM GAS  /tmp/cc0b6Xiw.s 			page 98


2140:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_try_cmpxchg(v, old, new);
2141:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_try_cmpxchg_relaxed)
2142:./include/linux/atomic/atomic-arch-fallback.h **** 	bool ret;
2143:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
2144:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_try_cmpxchg_relaxed(v, old, new);
2145:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
2146:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
2147:./include/linux/atomic/atomic-arch-fallback.h **** #else
2148:./include/linux/atomic/atomic-arch-fallback.h **** 	int r, o = *old;
2149:./include/linux/atomic/atomic-arch-fallback.h **** 	r = raw_atomic_cmpxchg(v, o, new);
2150:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(r != o))
2151:./include/linux/atomic/atomic-arch-fallback.h **** 		*old = r;
2152:./include/linux/atomic/atomic-arch-fallback.h **** 	return likely(r == o);
2153:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2154:./include/linux/atomic/atomic-arch-fallback.h **** }
2155:./include/linux/atomic/atomic-arch-fallback.h **** 
2156:./include/linux/atomic/atomic-arch-fallback.h **** /**
2157:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_try_cmpxchg_acquire() - atomic compare and exchange with acquire ordering
2158:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2159:./include/linux/atomic/atomic-arch-fallback.h ****  * @old: pointer to int value to compare with
2160:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
2161:./include/linux/atomic/atomic-arch-fallback.h ****  *
2162:./include/linux/atomic/atomic-arch-fallback.h ****  * If (@v == @old), atomically updates @v to @new with acquire ordering.
2163:./include/linux/atomic/atomic-arch-fallback.h ****  * Otherwise, @v is not modified, @old is updated to the current value of @v,
2164:./include/linux/atomic/atomic-arch-fallback.h ****  * and relaxed ordering is provided.
2165:./include/linux/atomic/atomic-arch-fallback.h ****  *
2166:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_try_cmpxchg_acquire() elsewhere.
2167:./include/linux/atomic/atomic-arch-fallback.h ****  *
2168:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: @true if the exchange occured, @false otherwise.
2169:./include/linux/atomic/atomic-arch-fallback.h ****  */
2170:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline bool
2171:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
2172:./include/linux/atomic/atomic-arch-fallback.h **** {
2173:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_try_cmpxchg_acquire)
2174:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_try_cmpxchg_acquire(v, old, new);
2175:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_try_cmpxchg_relaxed)
2176:./include/linux/atomic/atomic-arch-fallback.h **** 	bool ret = arch_atomic_try_cmpxchg_relaxed(v, old, new);
2177:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_acquire_fence();
2178:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
2179:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_try_cmpxchg)
2180:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_try_cmpxchg(v, old, new);
2181:./include/linux/atomic/atomic-arch-fallback.h **** #else
2182:./include/linux/atomic/atomic-arch-fallback.h **** 	int r, o = *old;
2183:./include/linux/atomic/atomic-arch-fallback.h **** 	r = raw_atomic_cmpxchg_acquire(v, o, new);
2184:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(r != o))
2185:./include/linux/atomic/atomic-arch-fallback.h **** 		*old = r;
2186:./include/linux/atomic/atomic-arch-fallback.h **** 	return likely(r == o);
2187:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2188:./include/linux/atomic/atomic-arch-fallback.h **** }
2189:./include/linux/atomic/atomic-arch-fallback.h **** 
2190:./include/linux/atomic/atomic-arch-fallback.h **** /**
2191:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_try_cmpxchg_release() - atomic compare and exchange with release ordering
2192:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2193:./include/linux/atomic/atomic-arch-fallback.h ****  * @old: pointer to int value to compare with
2194:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
2195:./include/linux/atomic/atomic-arch-fallback.h ****  *
2196:./include/linux/atomic/atomic-arch-fallback.h ****  * If (@v == @old), atomically updates @v to @new with release ordering.
ARM GAS  /tmp/cc0b6Xiw.s 			page 99


2197:./include/linux/atomic/atomic-arch-fallback.h ****  * Otherwise, @v is not modified, @old is updated to the current value of @v,
2198:./include/linux/atomic/atomic-arch-fallback.h ****  * and relaxed ordering is provided.
2199:./include/linux/atomic/atomic-arch-fallback.h ****  *
2200:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_try_cmpxchg_release() elsewhere.
2201:./include/linux/atomic/atomic-arch-fallback.h ****  *
2202:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: @true if the exchange occured, @false otherwise.
2203:./include/linux/atomic/atomic-arch-fallback.h ****  */
2204:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline bool
2205:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)
2206:./include/linux/atomic/atomic-arch-fallback.h **** {
2207:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_try_cmpxchg_release)
2208:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_try_cmpxchg_release(v, old, new);
2209:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_try_cmpxchg_relaxed)
2210:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_release_fence();
2211:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_try_cmpxchg_relaxed(v, old, new);
2212:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_try_cmpxchg)
2213:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_try_cmpxchg(v, old, new);
2214:./include/linux/atomic/atomic-arch-fallback.h **** #else
2215:./include/linux/atomic/atomic-arch-fallback.h **** 	int r, o = *old;
2216:./include/linux/atomic/atomic-arch-fallback.h **** 	r = raw_atomic_cmpxchg_release(v, o, new);
2217:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(r != o))
2218:./include/linux/atomic/atomic-arch-fallback.h **** 		*old = r;
2219:./include/linux/atomic/atomic-arch-fallback.h **** 	return likely(r == o);
2220:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2221:./include/linux/atomic/atomic-arch-fallback.h **** }
2222:./include/linux/atomic/atomic-arch-fallback.h **** 
2223:./include/linux/atomic/atomic-arch-fallback.h **** /**
2224:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_try_cmpxchg_relaxed() - atomic compare and exchange with relaxed ordering
2225:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2226:./include/linux/atomic/atomic-arch-fallback.h ****  * @old: pointer to int value to compare with
2227:./include/linux/atomic/atomic-arch-fallback.h ****  * @new: int value to assign
2228:./include/linux/atomic/atomic-arch-fallback.h ****  *
2229:./include/linux/atomic/atomic-arch-fallback.h ****  * If (@v == @old), atomically updates @v to @new with relaxed ordering.
2230:./include/linux/atomic/atomic-arch-fallback.h ****  * Otherwise, @v is not modified, @old is updated to the current value of @v,
2231:./include/linux/atomic/atomic-arch-fallback.h ****  * and relaxed ordering is provided.
2232:./include/linux/atomic/atomic-arch-fallback.h ****  *
2233:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_try_cmpxchg_relaxed() elsewhere.
2234:./include/linux/atomic/atomic-arch-fallback.h ****  *
2235:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: @true if the exchange occured, @false otherwise.
2236:./include/linux/atomic/atomic-arch-fallback.h ****  */
2237:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline bool
2238:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)
2239:./include/linux/atomic/atomic-arch-fallback.h **** {
2240:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_try_cmpxchg_relaxed)
2241:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_try_cmpxchg_relaxed(v, old, new);
2242:./include/linux/atomic/atomic-arch-fallback.h **** #elif defined(arch_atomic_try_cmpxchg)
2243:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_try_cmpxchg(v, old, new);
2244:./include/linux/atomic/atomic-arch-fallback.h **** #else
2245:./include/linux/atomic/atomic-arch-fallback.h **** 	int r, o = *old;
2246:./include/linux/atomic/atomic-arch-fallback.h **** 	r = raw_atomic_cmpxchg_relaxed(v, o, new);
2247:./include/linux/atomic/atomic-arch-fallback.h **** 	if (unlikely(r != o))
2248:./include/linux/atomic/atomic-arch-fallback.h **** 		*old = r;
2249:./include/linux/atomic/atomic-arch-fallback.h **** 	return likely(r == o);
2250:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2251:./include/linux/atomic/atomic-arch-fallback.h **** }
2252:./include/linux/atomic/atomic-arch-fallback.h **** 
2253:./include/linux/atomic/atomic-arch-fallback.h **** /**
ARM GAS  /tmp/cc0b6Xiw.s 			page 100


2254:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_sub_and_test() - atomic subtract and test if zero with full ordering
2255:./include/linux/atomic/atomic-arch-fallback.h ****  * @i: int value to subtract
2256:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2257:./include/linux/atomic/atomic-arch-fallback.h ****  *
2258:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - @i) with full ordering.
2259:./include/linux/atomic/atomic-arch-fallback.h ****  *
2260:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_sub_and_test() elsewhere.
2261:./include/linux/atomic/atomic-arch-fallback.h ****  *
2262:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: @true if the resulting value of @v is zero, @false otherwise.
2263:./include/linux/atomic/atomic-arch-fallback.h ****  */
2264:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline bool
2265:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_sub_and_test(int i, atomic_t *v)
2266:./include/linux/atomic/atomic-arch-fallback.h **** {
2267:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_sub_and_test)
2268:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_sub_and_test(i, v);
2269:./include/linux/atomic/atomic-arch-fallback.h **** #else
2270:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_sub_return(i, v) == 0;
2271:./include/linux/atomic/atomic-arch-fallback.h **** #endif
2272:./include/linux/atomic/atomic-arch-fallback.h **** }
2273:./include/linux/atomic/atomic-arch-fallback.h **** 
2274:./include/linux/atomic/atomic-arch-fallback.h **** /**
2275:./include/linux/atomic/atomic-arch-fallback.h ****  * raw_atomic_dec_and_test() - atomic decrement and test if zero with full ordering
2276:./include/linux/atomic/atomic-arch-fallback.h ****  * @v: pointer to atomic_t
2277:./include/linux/atomic/atomic-arch-fallback.h ****  *
2278:./include/linux/atomic/atomic-arch-fallback.h ****  * Atomically updates @v to (@v - 1) with full ordering.
2279:./include/linux/atomic/atomic-arch-fallback.h ****  *
2280:./include/linux/atomic/atomic-arch-fallback.h ****  * Safe to use in noinstr code; prefer atomic_dec_and_test() elsewhere.
2281:./include/linux/atomic/atomic-arch-fallback.h ****  *
2282:./include/linux/atomic/atomic-arch-fallback.h ****  * Return: @true if the resulting value of @v is zero, @false otherwise.
2283:./include/linux/atomic/atomic-arch-fallback.h ****  */
2284:./include/linux/atomic/atomic-arch-fallback.h **** static __always_inline bool
2285:./include/linux/atomic/atomic-arch-fallback.h **** raw_atomic_dec_and_test(atomic_t *v)
 740              		.loc 8 2285 1 view .LVU228
2286:./include/linux/atomic/atomic-arch-fallback.h **** {
2287:./include/linux/atomic/atomic-arch-fallback.h **** #if defined(arch_atomic_dec_and_test)
2288:./include/linux/atomic/atomic-arch-fallback.h **** 	return arch_atomic_dec_and_test(v);
2289:./include/linux/atomic/atomic-arch-fallback.h **** #else
2290:./include/linux/atomic/atomic-arch-fallback.h **** 	return raw_atomic_dec_return(v) == 0;
 741              		.loc 8 2290 2 view .LVU229
 742              	.LBB104:
 743              	.LBI104:
1237:./include/linux/atomic/atomic-arch-fallback.h **** {
 744              		.loc 8 1237 1 view .LVU230
1248:./include/linux/atomic/atomic-arch-fallback.h **** #endif
 745              		.loc 8 1248 2 view .LVU231
 746              	.LVL61:
 747              	.LBB105:
 748              	.LBI105:
 790:./include/linux/atomic/atomic-arch-fallback.h **** {
 749              		.loc 8 790 1 view .LVU232
 750              	.LBB106:
 795:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_pre_full_fence();
 751              		.loc 8 795 2 view .LVU233
 796:./include/linux/atomic/atomic-arch-fallback.h **** 	ret = arch_atomic_sub_return_relaxed(i, v);
 752              		.loc 8 796 2 view .LVU234
 753              	.LBB107:
 754              	.LBI107:
ARM GAS  /tmp/cc0b6Xiw.s 			page 101


 755              		.file 9 "./include/asm-generic/barrier.h"
   1:./include/asm-generic/barrier.h **** /* SPDX-License-Identifier: GPL-2.0-or-later */
   2:./include/asm-generic/barrier.h **** /*
   3:./include/asm-generic/barrier.h ****  * Generic barrier definitions.
   4:./include/asm-generic/barrier.h ****  *
   5:./include/asm-generic/barrier.h ****  * It should be possible to use these on really simple architectures,
   6:./include/asm-generic/barrier.h ****  * but it serves more as a starting point for new ports.
   7:./include/asm-generic/barrier.h ****  *
   8:./include/asm-generic/barrier.h ****  * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
   9:./include/asm-generic/barrier.h ****  * Written by David Howells (dhowells@redhat.com)
  10:./include/asm-generic/barrier.h ****  */
  11:./include/asm-generic/barrier.h **** 
  12:./include/asm-generic/barrier.h **** #include <linux/rwonce.h>
  13:./include/asm-generic/barrier.h **** 
  14:./include/asm-generic/barrier.h **** static inline void sync(void)
  15:./include/asm-generic/barrier.h **** {
  16:./include/asm-generic/barrier.h **** 	asm volatile("sync" : : : "memory");
  17:./include/asm-generic/barrier.h **** }
  18:./include/asm-generic/barrier.h **** 
  19:./include/asm-generic/barrier.h **** static inline void eieio(void)
  20:./include/asm-generic/barrier.h **** {
  21:./include/asm-generic/barrier.h **** 	asm volatile("eieio" : : : "memory");
  22:./include/asm-generic/barrier.h **** }
  23:./include/asm-generic/barrier.h **** 
  24:./include/asm-generic/barrier.h **** static inline void barrier(void)
 756              		.loc 9 24 20 view .LVU235
 757              	.LBB108:
  25:./include/asm-generic/barrier.h **** {
  26:./include/asm-generic/barrier.h **** 	asm volatile("" : : : "memory");
 758              		.loc 9 26 2 view .LVU236
 759              	.LBE108:
 760              	.LBE107:
 797:./include/linux/atomic/atomic-arch-fallback.h **** 	__atomic_post_full_fence();
 761              		.loc 8 797 2 view .LVU237
 762              	.LVL62:
 763              	.LBB109:
 764              	.LBI109:
 765              		.file 10 "./arch/arm_m/include/asm/atomic.h"
   1:./arch/arm_m/include/asm/atomic.h **** /* SPDX-License-Identifier: GPL-2.0-only */
   2:./arch/arm_m/include/asm/atomic.h **** /*
   3:./arch/arm_m/include/asm/atomic.h ****  * arch/arm/include/asm/atomic.h
   4:./arch/arm_m/include/asm/atomic.h ****  *
   5:./arch/arm_m/include/asm/atomic.h ****  * Copyright (C) 1996 Russell King.
   6:./arch/arm_m/include/asm/atomic.h ****  * Copyright (C) 2002 Deep Blue Solutions Ltd.
   7:./arch/arm_m/include/asm/atomic.h ****  * Modified for uClinux on STM32F407
   8:./arch/arm_m/include/asm/atomic.h ****  */
   9:./arch/arm_m/include/asm/atomic.h **** #ifndef __ASM_ARM_ATOMIC_H
  10:./arch/arm_m/include/asm/atomic.h **** #define __ASM_ARM_ATOMIC_H
  11:./arch/arm_m/include/asm/atomic.h **** 
  12:./arch/arm_m/include/asm/atomic.h **** #include <linux/compiler.h> /* Available */
  13:./arch/arm_m/include/asm/atomic.h **** #include <linux/types.h>    /* Available */
  14:./arch/arm_m/include/asm/atomic.h **** #include <asm/barrier.h>    /* Available */
  15:./arch/arm_m/include/asm/atomic.h **** 
  16:./arch/arm_m/include/asm/atomic.h **** /* Include architecture-specific configuration */
  17:./arch/arm_m/include/asm/atomic.h **** 
  18:./arch/arm_m/include/asm/atomic.h **** 
  19:./arch/arm_m/include/asm/atomic.h **** #ifdef __KERNEL__
ARM GAS  /tmp/cc0b6Xiw.s 			page 102


  20:./arch/arm_m/include/asm/atomic.h **** 
  21:./arch/arm_m/include/asm/atomic.h **** 
  22:./arch/arm_m/include/asm/atomic.h **** 
  23:./arch/arm_m/include/asm/atomic.h **** 
  24:./arch/arm_m/include/asm/atomic.h **** 
  25:./arch/arm_m/include/asm/atomic.h **** // typedef struct {
  26:./arch/arm_m/include/asm/atomic.h **** //     volatile int counter;
  27:./arch/arm_m/include/asm/atomic.h **** // } atomic_t;
  28:./arch/arm_m/include/asm/atomic.h **** 
  29:./arch/arm_m/include/asm/atomic.h **** #define ATOMIC_INIT(i) { (i) }
  30:./arch/arm_m/include/asm/atomic.h **** 
  31:./arch/arm_m/include/asm/atomic.h **** /*
  32:./arch/arm_m/include/asm/atomic.h ****  * On ARMv7-M, ordinary assignment (str instruction) doesn't clear the local
  33:./arch/arm_m/include/asm/atomic.h ****  * strex/ldrex monitor on some implementations. The reason we can use it for
  34:./arch/arm_m/include/asm/atomic.h ****  * atomic_set() is the clrex or dummy strex done on every exception return.
  35:./arch/arm_m/include/asm/atomic.h ****  */
  36:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_read(v) READ_ONCE((v)->counter)
  37:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_set(v,i)    WRITE_ONCE(((v)->counter), (i))
  38:./arch/arm_m/include/asm/atomic.h **** 
  39:./arch/arm_m/include/asm/atomic.h **** /*
  40:./arch/arm_m/include/asm/atomic.h ****  * ARMv6 UP and SMP safe atomic ops.  We use load exclusive and
  41:./arch/arm_m/include/asm/atomic.h ****  * store exclusive to ensure that these are atomic.  We may loop
  42:./arch/arm_m/include/asm/atomic.h ****  * to ensure that the update happens.
  43:./arch/arm_m/include/asm/atomic.h ****  *
  44:./arch/arm_m/include/asm/atomic.h ****  * For STM32F407 (Cortex-M4, ARMv7-M), these instructions are available.
  45:./arch/arm_m/include/asm/atomic.h ****  */
  46:./arch/arm_m/include/asm/atomic.h **** 
  47:./arch/arm_m/include/asm/atomic.h **** #define ATOMIC_OP(op, c_op, asm_op)                     \
  48:./arch/arm_m/include/asm/atomic.h **** static inline void arch_atomic_##op(int i, atomic_t *v)         \
  49:./arch/arm_m/include/asm/atomic.h **** {                                       \
  50:./arch/arm_m/include/asm/atomic.h ****     unsigned long tmp;                          \
  51:./arch/arm_m/include/asm/atomic.h ****     int result;                                 \
  52:./arch/arm_m/include/asm/atomic.h ****                                         \
  53:./arch/arm_m/include/asm/atomic.h ****     /* prefetchw(&v->counter); - prefetch not available */ \
  54:./arch/arm_m/include/asm/atomic.h ****     __asm__ __volatile__("@ atomic_" #op "\n"           \
  55:./arch/arm_m/include/asm/atomic.h **** "1: ldrex   %0, [%3]\n"                      \
  56:./arch/arm_m/include/asm/atomic.h **** "   " #asm_op " %0, %0, %4\n"                \
  57:./arch/arm_m/include/asm/atomic.h **** "   strex   %1, %0, [%3]\n"                      \
  58:./arch/arm_m/include/asm/atomic.h **** "   teq %1, #0\n"                         \
  59:./arch/arm_m/include/asm/atomic.h **** "   bne 1b"                                \
  60:./arch/arm_m/include/asm/atomic.h ****     : "=&r" (result), "=&r" (tmp), "+Qo" (v->counter)     \
  61:./arch/arm_m/include/asm/atomic.h ****     : "r" (&v->counter), "Ir" (i)                 \
  62:./arch/arm_m/include/asm/atomic.h ****     : "cc");                                   \
  63:./arch/arm_m/include/asm/atomic.h **** }
  64:./arch/arm_m/include/asm/atomic.h **** 
  65:./arch/arm_m/include/asm/atomic.h **** #define ATOMIC_OP_RETURN(op, c_op, asm_op)                  \
  66:./arch/arm_m/include/asm/atomic.h **** static inline int arch_atomic_##op##_return_relaxed(int i, atomic_t *v) \
  67:./arch/arm_m/include/asm/atomic.h **** {                                       \
  68:./arch/arm_m/include/asm/atomic.h ****     unsigned long tmp;                          \
  69:./arch/arm_m/include/asm/atomic.h ****     int result;                                 \
  70:./arch/arm_m/include/asm/atomic.h ****                                         \
  71:./arch/arm_m/include/asm/atomic.h ****     /* prefetchw(&v->counter); - prefetch not available */ \
  72:./arch/arm_m/include/asm/atomic.h ****     __asm__ __volatile__("@ atomic_" #op "_return\n"        \
  73:./arch/arm_m/include/asm/atomic.h **** "1: ldrex   %0, [%3]\n"                      \
  74:./arch/arm_m/include/asm/atomic.h **** "   " #asm_op " %0, %0, %4\n"                \
  75:./arch/arm_m/include/asm/atomic.h **** "   strex   %1, %0, [%3]\n"                      \
  76:./arch/arm_m/include/asm/atomic.h **** "   teq %1, #0\n"                         \
ARM GAS  /tmp/cc0b6Xiw.s 			page 103


  77:./arch/arm_m/include/asm/atomic.h **** "   bne 1b"                                \
  78:./arch/arm_m/include/asm/atomic.h ****     : "=&r" (result), "=&r" (tmp), "+Qo" (v->counter)     \
  79:./arch/arm_m/include/asm/atomic.h ****     : "r" (&v->counter), "Ir" (i)                 \
  80:./arch/arm_m/include/asm/atomic.h ****     : "cc");                                   \
  81:./arch/arm_m/include/asm/atomic.h ****                                         \
  82:./arch/arm_m/include/asm/atomic.h ****     return result;                              \
  83:./arch/arm_m/include/asm/atomic.h **** }
  84:./arch/arm_m/include/asm/atomic.h **** 
  85:./arch/arm_m/include/asm/atomic.h **** #define ATOMIC_FETCH_OP(op, c_op, asm_op)                   \
  86:./arch/arm_m/include/asm/atomic.h **** static inline int arch_atomic_fetch_##op##_relaxed(int i, atomic_t *v)  \
  87:./arch/arm_m/include/asm/atomic.h **** {                                       \
  88:./arch/arm_m/include/asm/atomic.h ****     unsigned long tmp;                          \
  89:./arch/arm_m/include/asm/atomic.h ****     int result, val;                             \
  90:./arch/arm_m/include/asm/atomic.h ****                                         \
  91:./arch/arm_m/include/asm/atomic.h ****     /* prefetchw(&v->counter); - prefetch not available */ \
  92:./arch/arm_m/include/asm/atomic.h ****     __asm__ __volatile__("@ atomic_fetch_" #op "\n"       \
  93:./arch/arm_m/include/asm/atomic.h **** "1: ldrex   %0, [%4]\n"                      \
  94:./arch/arm_m/include/asm/atomic.h **** "   " #asm_op " %1, %0, %5\n"                \
  95:./arch/arm_m/include/asm/atomic.h **** "   strex   %2, %1, [%4]\n"                      \
  96:./arch/arm_m/include/asm/atomic.h **** "   teq %2, #0\n"                         \
  97:./arch/arm_m/include/asm/atomic.h **** "   bne 1b"                                \
  98:./arch/arm_m/include/asm/atomic.h ****     : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Qo" (v->counter) \
  99:./arch/arm_m/include/asm/atomic.h ****     : "r" (&v->counter), "Ir" (i)                 \
 100:./arch/arm_m/include/asm/atomic.h ****     : "cc");                                   \
 101:./arch/arm_m/include/asm/atomic.h ****                                         \
 102:./arch/arm_m/include/asm/atomic.h ****     return result;                              \
 103:./arch/arm_m/include/asm/atomic.h **** }
 104:./arch/arm_m/include/asm/atomic.h **** 
 105:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_add_return_relaxed       arch_atomic_add_return_relaxed
 106:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_sub_return_relaxed       arch_atomic_sub_return_relaxed
 107:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_fetch_add_relaxed        arch_atomic_fetch_add_relaxed
 108:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_fetch_sub_relaxed        arch_atomic_fetch_sub_relaxed
 109:./arch/arm_m/include/asm/atomic.h **** 
 110:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_fetch_and_relaxed        arch_atomic_fetch_and_relaxed
 111:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_fetch_andnot_relaxed     arch_atomic_fetch_andnot_relaxed
 112:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_fetch_or_relaxed         arch_atomic_fetch_or_relaxed
 113:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_fetch_xor_relaxed        arch_atomic_fetch_xor_relaxed
 114:./arch/arm_m/include/asm/atomic.h **** 
 115:./arch/arm_m/include/asm/atomic.h **** static inline int arch_atomic_cmpxchg_relaxed(atomic_t *ptr, int old, int new)
 116:./arch/arm_m/include/asm/atomic.h **** {
 117:./arch/arm_m/include/asm/atomic.h ****     int oldval;
 118:./arch/arm_m/include/asm/atomic.h ****     unsigned long res;
 119:./arch/arm_m/include/asm/atomic.h **** 
 120:./arch/arm_m/include/asm/atomic.h ****     /* prefetchw(&ptr->counter); - prefetch not available */
 121:./arch/arm_m/include/asm/atomic.h **** 
 122:./arch/arm_m/include/asm/atomic.h ****     do {
 123:./arch/arm_m/include/asm/atomic.h ****         __asm__ __volatile__("@ atomic_cmpxchg\n"
 124:./arch/arm_m/include/asm/atomic.h **** "   ldrex   %1, [%3]\n"
 125:./arch/arm_m/include/asm/atomic.h **** "   mov     %0, #0\n"
 126:./arch/arm_m/include/asm/atomic.h **** "   teq     %1, %4\n"
 127:./arch/arm_m/include/asm/atomic.h **** "   strexeq %0, %5, [%3]\n"
 128:./arch/arm_m/include/asm/atomic.h ****         : "=&r" (res), "=&r" (oldval), "+Qo" (ptr->counter)
 129:./arch/arm_m/include/asm/atomic.h ****         : "r" (&ptr->counter), "Ir" (old), "r" (new)
 130:./arch/arm_m/include/asm/atomic.h ****         : "cc");
 131:./arch/arm_m/include/asm/atomic.h ****     } while (res);
 132:./arch/arm_m/include/asm/atomic.h **** 
 133:./arch/arm_m/include/asm/atomic.h ****     return oldval;
ARM GAS  /tmp/cc0b6Xiw.s 			page 104


 134:./arch/arm_m/include/asm/atomic.h **** }
 135:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_cmpxchg_relaxed        arch_atomic_cmpxchg_relaxed
 136:./arch/arm_m/include/asm/atomic.h **** 
 137:./arch/arm_m/include/asm/atomic.h **** static inline int arch_atomic_fetch_add_unless(atomic_t *v, int a, int u)
 138:./arch/arm_m/include/asm/atomic.h **** {
 139:./arch/arm_m/include/asm/atomic.h ****     int oldval, newval;
 140:./arch/arm_m/include/asm/atomic.h ****     unsigned long tmp;
 141:./arch/arm_m/include/asm/atomic.h **** 
 142:./arch/arm_m/include/asm/atomic.h ****     /* smp_mb(); - Memory barriers might need specific implementation */
 143:./arch/arm_m/include/asm/atomic.h ****     /* prefetchw(&v->counter); - prefetch not available */
 144:./arch/arm_m/include/asm/atomic.h **** 
 145:./arch/arm_m/include/asm/atomic.h ****     __asm__ __volatile__ ("@ atomic_add_unless\n"
 146:./arch/arm_m/include/asm/atomic.h **** "1: ldrex   %0, [%4]\n"
 147:./arch/arm_m/include/asm/atomic.h **** "   teq %0, %5\n"
 148:./arch/arm_m/include/asm/atomic.h **** "   beq 2f\n"
 149:./arch/arm_m/include/asm/atomic.h **** "   add %1, %0, %6\n"
 150:./arch/arm_m/include/asm/atomic.h **** "   strex   %2, %1, [%4]\n"
 151:./arch/arm_m/include/asm/atomic.h **** "   teq %2, #0\n"
 152:./arch/arm_m/include/asm/atomic.h **** "   bne 1b\n"
 153:./arch/arm_m/include/asm/atomic.h **** "2:"
 154:./arch/arm_m/include/asm/atomic.h ****     : "=&r" (oldval), "=&r" (newval), "=&r" (tmp), "+Qo" (v->counter)
 155:./arch/arm_m/include/asm/atomic.h ****     : "r" (&v->counter), "r" (u), "r" (a)
 156:./arch/arm_m/include/asm/atomic.h ****     : "cc");
 157:./arch/arm_m/include/asm/atomic.h **** 
 158:./arch/arm_m/include/asm/atomic.h ****     if (oldval != u)
 159:./arch/arm_m/include/asm/atomic.h ****         ; /* smp_mb(); - Memory barriers might need specific implementation */
 160:./arch/arm_m/include/asm/atomic.h **** 
 161:./arch/arm_m/include/asm/atomic.h ****     return oldval;
 162:./arch/arm_m/include/asm/atomic.h **** }
 163:./arch/arm_m/include/asm/atomic.h **** #define arch_atomic_fetch_add_unless         arch_atomic_fetch_add_unless
 164:./arch/arm_m/include/asm/atomic.h **** 
 165:./arch/arm_m/include/asm/atomic.h **** #define ATOMIC_OPS(op, c_op, asm_op)                    \
 166:./arch/arm_m/include/asm/atomic.h ****     ATOMIC_OP(op, c_op, asm_op)                     \
 167:./arch/arm_m/include/asm/atomic.h ****     ATOMIC_OP_RETURN(op, c_op, asm_op)                  \
 168:./arch/arm_m/include/asm/atomic.h ****     ATOMIC_FETCH_OP(op, c_op, asm_op)
 169:./arch/arm_m/include/asm/atomic.h **** 
 170:./arch/arm_m/include/asm/atomic.h **** ATOMIC_OPS(add, +=, add)
 171:./arch/arm_m/include/asm/atomic.h **** ATOMIC_OPS(sub, -=, sub)
 766              		.loc 10 171 1 view .LVU238
 767              	.LBB110:
 768              		.loc 10 171 1 view .LVU239
 769              		.loc 10 171 1 view .LVU240
 770              		.loc 10 171 1 view .LVU241
 771              		.syntax unified
 772              	@ 171 "./arch/arm_m/include/asm/atomic.h" 1
 773              		@ atomic_sub_return
 774 000a 53E8002F 	1: ldrex   r2, [r3]
 775 000e A2F10102 	   sub r2, r2, #1
 776 0012 43E80021 	   strex   r1, r2, [r3]
 777 0016 91F0000F 	   teq r1, #0
 778 001a F6D1     	   bne 1b
 779              	@ 0 "" 2
 780              	.LVL63:
 781              		.loc 10 171 1 view .LVU242
 782              		.loc 10 171 1 is_stmt 0 view .LVU243
 783              		.thumb
 784              		.syntax unified
ARM GAS  /tmp/cc0b6Xiw.s 			page 105


 785              	.LBE110:
 786              	.LBE109:
 798:./include/linux/atomic/atomic-arch-fallback.h **** 	return ret;
 787              		.loc 8 798 2 is_stmt 1 view .LVU244
 788              	.LBB111:
 789              	.LBI111:
  24:./include/asm-generic/barrier.h **** {
 790              		.loc 9 24 20 view .LVU245
 791              	.LBB112:
 792              		.loc 9 26 2 view .LVU246
 793              	.LBE112:
 794              	.LBE111:
 799:./include/linux/atomic/atomic-arch-fallback.h **** #else
 795              		.loc 8 799 2 view .LVU247
 796              	.LVL64:
 799:./include/linux/atomic/atomic-arch-fallback.h **** #else
 797              		.loc 8 799 2 is_stmt 0 view .LVU248
 798              	.LBE106:
 799              	.LBE105:
 800              	.LBE104:
 801              	.LBE103:
 802              	.LBE102:
  72:./block/bio.c ****         if (bio->bi_io_vec != bio->bi_inline_vecs) {
 803              		.loc 1 72 8 discriminator 1 view .LVU249
 804 001c 4AB9     		cbnz	r2, .L30
  73:./block/bio.c ****             kfree(bio->bi_io_vec);
 805              		.loc 1 73 9 is_stmt 1 view .LVU250
  73:./block/bio.c ****             kfree(bio->bi_io_vec);
 806              		.loc 1 73 16 is_stmt 0 view .LVU251
 807 001e C06B     		ldr	r0, [r0, #60]
 808              	.LVL65:
  73:./block/bio.c ****             kfree(bio->bi_io_vec);
 809              		.loc 1 73 28 view .LVU252
 810 0020 04F14403 		add	r3, r4, #68
  73:./block/bio.c ****             kfree(bio->bi_io_vec);
 811              		.loc 1 73 12 view .LVU253
 812 0024 9842     		cmp	r0, r3
 813 0026 01D0     		beq	.L32
  74:./block/bio.c ****         }
 814              		.loc 1 74 13 is_stmt 1 view .LVU254
 815              	.LVL66:
 816              	.LBB113:
 817              	.LBI113:
 383:./include/linux/slab.h **** 	__sfree__((void*)ptr);
 818              		.loc 6 383 20 view .LVU255
 819              	.LBB114:
 384:./include/linux/slab.h **** }
 820              		.loc 6 384 2 view .LVU256
 821 0028 FFF7FEFF 		bl	__sfree__
 822              	.LVL67:
 823              	.L32:
 384:./include/linux/slab.h **** }
 824              		.loc 6 384 2 is_stmt 0 view .LVU257
 825              	.LBE114:
 826              	.LBE113:
  76:./block/bio.c ****     }
 827              		.loc 1 76 9 is_stmt 1 view .LVU258
ARM GAS  /tmp/cc0b6Xiw.s 			page 106


 828              	.LBB115:
 829              	.LBI115:
 383:./include/linux/slab.h **** 	__sfree__((void*)ptr);
 830              		.loc 6 383 20 view .LVU259
 831              	.LBB116:
 384:./include/linux/slab.h **** }
 832              		.loc 6 384 2 view .LVU260
 833 002c 2046     		mov	r0, r4
 834 002e FFF7FEFF 		bl	__sfree__
 835              	.LVL68:
 836              	.L30:
 384:./include/linux/slab.h **** }
 837              		.loc 6 384 2 is_stmt 0 view .LVU261
 838              	.LBE116:
 839              	.LBE115:
  79:./block/bio.c **** 
 840              		.loc 1 79 1 view .LVU262
 841 0032 10BD     		pop	{r4, pc}
 842              	.LVL69:
 843              	.L34:
 844              	.LCFI12:
 845              		.cfi_def_cfa_offset 0
 846              		.cfi_restore 4
 847              		.cfi_restore 14
  79:./block/bio.c **** 
 848              		.loc 1 79 1 view .LVU263
 849 0034 7047     		bx	lr
 850              		.cfi_endproc
 851              	.LFE1037:
 853              		.section	.text.__bio_add_page,"ax",%progbits
 854              		.align	1
 855              		.global	__bio_add_page
 856              		.syntax unified
 857              		.thumb
 858              		.thumb_func
 860              	__bio_add_page:
 861              	.LVL70:
 862              	.LFB1039:
  92:./block/bio.c ****     struct bio_vec *bv;
 863              		.loc 1 92 1 is_stmt 1 view -0
 864              		.cfi_startproc
 865              		@ args = 0, pretend = 0, frame = 0
 866              		@ frame_needed = 0, uses_anonymous_args = 0
  93:./block/bio.c ****     if (unlikely(!bio || !page || off >= PAGE_SIZE)) return  -ENOMEM;;
 867              		.loc 1 93 5 view .LVU265
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 868              		.loc 1 94 5 view .LVU266
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 869              		.loc 1 94 8 is_stmt 0 discriminator 1 view .LVU267
 870 0000 48B3     		cbz	r0, .L39
 871 0002 8446     		mov	ip, r0
 872 0004 51B3     		cbz	r1, .L40
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 873              		.loc 1 94 9 discriminator 8 view .LVU268
 874 0006 B3F5805F 		cmp	r3, #4096
 875 000a 34BF     		ite	cc
 876 000c 0020     		movcc	r0, #0
ARM GAS  /tmp/cc0b6Xiw.s 			page 107


 877              	.LVL71:
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 878              		.loc 1 94 9 discriminator 8 view .LVU269
 879 000e 0120     		movcs	r0, #1
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 880              		.loc 1 94 8 discriminator 1 view .LVU270
 881 0010 38BB     		cbnz	r0, .L41
  92:./block/bio.c ****     struct bio_vec *bv;
 882              		.loc 1 92 1 view .LVU271
 883 0012 30B5     		push	{r4, r5, lr}
 884              	.LCFI13:
 885              		.cfi_def_cfa_offset 12
 886              		.cfi_offset 4, -12
 887              		.cfi_offset 5, -8
 888              		.cfi_offset 14, -4
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 889              		.loc 1 94 70 is_stmt 1 discriminator 10 view .LVU272
  95:./block/bio.c ****     if (bio->bi_iter.bi_size + len > (256 << PAGE_SHIFT))return -ENOMEM;;
 890              		.loc 1 95 5 view .LVU273
  95:./block/bio.c ****     if (bio->bi_iter.bi_size + len > (256 << PAGE_SHIFT))return -ENOMEM;;
 891              		.loc 1 95 12 is_stmt 0 view .LVU274
 892 0014 BCF834E0 		ldrh	lr, [ip, #52]
  95:./block/bio.c ****     if (bio->bi_iter.bi_size + len > (256 << PAGE_SHIFT))return -ENOMEM;;
 893              		.loc 1 95 28 view .LVU275
 894 0018 BCF83640 		ldrh	r4, [ip, #54]
  95:./block/bio.c ****     if (bio->bi_iter.bi_size + len > (256 << PAGE_SHIFT))return -ENOMEM;;
 895              		.loc 1 95 8 view .LVU276
 896 001c A645     		cmp	lr, r4
 897 001e 23D2     		bcs	.L42
  96:./block/bio.c **** 
 898              		.loc 1 96 5 is_stmt 1 view .LVU277
  96:./block/bio.c **** 
 899              		.loc 1 96 21 is_stmt 0 view .LVU278
 900 0020 DCF81C40 		ldr	r4, [ip, #28]
  96:./block/bio.c **** 
 901              		.loc 1 96 30 view .LVU279
 902 0024 1444     		add	r4, r4, r2
  96:./block/bio.c **** 
 903              		.loc 1 96 8 view .LVU280
 904 0026 B4F5801F 		cmp	r4, #1048576
 905 002a 20D8     		bhi	.L43
  96:./block/bio.c **** 
 906              		.loc 1 96 73 is_stmt 1 discriminator 2 view .LVU281
  98:./block/bio.c ****     bv->bv_page = page;
 907              		.loc 1 98 5 view .LVU282
  98:./block/bio.c ****     bv->bv_page = page;
 908              		.loc 1 98 14 is_stmt 0 view .LVU283
 909 002c DCF83C50 		ldr	r5, [ip, #60]
  98:./block/bio.c ****     bv->bv_page = page;
 910              		.loc 1 98 25 view .LVU284
 911 0030 0EEB4E0E 		add	lr, lr, lr, lsl #1
  98:./block/bio.c ****     bv->bv_page = page;
 912              		.loc 1 98 8 view .LVU285
 913 0034 05EB8E04 		add	r4, r5, lr, lsl #2
 914              	.LVL72:
  99:./block/bio.c ****     bv->bv_offset = off;
 915              		.loc 1 99 5 is_stmt 1 view .LVU286
ARM GAS  /tmp/cc0b6Xiw.s 			page 108


  99:./block/bio.c ****     bv->bv_offset = off;
 916              		.loc 1 99 17 is_stmt 0 view .LVU287
 917 0038 45F82E10 		str	r1, [r5, lr, lsl #2]
 100:./block/bio.c ****     bv->bv_len = len;
 918              		.loc 1 100 5 is_stmt 1 view .LVU288
 100:./block/bio.c ****     bv->bv_len = len;
 919              		.loc 1 100 19 is_stmt 0 view .LVU289
 920 003c A360     		str	r3, [r4, #8]
 101:./block/bio.c **** 
 921              		.loc 1 101 5 is_stmt 1 view .LVU290
 101:./block/bio.c **** 
 922              		.loc 1 101 16 is_stmt 0 view .LVU291
 923 003e 6260     		str	r2, [r4, #4]
 103:./block/bio.c ****     bio->bi_vcnt++;
 924              		.loc 1 103 5 is_stmt 1 view .LVU292
 103:./block/bio.c ****     bio->bi_vcnt++;
 925              		.loc 1 103 17 is_stmt 0 view .LVU293
 926 0040 DCF81C30 		ldr	r3, [ip, #28]
 927              	.LVL73:
 103:./block/bio.c ****     bio->bi_vcnt++;
 928              		.loc 1 103 26 view .LVU294
 929 0044 1344     		add	r3, r3, r2
 930 0046 CCF81C30 		str	r3, [ip, #28]
 931              	.LVL74:
 104:./block/bio.c ****     return 0;
 932              		.loc 1 104 5 is_stmt 1 view .LVU295
 104:./block/bio.c ****     return 0;
 933              		.loc 1 104 8 is_stmt 0 view .LVU296
 934 004a BCF83430 		ldrh	r3, [ip, #52]
 104:./block/bio.c ****     return 0;
 935              		.loc 1 104 17 view .LVU297
 936 004e 0133     		adds	r3, r3, #1
 937 0050 ACF83430 		strh	r3, [ip, #52]	@ movhi
 105:./block/bio.c **** }
 938              		.loc 1 105 5 is_stmt 1 view .LVU298
 939              	.LVL75:
 940              	.L37:
 106:./block/bio.c **** 
 941              		.loc 1 106 1 is_stmt 0 view .LVU299
 942 0054 30BD     		pop	{r4, r5, pc}
 943              	.LVL76:
 944              	.L39:
 945              	.LCFI14:
 946              		.cfi_def_cfa_offset 0
 947              		.cfi_restore 4
 948              		.cfi_restore 5
 949              		.cfi_restore 14
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 950              		.loc 1 94 62 discriminator 9 view .LVU300
 951 0056 6FF00B00 		mvn	r0, #11
 952              	.LVL77:
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 953              		.loc 1 94 62 discriminator 9 view .LVU301
 954 005a 7047     		bx	lr
 955              	.LVL78:
 956              	.L40:
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
ARM GAS  /tmp/cc0b6Xiw.s 			page 109


 957              		.loc 1 94 62 discriminator 9 view .LVU302
 958 005c 6FF00B00 		mvn	r0, #11
 959              	.LVL79:
  94:./block/bio.c ****     if (bio->bi_vcnt >= bio->bi_max_vecs) return -ENOMEM;
 960              		.loc 1 94 62 discriminator 9 view .LVU303
 961 0060 7047     		bx	lr
 962              	.L41:
 963 0062 6FF00B00 		mvn	r0, #11
 106:./block/bio.c **** 
 964              		.loc 1 106 1 view .LVU304
 965 0066 7047     		bx	lr
 966              	.L42:
 967              	.LCFI15:
 968              		.cfi_def_cfa_offset 12
 969              		.cfi_offset 4, -12
 970              		.cfi_offset 5, -8
 971              		.cfi_offset 14, -4
  95:./block/bio.c ****     if (bio->bi_iter.bi_size + len > (256 << PAGE_SHIFT))return -ENOMEM;;
 972              		.loc 1 95 50 discriminator 1 view .LVU305
 973 0068 6FF00B00 		mvn	r0, #11
 974 006c F2E7     		b	.L37
 975              	.L43:
  96:./block/bio.c **** 
 976              		.loc 1 96 65 discriminator 1 view .LVU306
 977 006e 6FF00B00 		mvn	r0, #11
 978 0072 EFE7     		b	.L37
 979              		.cfi_endproc
 980              	.LFE1039:
 982              		.section	.text.bio_add_page,"ax",%progbits
 983              		.align	1
 984              		.global	bio_add_page
 985              		.syntax unified
 986              		.thumb
 987              		.thumb_func
 989              	bio_add_page:
 990              	.LVL80:
 991              	.LFB1038:
  85:./block/bio.c ****     return __bio_add_page(bio,page,len,offset);
 992              		.loc 1 85 1 is_stmt 1 view -0
 993              		.cfi_startproc
 994              		@ args = 0, pretend = 0, frame = 0
 995              		@ frame_needed = 0, uses_anonymous_args = 0
  85:./block/bio.c ****     return __bio_add_page(bio,page,len,offset);
 996              		.loc 1 85 1 is_stmt 0 view .LVU308
 997 0000 08B5     		push	{r3, lr}
 998              	.LCFI16:
 999              		.cfi_def_cfa_offset 8
 1000              		.cfi_offset 3, -8
 1001              		.cfi_offset 14, -4
  86:./block/bio.c **** }
 1002              		.loc 1 86 5 is_stmt 1 view .LVU309
  86:./block/bio.c **** }
 1003              		.loc 1 86 12 is_stmt 0 view .LVU310
 1004 0002 FFF7FEFF 		bl	__bio_add_page
 1005              	.LVL81:
  87:./block/bio.c **** 
 1006              		.loc 1 87 1 view .LVU311
ARM GAS  /tmp/cc0b6Xiw.s 			page 110


 1007 0006 08BD     		pop	{r3, pc}
 1008              		.cfi_endproc
 1009              	.LFE1038:
 1011              		.section	.rodata.submit_bio.str1.4,"aMS",%progbits,1
 1012              		.align	2
 1013              	.LC0:
 1014 0000 5F726571 		.ascii	"_request io thread %d\000"
 1014      75657374 
 1014      20696F20 
 1014      74687265 
 1014      61642025 
 1015              		.section	.text.submit_bio,"ax",%progbits
 1016              		.align	1
 1017              		.global	submit_bio
 1018              		.syntax unified
 1019              		.thumb
 1020              		.thumb_func
 1022              	submit_bio:
 1023              	.LVL82:
 1024              	.LFB1041:
 121:./block/bio.c **** 
 122:./block/bio.c **** static int number;
 123:./block/bio.c **** void submit_bio(struct bio *bio){
 1025              		.loc 1 123 33 is_stmt 1 view -0
 1026              		.cfi_startproc
 1027              		@ args = 0, pretend = 0, frame = 0
 1028              		@ frame_needed = 0, uses_anonymous_args = 0
 1029              		.loc 1 123 33 is_stmt 0 view .LVU313
 1030 0000 08B5     		push	{r3, lr}
 1031              	.LCFI17:
 1032              		.cfi_def_cfa_offset 8
 1033              		.cfi_offset 3, -8
 1034              		.cfi_offset 14, -4
 1035 0002 0146     		mov	r1, r0
 124:./block/bio.c ****     kthread_run(submit_bio_wait,bio,"_request io thread %d",number);
 1036              		.loc 1 124 5 is_stmt 1 view .LVU314
 1037 0004 0023     		movs	r3, #0
 1038 0006 024A     		ldr	r2, .L52
 1039 0008 0248     		ldr	r0, .L52+4
 1040              	.LVL83:
 1041              		.loc 1 124 5 is_stmt 0 view .LVU315
 1042 000a FFF7FEFF 		bl	kthread_run
 1043              	.LVL84:
 125:./block/bio.c **** }
 1044              		.loc 1 125 1 view .LVU316
 1045 000e 08BD     		pop	{r3, pc}
 1046              	.L53:
 1047              		.align	2
 1048              	.L52:
 1049 0010 00000000 		.word	.LC0
 1050 0014 00000000 		.word	submit_bio_wait
 1051              		.cfi_endproc
 1052              	.LFE1041:
 1054              		.global	fs_bio_set
 1055              		.section	.bss.fs_bio_set,"aw",%nobits
 1056              		.align	2
 1059              	fs_bio_set:
ARM GAS  /tmp/cc0b6Xiw.s 			page 111


 1060 0000 00000000 		.space	108
 1060      00000000 
 1060      00000000 
 1060      00000000 
 1060      00000000 
 1061              		.text
 1062              	.Letext0:
 1063              		.file 11 "./include/linux/stdarg.h"
 1064              		.file 12 "./include/asm-generic/int-l64.h"
 1065              		.file 13 "./include/asm-generic/posix_types.h"
 1066              		.file 14 "./include/uapi/linux/types.h"
 1067              		.file 15 "./include/linux/types.h"
 1068              		.file 16 "./include/linux/time64.h"
 1069              		.file 17 "./arch/arm_m/include/asm/sched.h"
 1070              		.file 18 "./include/linux/sched.h"
 1071              		.file 19 "./include/linux/spinlock_types_raw.h"
 1072              		.file 20 "./include/linux/spinlock_types.h"
 1073              		.file 21 "./include/linux/mutex.h"
 1074              		.file 22 "./include/linux/errseq.h"
 1075              		.file 23 "./include/linux/rbtree_types.h"
 1076              		.file 24 "./include/linux/uidgid_types.h"
 1077              		.file 25 "./include/linux/projid.h"
 1078              		.file 26 "./include/linux/fs.h"
 1079              		.file 27 "./include/linux/mnt_idmapping.h"
 1080              		.file 28 "./include/linux/uio.h"
 1081              		.file 29 "./include/linux/migrate_mode.h"
 1082              		.file 30 "./include/linux/wait.h"
 1083              		.file 31 "./include/linux/xarray.h"
 1084              		.file 32 "./include/linux/lockref.h"
 1085              		.file 33 "./include/linux/dcache.h"
 1086              		.file 34 "./include/linux/path.h"
 1087              		.file 35 "./include/linux/stddef.h"
 1088              		.file 36 "./include/linux/gfp_types.h"
 1089              		.file 37 "./include/linux/reciprocal_div.h"
 1090              		.file 38 "./include/linux/mm_type.h"
 1091              		.file 39 "./include/linux/statfs.h"
 1092              		.file 40 "./include/linux/stat.h"
 1093              		.file 41 "./include/linux/bvec.h"
 1094              		.file 42 "./include/linux/blk_types.h"
 1095              		.file 43 "./include/linux/rw_hint.h"
 1096              		.file 44 "./include/linux/bio.h"
 1097              		.file 45 "./include/linux/mempool_super_haper.h"
 1098              		.file 46 "./include/linux/mempool.h"
 1099              		.file 47 "./include/linux/lockdep_types.h"
 1100              		.file 48 "./include/linux/workqueue_types.h"
 1101              		.file 49 "./include/linux/blk-mq.h"
 1102              		.file 50 "./include/uapi/linux/pr.h"
 1103              		.file 51 "./include/linux/pr.h"
 1104              		.file 52 "./include/linux/hdreg.h"
 1105              		.file 53 "./include/linux/sprintf.h"
 1106              		.file 54 "./arch/arm_m/include/asm/string.h"
 1107              		.file 55 "./include/linux/instrumented.h"
 1108              		.file 56 "./include/linux/kcsan-checks.h"
 1109              		.file 57 "./include/linux/kasan-checks.h"
 1110              		.file 58 "<built-in>"
ARM GAS  /tmp/cc0b6Xiw.s 			page 112


DEFINED SYMBOLS
                            *ABS*:00000000 bio.c
     /tmp/cc0b6Xiw.s:21     .text.__spin_unlock:00000000 $t
     /tmp/cc0b6Xiw.s:26     .text.__spin_unlock:00000000 __spin_unlock
     /tmp/cc0b6Xiw.s:45     .text.spin_unlock:00000000 $t
     /tmp/cc0b6Xiw.s:50     .text.spin_unlock:00000000 spin_unlock
     /tmp/cc0b6Xiw.s:73     .text.__spin_lock:00000000 $t
     /tmp/cc0b6Xiw.s:78     .text.__spin_lock:00000000 __spin_lock
     /tmp/cc0b6Xiw.s:128    .text.spin_lock:00000000 $t
     /tmp/cc0b6Xiw.s:133    .text.spin_lock:00000000 spin_lock
     /tmp/cc0b6Xiw.s:193    .text.submit_bio_wait:00000000 $t
     /tmp/cc0b6Xiw.s:199    .text.submit_bio_wait:00000000 submit_bio_wait
     /tmp/cc0b6Xiw.s:336    .text.kthread_run:00000000 $t
     /tmp/cc0b6Xiw.s:341    .text.kthread_run:00000000 kthread_run
     /tmp/cc0b6Xiw.s:437    .text.bio_alloc_bioset:00000000 $t
     /tmp/cc0b6Xiw.s:443    .text.bio_alloc_bioset:00000000 bio_alloc_bioset
     /tmp/cc0b6Xiw.s:704    .text.bio_put:00000000 $t
     /tmp/cc0b6Xiw.s:710    .text.bio_put:00000000 bio_put
     /tmp/cc0b6Xiw.s:854    .text.__bio_add_page:00000000 $t
     /tmp/cc0b6Xiw.s:860    .text.__bio_add_page:00000000 __bio_add_page
     /tmp/cc0b6Xiw.s:983    .text.bio_add_page:00000000 $t
     /tmp/cc0b6Xiw.s:989    .text.bio_add_page:00000000 bio_add_page
     /tmp/cc0b6Xiw.s:1012   .rodata.submit_bio.str1.4:00000000 $d
     /tmp/cc0b6Xiw.s:1016   .text.submit_bio:00000000 $t
     /tmp/cc0b6Xiw.s:1022   .text.submit_bio:00000000 submit_bio
     /tmp/cc0b6Xiw.s:1049   .text.submit_bio:00000010 $d
     /tmp/cc0b6Xiw.s:1059   .bss.fs_bio_set:00000000 fs_bio_set
     /tmp/cc0b6Xiw.s:1056   .bss.fs_bio_set:00000000 $d

UNDEFINED SYMBOLS
stop_all_scheduler
start_all_scheduler
get_current_task
__delay
request_alloc
__blk_insert_request
request_queue_add
block_scheduler
vsnprintf
task_run
run_scheduler
__smalloc__
memset
__sfree__
